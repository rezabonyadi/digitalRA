[
	{
		"uid": "S2:9506874e70c07a8016c0e82872dc794384c98678",
		"title": "Self-Supervised Learning for Scanned Halftone Classification with Novel Augmentation Techniques",
		"doi": "10.1109/icip49359.2023.10222434",
		"rank": 1,
		"year": 2023,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Jingcai Guo",
			"Sankarasrinivasan Seshathiri"
		]
	},
	{
		"uid": "S2:2812cc5b514a5c87e29de2e051b3ebadc021897d",
		"title": "Augmentation-Free Self-Supervised Learning on Graphs",
		"doi": "10.1609/aaai.v36i7.20700",
		"abstract": "Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, we argue that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentation-based methods is highly dependent on the choice of augmentation scheme, i.e., augmentation hyperparameters and combinations of augmentation. In this paper, we propose a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, we generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various real-world datasets demonstrate the superiority of AFGRL. The source code for AFGRL is available at https://github.com/Namkyeong/AFGRL.",
		"rank": 2,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 69,
		"ecc": 69,
		"use": 1,
		"authors": [
			"Namkyeong Lee",
			"Junseok Lee",
			"Chanyoung Park"
		]
	},
	{
		"uid": "S2:9a260cc14669aeb312936b212d45db89952908bc",
		"title": "Self-Supervised Learning with Attention-based Latent Signal Augmentation for Sleep Staging with Limited Labeled Data",
		"doi": "10.24963/ijcai.2022/537",
		"abstract": "Sleep staging is an important task that enables sleep quality assessment and disorder diagnosis. Due to dependency on manually labeled data, many researches have turned from supervised approaches to self-supervised learning (SSL) for sleep staging. While existing SSL methods have made significant progress in terms of its comparable performance to supervised methods, there are still some limitations. Contrastive learning could potentially lead to false negative pair assignments in sleep signal data. Moreover, existing data augmentation techniques directly modify the original signal data, making it likely to lose important information. To mitigate these issues, we propose Self-Supervised Learning with Attention-aided Positive Pairs (SSLAPP). Instead of the contrastive learning, SSLAPP carefully draws high-quality positive pairs and exploits them in representation learning. Here, we propose attention-based latent signal augmentation, which plays a key role by capturing important features without losing valuable signal information. Experimental results show that our proposed method achieves state-of-the-art performance in sleep stage classification with limited labeled data. The code is available at: https://github.com/DILAB-HYU/SSLAPP",
		"rank": 3,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 2,
		"ecc": 2,
		"use": 1,
		"authors": [
			"Harim Lee",
			"Eunseon Seong",
			"Dong-Kyu Chae"
		]
	},
	{
		"uid": "S2:865e088cb2cba2fe80949b83c701de8ab32f8c00",
		"title": "Monocular three-dimensional object detection using data augmentation and self-supervised learning in autonomous driving",
		"doi": "10.1117/1.JEI.32.1.011004",
		"abstract": "Abstract. Monocular three-dimensional (3D) object detection (OD) is an essential and challenging task in the domain of autonomous driving. Modern convolution neural network-based architectures for OD heavily rely on data augmentation (DA) and self-supervised learning (SSL). However, they have been relatively less explored for monocular 3D OD, especially in the field of autonomous driving. DAs for two-dimensional OD techniques do not directly extend to the 3D objects. Literature shows that this requires adaptation of the 3D geometry of the input scene and synthesis of new viewpoints. This requires accurate depth information of the scene which may not be available always. We propose augmentations for monocular 3D OD without creating view synthesis. The proposed method uses DA with SSL approach via multiobject labeling as the pretext task. We evaluate the proposed DA-SSL approach on RTM3D detection model (baseline), with and without the application of DA. The results demonstrate improvements between 2% and 3% in mAP 3D and 0.9% to 1.5% BEV scores using SSL over the baseline scores. We propose an inverse class frequency weighted (ICFW) mAP score that highlights improvements in detection for low-frequency classes in a class imbalanced datasets with long tails. We observe improvements in both ICFW mAP 3D and Bird’s Eye View (BEV) scores to take into account the class imbalance in the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) validation dataset. We achieve 4% to 5% increase in ICFW metrics with the pretext task.",
		"rank": 4,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Sugirtha Thayalan",
			"S. Muthukumarasamy",
			"K. Santhakumar",
			"Kiran Bangalore Ravi",
			"Hao Liu",
			"T. Gauthier",
			"S. Yogamani"
		]
	},
	{
		"uid": "S2:b159aa9f0ab0bb5389645816cc7980bade610ba8",
		"title": "Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning",
		"abstract": "Self-supervised representation learning follows a paradigm of withholding some part of the data and tasking the network to predict it from the remaining part. Among many techniques, data augmentation lies at the core for creating the information gap. Towards this end, masking has emerged as a generic and powerful tool where content is withheld along the sequential dimension, e.g., spatial in images, temporal in audio, and syntactic in language. In this paper, we explore the orthogonal channel dimension for generic data augmentation by exploiting precision redundancy. The data for each channel is quantized through a non-uniform quantizer, with the quantized value sampled randomly within randomly sampled quantization bins. From another perspective, quantization is analogous to channel-wise masking, as it removes the information within each bin, but preserves the information across bins. Our approach significantly surpasses existing generic data augmentation methods, while showing on par performance against modality-specific augmentations. We comprehensively evaluate our approach on vision, audio, 3D point clouds, as well as the DABS benchmark which is comprised of various data modalities. The code is available at https: //github.com/microsoft/random_quantize.",
		"rank": 5,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Huisi Wu",
			"Chenyang Lei",
			"Xiao Sun",
			"Pengju Wang",
			"Qifeng Chen",
			"Kwang-Ting Cheng",
			"Stephen Lin",
			"Zhirong Wu"
		]
	},
	{
		"uid": "S2:d0af89ae70c723f78ddcca04df299933e1396c7e",
		"title": "Robust Question-Answering with Data Augmentation and Self-Supervised Learning",
		"abstract": "Machine reading comprehension has made significant strides in recent years. How-ever, Question-Answering (QA) systems have struggled to generalize beyond training data. In Robust QA, we tackle situations where the train and test distributions are different, with significantly fewer labeled out-of-domain (OOD) examples. We implement several data augmentation approaches to improve OOD robustness: data mixing, selective masking, Easy Data Augmentation (EDA), and Back-Translation (BT). We also propose and investigate a self-supervised (SS) learning approach, inspired by similar techniques in the vision domain. Combining our three best techniques (EDA, BT, SS) yields the best OOD validation performance. To maximally improve test set performance, we use the best technique for each OOD dataset to generate predictions; this strategy enables us to achieve third on the RobustQA test leaderboard by F1, with an F1 score of 62.99 & EM of 45.05.",
		"rank": 6,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Arsh Zahed",
			"A. Sridhar"
		]
	},
	{
		"uid": "S2:0c635829365f8639f699892764669ad143bff88a",
		"title": "BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation",
		"doi": "10.1109/IJCNN52387.2021.9534474",
		"abstract": "Inspired by the recent progress in self-supervised learning for computer vision that generates supervision using data augmentations, we explore a new general-purpose audio representation learning approach. We propose learning general-purpose audio representation from a single audio segment without expecting relationships between different time segments of audio samples. To implement this principle, we introduce Bootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced “viola”), an audio self-supervised learning method based on BYOL for learning general-purpose audio representation. Unlike most previous audio self-supervised learning methods that rely on agreement of vicinity audio segments or disagreement of remote ones, BYOL-A creates contrasts in an augmented audio segment pair derived from a single audio segment. With a combination of normalization and augmentation techniques, BYOL-A achieves state-of-the-art results in various downstream tasks. Extensive ablation studies also clarified the contribution of each component and their combinations.",
		"rank": 7,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 91,
		"ecc": 91,
		"use": 1,
		"authors": [
			"Daisuke Niizumi",
			"Daiki Takeuchi",
			"Yasunori Ohishi",
			"N. Harada",
			"K. Kashino"
		]
	},
	{
		"uid": "S2:7bcd50b75b84552b9e5d806efe376651132d78c1",
		"title": "Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning",
		"doi": "10.48550/arXiv.2204.04170",
		"abstract": "Contrastive learning enables learning useful audio and speech representations without ground-truth labels by maximizing the similarity between latent representations of similar signal segments. In this framework various data augmentation techniques are usually exploited to help enforce desired invariances within the learned representations, improving performance on various audio tasks thanks to more robust embeddings. Now, selecting the most relevant augmentations has proven crucial for better downstream performances. Thus, this work introduces a conditional independance-based method which allows for automatically selecting a suitable distribution on the choice of augmentations and their parametrization from a set of predefined ones, for contrastive self-supervised pre-training. This is performed with respect to a downstream task of interest, hence saving a costly hyper-parameter search. Experiments performed on two different downstream tasks validate the proposed approach showing better results than experimenting without augmentation or with baseline augmentations. We furthermore conduct a qualitative analysis of the automatically selected augmentations and their variation according to the considered final downstream dataset.",
		"rank": 8,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 4,
		"ecc": 4,
		"use": 1,
		"authors": [
			"Salah Zaiem",
			"Titouan Parcollet",
			"S. Essid"
		]
	},
	{
		"uid": "S2:2c8b82f018d64ec9b6035007e09ef9fe61a8c016",
		"title": "Self-Supervised Representation Learning Framework for Remote Physiological Measurement Using Spatiotemporal Augmentation Loss",
		"doi": "10.1609/aaai.v36i2.20143",
		"abstract": "Recent advances in supervised deep learning methods are enabling remote measurements of photoplethysmography-based physiological signals using facial videos. The performance of these supervised methods, however, are dependent on the availability of large labelled data. Contrastive learning as a self-supervised method has recently achieved state-of-the-art performances in learning representative data features by maximising mutual information between different augmented views. However, existing data augmentation techniques for contrastive learning are not designed to learn physiological signals from videos and often fail when there are complicated noise and subtle and periodic colour/shape variations between video frames. To address these problems, we present a novel self-supervised spatiotemporal learning framework for remote physiological signal representation learning, where there is a lack of labelled training data. Firstly, we propose a landmark-based spatial augmentation that splits the face into several informative parts based on the Shafer’s dichromatic reﬂection model to characterise subtle skin colour fluctuations. We also formulate a sparsity-based temporal augmentation exploiting Nyquist–Shannon sampling theorem to effectively capture periodic temporal changes by modelling physiological signal features. Furthermore, we introduce a constrained spatiotemporal loss which generates pseudo-labels for augmented video clips. It is used to regulate the training process and handle complicated noise. We evaluated our framework on 3 public datasets and demonstrated superior performances than other self-supervised methods and achieved competitive accuracy compared to the state-of-the-art supervised methods. Code is available at https://github.com/Dylan-H-Wang/SLF-RPM.",
		"rank": 9,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 15,
		"ecc": 15,
		"use": 1,
		"authors": [
			"Hao Wang",
			"E. Ahn",
			"Jinman Kim"
		]
	},
	{
		"uid": "S2:beccfd2feea3ad8a74e5dda866ab69446383c6fa",
		"title": "Self-Supervised Learning for Panoptic Segmentation of Multiple Fruit Flower Species",
		"doi": "10.1109/LRA.2022.3217000",
		"abstract": "Convolutional neural networks trained using manually generated labels are commonly used for semantic or instance segmentation. In precision agriculture, automated flower detection methods use supervised models and post-processing techniques that may not perform consistently as the appearance of the flowers and the data acquisition conditions vary. We propose a self-supervised learning strategy to enhance the sensitivity of segmentation models to different flower species using automatically generated pseudo-labels. We employ a data augmentation and refinement approach to improve the accuracy of the model predictions. The augmented semantic predictions are then converted to panoptic pseudo-labels to iteratively train a multi-task model. The self-supervised model predictions can be refined with existing post-processing approaches to further improve their accuracy. An evaluation on a multi-species fruit tree flower dataset demonstrates that our method outperforms state-of-the-art models without computationally expensive post-processing steps, providing a new baseline for flower detection applications.",
		"rank": 10,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 4,
		"ecc": 4,
		"use": 1,
		"authors": [
			"Abubakar Siddique",
			"A. Tabb",
			"Henry Medeiros"
		]
	},
	{
		"uid": "S2:d08775cf2bebcffa05c6fa506f687ef56953f128",
		"title": "AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning",
		"abstract": "Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.",
		"rank": 11,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 26,
		"ecc": 26,
		"use": 1,
		"authors": [
			"J. Song",
			"Kyeongbo Kong",
			"Ye In Park",
			"Seonggyun Kim",
			"Suk-Ju Kang"
		]
	},
	{
		"uid": "S2:9ce6f8853a4a71d98cb4989c6efe790be9b64412",
		"title": "Saliency Can Be All You Need In Contrastive Self-Supervised Learning",
		"doi": "10.48550/arXiv.2210.16776",
		"abstract": "We propose an augmentation policy for Contrastive Self-Supervised Learning (SSL) in the form of an already established Salient Image Segmentation technique entitled Global Contrast based Salient Region Detection. This detection technique, which had been devised for unrelated Computer Vision tasks, was empirically observed to play the role of an augmentation facilitator within the SSL protocol. This observation is rooted in our practical attempts to learn, by SSL-fashion, aerial imagery of solar panels, which exhibit challenging bound-ary patterns. Upon the successful integration of this technique on our problem domain, we formulated a generalized procedure and conducted a comprehensive, systematic performance assessment with various Contrastive SSL algorithms subject to standard augmentation techniques. This evaluation, which was conducted across multiple datasets, indicated that the proposed technique indeed contributes to SSL. We hypothesize whether salient image segmentation may sufﬁce as the only augmentation policy in Contrastive SSL when treating downstream segmentation tasks.",
		"rank": 12,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 1,
		"ecc": 1,
		"use": 1,
		"authors": [
			"Veysel Kocaman",
			"O. M. Shir",
			"Thomas Bäck",
			"A. Belbachir"
		]
	},
	{
		"uid": "S2:6e3d6fc792f4ae43a8e7ba48da24211df3bf49bc",
		"title": "STab: Self-supervised Learning for Tabular Data",
		"abstract": "Self-supervised learning has drawn recent interest for learning generalizable, trans-ferable and robust representations from unlabeled tabular data. Unfortunately, unlike its image and language counterparts which have unique spatial or semantic structure information, it is difficult to design an effective augmentation method generically beneficial to downstream tasks in the tabular setting, owing to its lack of common structure and diverse nature. On the other hand, most existing augmentation methods are domain-specific (such as rotation in vision, token masking for NLP, and edge dropping for graphs), making them less effective for real-world tabular data. This significantly limits tabular self-supervised learning and hin-ders progress in this domain. Aiming to fill this crucial gap, we propose STab , an augmentation-free self-supervised representation learning based on stochastic regularization techniques that does not rely on negative pairs, to capture highly heterogeneous and non-structured information in tabular data. Our experiments show that STab achieves state-of-the-art performance compared to existing contrastive and pretext task self-supervised methods.",
		"rank": 13,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 3,
		"ecc": 3,
		"use": 1,
		"authors": [
			"Ehsan Hajiramezanali",
			"Max W. Shen",
			"Gabriele Scalia",
			"N. Diamant"
		]
	},
	{
		"uid": "S2:6ec173159e3d4d00eedb81a3f1ab8e1b3aba6974",
		"title": "Facilitating Radar-Based Gesture Recognition With Self-Supervised Learning",
		"doi": "10.1109/SECON55815.2022.9918549",
		"abstract": "With deep learning, millimeter-wave radar-based gesture recognition applications have achieved satisfactory results. However, most existing approaches highly rely on highquality labeled data, and they suffer from severe over-fitting when labeled data are scarce. To end this, we present RadarAE, a novel representation learning framework for radar sensing applications. RadarAE learns sophisticated representations from massive low-cost unlabeled radar data, which enables accurate gesture recognition with few labeled data. To achieve this goal, we first meticulously observe the characteristics of raw radar data and extract an effective feature, Spatio-Temporal Motion Map (STMM). Then we borrow the key principle of Masked Autoencoders (MAE), a self-supervised learning technique for images, and propose an MAE-like model to learn useful representations from STMM. To adapt RadarAE to radar sensing applications, we present a series of customization techniques, including data augmentation, optimized model structure, and adaptive pretraining method. With the learned high-level representations, gesture recognition models can achieve superior performance in few-shot scenarios. Experiment results show that our model can achieve 79.1%, 92.1%, 97.8%, and 99.5% recognition accuracy in the 1, 2, 4, and 8-shot scenarios, respectively, where x-shot refers to the number of labeled samples for each gesture type. The source codes and dataset are made publicly available11https://githuh.com/Ela-Boska/RadarAE.",
		"rank": 14,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 1,
		"ecc": 1,
		"use": 1,
		"authors": [
			"Zhiyao Sheng",
			"Huatao Xu",
			"Qian Zhang",
			"Dong Wang"
		]
	},
	{
		"uid": "S2:d6572f2250337975fc4a54abfcdf08b4829b2cfc",
		"title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
		"doi": "10.48550/arXiv.2204.02967",
		"abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .",
		"rank": 15,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 33,
		"ecc": 33,
		"use": 1,
		"authors": [
			"Sravya Popuri",
			"Peng-Jen Chen",
			"Changhan Wang",
			"J. Pino",
			"Yossi Adi",
			"Jiatao Gu",
			"Wei-Ning Hsu",
			"Ann Lee"
		]
	},
	{
		"uid": "S2:edc144b404d9debe523cd4114d61deb20e5e8601",
		"title": "Multi-view contrastive self-supervised learning of accounting data representations for downstream audit tasks",
		"doi": "10.1145/3490354.3494373",
		"abstract": "International audit standards require the direct assessment of a financial statement's underlying accounting transactions, referred to as journal entries. Recently, driven by the advances in artificial intelligence, deep learning inspired audit techniques have emerged in the field of auditing vast quantities of journal entry data. Nowadays, the majority of such methods rely on a set of specialized models, each trained for a particular audit task. At the same time, when conducting a financial statement audit, audit teams are confronted with (i) challenging time-budget constraints, (ii) extensive documentation obligations, and (iii) strict model interpretability requirements. As a result, auditors prefer to harness only a single preferably 'multi-purpose' model throughout an audit engagement. We propose a contrastive self-supervised learning framework designed to learn audit task invariant accounting data representations to meet this requirement. The framework encompasses deliberate interacting data augmentation policies that utilize the attribute characteristics of journal entry data. We evaluate the framework on two real-world datasets of city payments and transfer the learned representations to three downstream audit tasks: anomaly detection, audit sampling, and audit documentation. Our experimental results provide empirical evidence that the proposed framework offers the ability to increase the efficiency of audits by learning rich and interpretable 'multi-task' representations.",
		"rank": 16,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 10,
		"ecc": 10,
		"use": 1,
		"authors": [
			"Marco Schreyer",
			"Timur Sattarov",
			"Damian Borth"
		]
	},
	{
		"uid": "S2:a4f4ad122d446b0efcf761f7133e7d37d09762a9",
		"title": "A Brief Summary of Interactions Between Meta-Learning and Self-Supervised Learning",
		"abstract": "This paper briefly reviews the connections between meta-learning and self-supervised learning. Meta-learning can be applied to improve model generalization capability and to construct general AI algorithms. Self-supervised learning utilizes self-supervision from original data and extracts higher-level generalizable features through unsupervised pre-training or optimization of contrastive loss objectives. In self-supervised learning, data augmentation techniques are widely applied and data labels are not required since pseudo labels can be estimated from trained models on similar tasks. Meta-learning aims to adapt trained deep models to solve diverse tasks and to develop general AI algorithms. We review the associations of meta-learning with both generative and contrastive self-supervised learning models. Unlabeled data from multiple sources can be jointly considered even when data sources are vastly different. We show that an integration of meta-learning and self-supervised learning models can best contribute to the improvement of model generalization capability. Self-supervised learning guided by meta-learner and general meta-learning algorithms under self-supervision are both examples of possible combinations.",
		"rank": 17,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 3,
		"ecc": 3,
		"use": 1,
		"authors": [
			"Huimin Peng"
		]
	},
	{
		"uid": "S2:ef25af482d338ecd8e6d244b3756ef1f371e2c01",
		"title": "Noise2Recon: Enabling Joint MRI Reconstruction and Denoising with Semi-Supervised and Self-Supervised Learning",
		"abstract": "Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, supervised DL methods depend on extensive amounts of fully-sampled (labeled) data and are sensitive to out-of-distribution (OOD) shifts, particularly low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose Noise2Recon, a model-agnostic, consistency training method for joint MRI reconstruction and denoising that can use both fully-sampled (labeled) and undersampled (unlabeled) scans in semi-supervised and self-supervised settings. With limited or no labeled training data, Noise2Recon outperforms compressed sensing and deep learning baselines, including supervised networks, augmentation-based training, fine-tuned denoisers, and self-supervised methods, and matches performance of supervised models, which were trained with 14x more fully-sampled scans. Noise2Recon also outperforms all baselines, including state-of-the-art fine-tuning and augmentation techniques, among low-SNR scans and when generalizing to other OOD factors, such as changes in acceleration factors and different datasets. Augmentation extent and loss weighting hyperparameters had negligible impact on Noise2Recon compared to supervised methods, which may indicate increased training stability. Our code is available at https://github.com/ad12/meddlr.",
		"rank": 18,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 2,
		"ecc": 2,
		"use": 1,
		"authors": [
			"Arjun D Desai",
			"Batu Mehmet Ozturkler",
			"Christopher M. Sandino",
			"R. Boutin",
			"M. Willis",
			"S. Vasanawala",
			"B. Hargreaves",
			"Christopher Ré",
			"J. Pauly",
			"A. Chaudhari"
		]
	},
	{
		"uid": "S2:2b182248a3b2af496caba031f6209c20378b649a",
		"title": "Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where",
		"abstract": "While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for one view in a contrastive sample-pair the randomly-sampled masking regions could be overly concentrated on important/salient objects thus resulting in misleading contrastiveness to the other view. To this end, we propose to explicitly take the saliency constraint into consideration in which the masked regions are more evenly distributed among the foreground and background for realizing the masking-based augmentation. Moreover, we introduce hard negative samples by masking larger regions of salient patches in an input image. Extensive experiments conducted on various datasets, contrastive learning mechanisms, and downstream tasks well verify the efficacy as well as the superior performance of our proposed method with respect to several state-of-the-art baselines.",
		"rank": 19,
		"year": 2023,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Zhi-Yi Chin",
			"Chieh-Ming Jiang",
			"Ching-Chun Huang",
			"Pin-Yu Chen",
			"Wei-Chen Chiu"
		]
	},
	{
		"uid": "S2:b0f6f1c464700c79fa20fb7b2e3975c60208a559",
		"title": "Improving Cross-Domain Detection with Self-Supervised Learning",
		"doi": "10.1109/CVPRW59228.2023.00503",
		"abstract": "Cross-Domain Detection (XDD) aims to train a domain-adaptive object detector using unlabeled images from a target domain and labeled images from a source domain. Existing approaches achieve this either by transferring the style of source images to that of target images, or by aligning the features of images from the two domains. In this paper, rather than proposing another method following the existing lines, we introduce a new framework complementary to existing methods. Our framework unifies some popular Self-Supervised Learning (SSL) techniques (e.g., rotation angle prediction, strong/weak data augmentation, mean teacher modeling) and adapts them to the XDD task. Our basic idea is to leverage the unsupervised nature of these SSL techniques and apply them simultaneously across domains (source and target) and models (student and teacher). These SSL techniques can thus serve as shared bridges that facilitate knowledge transfer between domains. More importantly, as these techniques are independently applied in each domain, they are complementary to existing domain alignment techniques that relies on interactions between domains (e.g., adversarial alignment). We perform extensive analyses on these SSL techniques and show that they significantly improve the performance of existing methods. In addition, we reach comparable or even better performance than the state-of-the-art methods when integrating our framework with an old well-established method.",
		"rank": 20,
		"year": 2023,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"K. Li",
			"Curtis Wigington",
			"Chris Tensmeyer",
			"Vlad I. Morariu",
			"Handong Zhao",
			"Varun Manjunatha",
			"Nikolaos Barmpalios",
			"Y. Fu"
		]
	}
]
