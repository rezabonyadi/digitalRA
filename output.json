[
	{
		"uid": "S2:0766019f10ab3eb7d16b8ee6d9d0875421603c20",
		"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning",
		"abstract": "Autoencoding has been a popular topic across many fields and recently emerged in the 3D domain. However, many 3D representations (e.g., point clouds) are discrete samples of the underlying continuous 3D surface which makes them different from other data modalities. This process inevitably introduces sampling variations on the underlying 3D shapes. In learning 3D representation, a desirable goal is to disregard such sampling variations while focusing on capturing transferable knowledge of the underlying 3D shape. This aim poses a grand challenge to existing representation learning paradigms. For example, the standard autoencoding paradigm forces the encoder to capture such sampling variations as the decoder has to reconstruct the original point cloud. In this paper, we introduce the Implicit Autoencoder (IAE). This simple yet effective method addresses this challenge by replacing the point cloud decoder with an implicit decoder. The implicit decoder can output a continuous representation that is shared among different point cloud samplings of the same model. Reconstructing under the implicit representation can prioritize that the encoder discards sampling variations, introducing appropriate inductive bias to learn more generalizable feature representations. We validate this claim via experimental analysis. Moreover, our implicit decoder offers excellent flexibility in designing suitable implicit representations for different tasks. We demonstrate the usefulness of IAE across various self-supervised learning tasks for both 3D objects and 3D scenes. Experimental results show that IAE consistently outperforms the state-of-the-art in each task.",
		"rank": 1,
		"year": 2022,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Siming Yan",
			"Zhenpei Yang",
			"Haoxiang Li",
			"Li Guan",
			"Hao Kang",
			"G. Hua",
			"Qi-Xing Huang"
		]
	},
	{
		"uid": "S2:23720e9f103dc3272e673aeea58c812a67d9ffab",
		"title": "Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning",
		"doi": "10.48550/arXiv.2308.16481",
		"abstract": "We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. During training, our model is trained using a meta-auxiliary learning approach, such that the adapted model via auxiliary tasks improves the accuracy of the primary task. Experimental results demonstrate the effectiveness of our approach in improving generalization of point cloud registration and outperforming other state-of-the-art approaches.",
		"rank": 2,
		"year": 2023,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"A. Hatem",
			"Yiming Qian",
			"Yang Wang"
		]
	},
	{
		"uid": "S2:f9308579bb3b27ae331d7b3b5145596d199a07bd",
		"title": "Seeing the whole picture instead of a single point: Self-supervised likelihood learning for deep generative models",
		"abstract": "Recent findings show that deep generative models can judge out-of-distribution samples as more likely than those drawn from the same distribution as the training data. In this work, we focus on variational autoencoders (VAEs) and address the problem of misaligned likelihood estimates on image data. We develop a novel likelihood function that is based not only on the parameters returned by the VAE but also on the features of the data learned in a self-supervised fashion. In this way, the model additionally captures the semantic information that is disregarded by the usual VAE likelihood function. We demonstrate the improvements in reliability of the estimates with experiments on the FashionMNIST and MNIST datasets.",
		"rank": 3,
		"year": 2019,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Petra Poklukar"
		]
	},
	{
		"uid": "S2:1365311c4ee7e1d2cfd58d222c808fcde6b21505",
		"title": "ACL-SPC: Adaptive Closed-Loop System for Self-Supervised Point Cloud Completion",
		"doi": "10.1109/CVPR52729.2023.00910",
		"abstract": "Point cloud completion addresses filling in the missing parts of a partial point cloud obtained from depth sensors and generating a complete point cloud. Although there has been steep progress in the supervised methods on the synthetic point cloud completion task, it is hardly applicable in real-world scenarios due to the domain gap between the synthetic and real-world datasets or the requirement of prior information. To overcome these limitations, we propose a novel self-supervised framework ACL-SPC for point cloud completion to train and test on the same data. ACL-SPC takes a single partial input and attempts to output the complete point cloud using an adaptive closed-loop (ACL) system that enforces the output same for the variation of an input. We evaluate our ACL-SPC on various datasets to prove that it can successfully learn to complete a partial point cloud as the first self-supervised scheme. Results show that our method is comparable with unsupervised methods and achieves superior performance on the real-world dataset compared to the supervised methods trained on the synthetic dataset. Extensive experiments justify the necessity of self-supervised learning and the effectiveness of our proposed method for the real-world point cloud completion task. The code is publicly available from this link.",
		"rank": 4,
		"year": 2023,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 1,
		"ecc": 1,
		"use": 1,
		"authors": [
			"S. Hong",
			"Mohsen Yavartanoo",
			"Reyhaneh Neshatavar",
			"Kyoung Mu Lee"
		]
	},
	{
		"uid": "S2:d90a630a9f5e3bfb4b8e57eea3f6adce82031830",
		"title": "Domain Adaptation on Point Clouds via Geometry-Aware Implicits",
		"doi": "10.1109/CVPR52688.2022.00708",
		"abstract": "As a popular geometric representation, point clouds have attracted much attention in 3D vision, leading to many applications in autonomous driving and robotics. One important yet unsolved issue for learning on point cloud is that point clouds of the same object can have significant geometric variations if generated using different procedures or captured using different sensors. These inconsistencies induce domain gaps such that neural networks trained on one domain may fail to generalize on others. A typical technique to reduce the domain gap is to perform adversarial training so that point clouds in the feature space can align. However, adversarial training is easy to fall into degenerated local minima, resulting in negative adaptation gains. Here we propose a simple yet effective method for unsupervised domain adaptation on point clouds by employing a self-supervised task of learning geometry-aware implicits, which plays two critical roles in one shot. First, the geometric information in the point clouds is preserved through the implicit representations for downstream tasks. More importantly, the domain-specific variations can be effectively learned away in the implicit space. We also propose an adaptive strategy to compute unsigned distance fields for arbitrary point clouds due to the lack of shape models in practice. When combined with a task loss, the proposed outperforms state-of-the-art unsupervised domain adaptation methods that rely on adversarial domain alignment and more complicated self-supervised tasks. Our method is evaluated on both PointDA-10 and GraspNet datasets. Code and data are available at: https://github.com/Jhonve/ImplicitPCDA.",
		"rank": 5,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 17,
		"ecc": 17,
		"use": 1,
		"authors": [
			"Yuefan Shen",
			"Yanchao Yang",
			"Mi Yan",
			"He Wang",
			"Youyi Zheng",
			"L. Guibas"
		]
	},
	{
		"uid": "S2:1f10084f5cff2512891ca56542102978143f6c41",
		"title": "Teaching Yourself: A Self-Knowledge Distillation Approach to Action Recognition",
		"doi": "10.1109/ACCESS.2021.3099856",
		"abstract": "Knowledge distillation, which is a process of transferring complex knowledge learned by a heavy network, i.e., a teacher, to a lightweight network, i.e., a student, has emerged as an effective technique for compressing neural networks. To reduce the necessity of training a large teacher network, this paper leverages the recent self-knowledge distillation approach to train a student network progressively by distilling its own knowledge without a pre-trained teacher network. Far from the existing self-knowledge distillation methods, which mainly focus on still images, our proposed Teaching Yourself is a self-knowledge distillation technique that targets at videos for human action recognition. Our proposed Teaching Yourself is not only designed as an effective lightweight network but also a high generalization capability model. In our approach, the network is able to update itself using the best past model, termed the preceding model, which is then utilized to guide the training process to update the present model. Inspired by consistency training in state-of-the-art semi-supervised learning methods, we also introduce an effective augmentation strategy to increase data diversity and improve network generalization and consistent predictions for our proposed Teaching Yourself approach. Our benchmark has been conducted on both the 3D Resnet-18 and 3D ResNet-50 backbone networks and evaluated on various standard datasets such as UCF101, HMDB51, and Kinetics400 datasets. The experimental results have shown that our teaching yourself method significantly improves the action recognition performance in terms of accuracy compared to existing supervised learning and knowledge distillation methods. We also have conducted an expensive ablation study to demonstrate that our approach mitigates overconfident predictions on dark knowledge and generates more consistent predictions in input variations of the same data point. The code is available at https://github.com/vdquang1991/Self-KD.",
		"rank": 6,
		"year": 2021,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 14,
		"ecc": 14,
		"use": 1,
		"authors": [
			"Duc-Quang Vu",
			"Ngan T. H. Le",
			"Jia-Ching Wang"
		]
	},
	{
		"uid": "S2:5513f1f1407c17c25cf3868000b88195f1df3506",
		"title": "Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",
		"abstract": "Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n",
		"rank": 7,
		"year": 2019,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 4,
		"ecc": 4,
		"use": 1,
		"authors": [
			"T. Schmid"
		]
	},
	{
		"uid": "S2:b13bbe56509af13566f1220e3e79e4d58b110885",
		"title": "Supporting Novice Teacher Enactments in the Field Class",
		"abstract": "Teacher preparation programs at NLU are developing practice-centered approaches to teacher education that entail a core set of teaching practices and intentionally designed field-learning opportunities. One addition to prior practice with this approach is the inclusion of a video coaching context where candidates receive feedback from field supervisors on their teaching videos. This study investigated candidate and supervisor perceptions of the feedback in the video coaching context affords in addition to the traditional contexts of face to face field visits and competency appraisal surveys. The findings point to a unique self-regulatory benefit to the video coaching context. They also suggest the need to carefully prepare teacher candidates and field supervisors to effectively utilize each coaching context to ensure they complement one another to best support teacher candidate learning. The Problem of Practice in Teacher Preparation Attention to teacher quality has increased nationwide, in particular, to the manner in which new teachers are prepared for the complexities of work in schools. Teachers must respond to a broad range of cultural and linguistic diversity among their students, manage new learning technologies, and effectively implement curricula to ensure students achieve dynamic learning standards. In addition to these issues of complexity, Colleges of Education have also long been challenged by the “problem of enactment” (Kennedy, 1999), which refers to the gap between what novices know about teaching and what they can actually do. This gap is often exacerbated by teacher preparation curriculum that tends to cast a wide net prioritizing exposure to a range of knowledge about teaching rather than a focused, coherent vision of the skills involved in effective teaching and what teacher candidates can realistically learn to do. The problems of complexity and enactment in learning to teach pose a tension in the design of initial teacher preparation that sets up competing demands. Preparation for the complexities of work in schools suggest a dynamic learning environment where novices explore critical issues in educational communities and among diverse learners, while preparation for fluency in executing effective instruction suggests a more stable learning environment where novices can focus on mastering specific skills. Resolving this tension presents a significant challenge that requires thoughtful balancing of exposure to varieties of teaching contexts while holding features of these contexts constant, so novices can gain fluency with the mechanics of specific instructional skills. Learning designs overly focused on one side of this equation do so at the expense of the other side. One concept emerging in the field that may address this challenge is optimal corridor of adaptability (Hammerness, Darling-Hammond, & Bransford, 2005). This concept refers to learning designs that achieve a balance between stability/consistency for efficient skill mastery and variation that reflects the complexity of teaching and allows for innovation and problem solving (Bransford, et. al, 2005; Schwartz, Bransford, & Sears, 2005). Initial preparation programs that offer teacher candidates an optimal corridor of adaptability are more likely to foster their adaptive expertise. This would entail a level of proficiency to know how to analyze and adjust instruction to learner needs/circumstances and an appreciation for complexity to ensure a willingness to continuously expand their knowledge and skill base. Indeed, initial teacher preparation needs to foster the skills and habits of mind that enable teacher candidates to learn through their teaching (Hiebert, Morris, Berk, & Jansen, 2007). Supporting Novice Teacher Enactments 2 To this end – teacher preparation at NLU embarked upon a redesign of its teacher preparation programs focusing on a core set of teaching practices and intentionally designed field-learning opportunities that allow candidates to examine and adapt core practices to the demands of diverse teaching/learning settings. The aim of this re-design is to achieve an optimal learning path for novice teachers by integrating pedagogies of enactment, reflection, and investigation (Ball and Forzani 2009) while effectively supporting adaptive mastery of core teaching practices (Ambrose, Bridges, Lovett, DiPietro, & Norman, 2010; Ericsson & Pool, 2016). Drawing from the literature in teacher education and research on learning, we articulated three design principles to guide NLU’s teacher preparation practice-based improvement efforts. 1. Teacher candidates develop a deep knowledge of teaching through a practice-centered curriculum focusing on a limited set of core practices that maintains the complexity of teaching contexts. 2. Teacher candidates grow and learn to improve through teaching opportunities that entail cycles of deliberate practice, reflective analysis of practice, and targeted feedback. 3. NLU teacher candidates acquire an adaptive stance to teaching through situated learning opportunities in prek-12 learning environments. These principles serve as a “local theory” informing ongoing design research on NLU’s practicebased teacher preparation reform efforts. A dual focus on “practice” is central to our local theory. We aim to strengthen the teacher preparation curriculum through a focus on a small number of high leverage teaching practices, ensuring that candidates have opportunities to see multiple representations and to decompose each practice into the essential features that promote learning (Grossman, Compton, Igra, Ronfeldt, Shahan, & Williamson, 2009). In concert with representation and decomposition of practice, we aim for candidates to have multiple opportunities to enact these practices. Ericisson and Pool (2016) show the importance deliberate practice across domains for the development of expertise. Similarly, teacher candidates need repeated opportunities for focused practice of specific instructional skill sets, including feedback and refinement – all essential to the learning process (Hattie & Timbeley, 2007). In this investigation, we focused on teacher candidate and field supervisor perceptions of feedback provided during the field practicum experience. In addition to traditional contexts of field visits and competency appraisal surveys, the program added a video coaching context that focused on one core instructional practice, discussion facilitation. Specifically, we investigated perceptions of the types of feedback the three different coaching contexts seem to afford to understand how best to support teacher candidates’ development of instructional skills. While focusing on the nature of feedback, the overarching aim was to inform the structure and distribution of opportunities for teacher candidates to enact core teaching practices in the preparation program. Context of the Investigation The context in which the field coaching/feedback occurred was the graduate (M.A.T.) program practicum course required prior to student teaching. Candidates enrolled in this course prior to student teaching and were placed in the classrooms where they would complete their final student teaching. The assigned field supervisor typically visited at least twice during the quarter and completed a competency appraisal on the candidates’ teaching. In the fall of 2018-19, field supervisors provided additional coaching/feedback through a newly designed video coaching approach in a cloud-based software, Livetext. While the coaching and feedback provided in the traditional contexts was broader, the focus of the video coaching was on one particular core teaching practice, discussion facilitation, Candidates were asked to submit in Livetext a video of their discussion facilitation with students in their Supporting Novice Teacher Enactments 3 practicum classroom. Field supervisors provided feedback by annotating the video in the Livetext applications. Candidates reflected on their video using the same annotation process. Research Questions How do the practice-based coaching/feedback contexts in NLU graduate teacher preparation support candidates through their enactments of core practices in field classrooms? o How do candidates perceive the feedback they receive from different field coaching contexts (face-to-face, traditional competency appraisal surveys, and video coaching)? o What feedback do supervisors feel they are able to provide through the different field coaching formats (face-to-face, traditional competency appraisal surveys, and video coaching) ? Participants Participants were recruited initially from both the B.A. and M.A.T. teacher preparation courses scheduled to be offered during the fall and winter of the 2018-2019 academic year and in which candidates have opportunities to enact core practices and in which coaching/feedback tools are used. Candidates were recruited to participate through an email during the final week of the fall and winter practicum quarters. Due to low enrollment in the undergraduate program, the focus of this report is on the graduate students’ experience. Thirty-five M.A.T. candidates participated by completing the survey on the quality of the feedback they received in the different coaching contexts in practicum. Thirty-three field supervisors completed the corresponding survey on the feedback they believed they were able to provide in the different contexts. Data Source The research questions were addressed through an electronic survey administered at the end of the practicum 2 experience. The survey contained 14 questions, including both Likert type items and openended questions. The Likert scale items asked candidates and supervisors to rate the degree to which certain types of feedback occurred in each coaching context. These feedback types were derived from Hattie and Timberley’s (2007) framework and included the following: • General encouragement",
		"rank": 8,
		"year": 2019,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Diane Salmon",
			"Kavita Kapadia Matsko",
			"Ryan J Mccarty",
			"Aleksandra Veselovsky",
			"Lisa Mozer",
			"Xue Han"
		]
	},
	{
		"uid": "S2:81394bf530f12f84f5091d0e462eb3eb11247200",
		"title": "CLASSIFYING SEGMENTED MULTITEMPORAL SAR DATA FROM AGRICULTURAL AREAS USING SUPPORT VECTOR MACHINES",
		"abstract": "In the presented study the performance of support vector machines (SVM) for classifying segmented multi-temporal SAR data is investigated. Results show that multi-temporal SAR data from an area dominated by agriculture can be successfully classified using SVM. Classification accuracy (78.2%) and degree of differentiation between land cover types is similar or better than results achieved with a decision tree classifier. A positive influence of image segmentation on classification results can be reported which varies with object size. A comparison of classification results derived on different aggregation levels shows, that a medium segment size should be preferred. It is better to work with segments that are smaller than the natural features of interest and segments that are greater than natural features should be avoided. INTRODUCTION Land cover classifications are one of the widest used applications in the field of remote sensing. Supervised classification techniques are often used in this context. Besides the chosen classification algorithm, the set of training samples as well as the input images or input features are dominating factors for the accuracy and performance of a supervised classifier (i,ii). The availability of both ground truth data and remote sensing imagery are often limited and can not be influenced by the user. In addition neither the training samples nor the selected features can be assumed to be ideal for a representative training. Against this background, the choice of an adequate classification approach is an important step in data analysis. Regions with agricultural land use are investigated in many remote sensing based land cover studies. Mono-temporal approaches can be inefficient due to great temporal variability of individual plots. In several studies the classification accuracy is increased by multi-temporal data sets (iii,iv). Thus multi-temporal applications seem more appropriate for land cover classifications. However, the availability of optical data is often limited by solar illumination and cloud cover. This is a drawback, particularly for operational monitoring systems. Hence SAR data, which are independent from these factors, are better suited for multi-temporal applications. In regard to upcoming missions with high revisit times and better spatial resolutions like TerraSAR-X, multi-temporal approaches become even more interesting. Considering such future datasets with high spatial and temporal resolution adequate classifiers are needed. Statistical methods like the maximum likelihood classification are widely known. They can achieve good results, if an adequate data distribution model is known (v). In the context of many remote sensing applications a Gaussian distribution of the data is assumed; admittedly such an assumption is not necessarily met and the approach might in many cases be inefficient. Hence non-parametric approaches, like self-learning decision trees (DT) or support vector machines (SVM) have been introduced (ii,vi,vii,viii). The concept of SVMs is well known in pattern recognition and has lead to good results in several remote sensing studies for the classification of optical data (vii,viii). In contrast to other non-parametric methods only a few studies are known that use the approach for classifying SAR data (ix,x). In several studCenter for Remote Sensing of Land Surfaces, Bonn, 28-30 September 2006 49 ies segment based classifications outperform per-pixel approaches (xi,xii). This seems particularly interesting in regard to the SAR typical noise. In addition, image segmentation can reduce the physical size of the data set and hence processing times (xiv) – a relevant issue in regard to high resolution time series. In the presented study the applicability of SVM for the classification of multi-temporal SAR data is investigated. Different levels of image segmentation are generated and classified without using any segment specific features like segment size, shape etc., to investigate the impact of generalization as conducted during the segmentation process on the SVM performance and classification accuracy. The results of the SVMs are compared to classification results achieved by self-learning decision trees. DATA SET AND PREPROCESSING The nearly flat study site is located near Bonn, in the German state North Rhine-Westphalia. The area is dominated by agriculture and characterized by typical spatial patterns and temporal variation caused by differences in the crop phenology. The field plot size varies between approximately 3 and 5 ha, with cereals and sugar beets being the main crops. A dataset of 14 images from 9 acquisition dates, containing 5 Envisat ASAR alternating polarization and 4 ERS-2 precision images was used (Table 1). Thus, the data set comprised information from varying phenological stages and different polarizations. In addition, a Landsat 5 TM image was available, which was used for the image segmentation. A map from a detailed field survey was used for generating the training and validation sample sets. An orthorectification of the Landsat image was performed, using a digital elevation model. The SAR imagery was calibrated to backscatter intensity following a common procedure. Subsequently all data sets were co-registered and an enhanced Frost filter was applied to reduce the speckle. Finally the SAR images were orthorectified using a digital elevation model, orbit parameters and the corrected Landsat image as reference data set. Table 1: Multi-temporal SAR Data set Sensor Date Track / Swath Polarization Orbit ASAR 12-Apr-05 6208 HH / HV asc ERS-2 21-Apr-05 337 VV des ERS-2 26-May-05 337 VV des ERS-2 30-Jun-05 337 VV des ASAR 13-Jul-05 3029 HH / HV asc ASAR 22-Jul-05 7158 HH / HV asc ERS-2 4-Aug-05 337 VV des ASAR 14-Aug-05 2487 HH / HV asc ASAR 18-Sep-05 2487 HH / HV asc Although several segmentation methods have been developed for SAR data, segmentation is still difficult due to the speckle. Outlines derived from optical data seem more appropriate (xv). Hence a segmentation of the Landsat image was performed. Afterwards the segment outlines were transferred onto the SAR data set. Several techniques for image segmentation of optical data sets exist (xvi,xvii,xviii). Region-growing methods assume that pixels of the same natural feature have a certain spectral homogeneity. In this study the commonly available region-growing approach by Baatz and Schäpe (xvii) was used. In the initial phase of the process, pixels are handled as individual segments, which are iteratively merged into larger segments. Candidate pairs of adjacent segments are found by local mutual best fitting. The difference between the heterogeneity of a possible new segment compared to that of its two constituent segments is used as a stopping criterion for the region-growing. If it exceeds a user defined value, the growing process stops. In the presented Proceedings of the 2 Workshop of the EARSeL SIG on Land Use and Land Cover 50 study only the spectral information was used to estimate the segments’ heterogeneity. In doing so the segments were not constrained to any pre-defined shape. To investigate the impact of the segmentation on classification accuracy three different image segmentations were generated. By computing each of the three aggregation levels (scale 1-3) separately, all segmentations were independent from the prior result. The average segment size of scale 1 was 10 pixels (~0.9 ha), of scale 2 25(~2.2 ha) and 65 of scale 3 (~5.8 ha). Figure 1: Landsat 5 TM (4,3,2) and multi-temporal SAR images with segment outlines from TM data. Average segment size 10 pixels, 25 pixels and 65 pixels (from left to right). METHODS SVM delineate two classes by fitting an optimal separating hyperplane to the multidimensional feature space. This optimization bases on structural risk minimization and tries to maximize the margin between the hyperplane and the closest training data points, the socalled support vectors. Thus, SVM only consider training samples close to the class boundary and might work well with small sample sets (xix). For linearly not separable classes the input data are mapped into a high dimensional space wherein the newly spread data point distribution enables the fitting of a linear hyperplane. A detailed description on the general concept of SVM is given in Vapnik (xx) and Burges (xxi). Comprehensive introductions in a remote sensing context are given by Huang et al. (vii) or Foody & Mathur (viii). The binary nature of the SVM requires a useful strategy to solve a multi-class problem (viii). Two main approaches exist: the one-against-one strategy (OAO) and the one-against-all strategy (OAA). OAO applies a set of individual classifiers to all possible pairs of classes and performs a majority vote to assign the winning class. In the case of OAA, a set of binary classifiers is trained to separate each class from the rest. The maximum decision value determines the final class label. In this work, the OAO strategy was performed. A Gauss kernel was used for the training of the SVM. The training parameters were set following the leaveone-out cross validation approach Looms by Lee & Lin (xxii). For the generation of training and validation data sets an extensive ground truth campaign was conducted in summer 2005. A training data set can be generated in different ways: e.g. simple random sampling, systematic sampling or stratified random sampling. Using the first Center for Remote Sensing of Land Surfaces, Bonn, 28-30 September 2006 51 method, each sample has an equal chance to be selected, the systematic approach selects samples with an equal interval over the study area. Stratified random sampling combines a priori knowledge about a study area – like land cover information – with the simple random sampling approach (xxiii). Using land cover classes as a priori knowledge, the stratified random sampling guarantees, that all classes are included in the sample set. I",
		"rank": 9,
		"year": 2007,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 1,
		"ecc": 1,
		"use": 1,
		"authors": [
			"B. Waske",
			"S. Schiefer"
		]
	},
	{
		"uid": "S2:7e41f8236169941916b5f76330b736d99c20c3c2",
		"title": "Effects of Job Satisfaction on Employee ‟ s Performance",
		"abstract": "The research reported in this thesis was on “Effects of job satisfaction on employee‟s performance”. The purpose of research was to study the various factors that have been significant in determining employee performance and their importance with respect to the employees of Pakistan. The secondary data was collected by Internet and also from the material printed by different Scholars from all over the world. The primary data was gathered by floating questionnaires in Hassan leather industry Sheikhupura, Pakistan. SPSS was applied to analyze the gathered information through running Correlation tests on the variables in the study. The reliability of the data was measured with the help of the Cronbach‟s Alpha value which too was calculated using the SPSS software. The findings provided an insight and estimation towards the impact of financial and Non-Financial reward system on employee‟s performance. All the variables have been found to be significant in determining employee‟s performance. The paper elaborates the characteristics and implications regarding the variables in detail to give a picture and a base for further research. © 2018 Elixir All rights reserved. Elixir Org. Behaviour 118 (2018) 50899-50908 Organizational Behaviour Available online at www.elixirpublishers.com (Elixir International Journal) Mustafa Amir Zia and Haziq Sheikh / Elixir Org. Behaviour 118 (2018) 50899-50908 50900 1.3 With respect to the world In the recent years it has come to light a growing interest in the study of job satisfaction amongst the employees working in a wide range of companies in the U.S.A., Britain and other developed countries. In Europe numerous determinants impact satisfaction of employees, however some EU-overviews show the same result: A correlation of work satisfaction (measured on a scale from 1 \"not in any way fulfilled\" to 4 \"exceptionally fulfilled\") in Europe shows that the normal level of satisfaction in most nations is high. In this case it could be distinguished that the level of work satisfaction differs at national level. Denmark is the nation with the most fulfilled workers in normal. Second is the United Kingdom, accompanied by Norway, Switzerland and Austria. On the other hand, there are numerous Eastern European nations with a low level of work satisfaction in normal. The nations with the most reduced work satisfaction are Lithuania, Bulgaria, Romania, and Greece and at last Turkey, which is extremely striking, it has by far the most noticeably bad results. It is quite evident that the higher the welfare of a nation is, the higher it`s G.D.P (Gross domestic product) is and the more improved and developed it is, the higher is the job satisfaction in average. In a few nations it is not a surprise that the workers are fulfilled by their employment due to their sound economy and low unemployed rates. 1.4 With respect to Pakistan The relationship between job satisfaction and employee performance at work has been one of the broadly investigated concepts in the field of management in connection to distinctive professions, however in Pakistan not many studies have investigated this notion. There have been a few studies but they have shown various entities that are a cause of employee dissatisfaction. As Pakistan is an under developed country and has a lot of crisis so job satisfaction is seen very little with the people of Pakistan which is due to various reasons social, economic, political etc. As Pakistan faces political instability and other problems hence it is a common practice amongst employees to switch jobs etc. Some companies and organizations try to take measures to facilitate the employees but they become restricted to a certain limit because of the rising economic crisis in the country. 1.5 Research Question What is the effect of job satisfaction on employee performance? Significance of the study There are many factors that are a cause of job satisfaction which may include benefits, coworker relationships, work conditions, job security, work flexibility, recognition, importance, workload, work content, salary, work life balance, learning, autonomy, skills, challenges, prestige, advancement, supervision, commute and may more. Job satisfaction is exceptionally critical for an association on the grounds that it helps representatives to perform better or perform poor, in light of the fact that high work satisfactions heads specialist with better execution which is an exceptional sign for the association and low work satisfaction is disturbing for the organization, it is imperative for the organization and manager to know how fulfilled are the workers with their employment, it‟s extremely significant for any organization to think about the issue they confront and after that taking active measures to take care of the issue.This issue has an extraordinary essentialness since for each organization workers are the human asset and each organization need to supervise it well in light of the fact that they are one of the key members of a company. The fundamental driver of the exploration is to uncover the satisfaction level of the worker in the organization, economic slowdown and inflation makes workers more anxiety full so this makes them disappointed with the work due to low compensation so this research will help in uncovering some primary variables which might influence them. Literature Review The literature review is a representation or the presentation of previously available literature by researchers. The main aim of this research is to present a deep understanding of the employee performance. The factors or entities that are a cause of satisfaction and are the motivators that fuels the employees to perform the best they can and produce the maximum output and do their best effort. There have been studies in the past that have shown that management system tools, leadership styles and job designs are a cause of influence to the employee performance. Pugno & Depedri (2009) Reported that job performance is initiate to be positively related to job satisfaction. They also pointed out that the key elements that are of great importance are the intrinsic motivations and also self-esteem which help in explaining both job satisfaction as well as job performance. And suggest that the employers can find other means like friendly actions, other than only going for incentives to control or enhance performance of the employees. M.D. Pushpakumani (2008) Work showed that if the employee or worker is satisfied and happy he will be a productive worker. A satisfied workforce will lead to a friendly and healthy atmosphere in the organization which will help them perform at their best. This study also highlights the influences of entities like age, sex and experience of the employees on the level of job satisfaction and also evaluates that which rewards may they be intrinsic or may they be extrinsic determine the employees job satisfaction. Aftab & Idrees (2012) Reported in their study about job satisfaction and its link to employee performance through highlighting the banking sector of Pakistan and have opted two models to further define the relationship between these two. Which were the Herzberg‟s two-factor theory and Rolebased performance scale. They also used demographic factors like age, gender, expenses and salary to show the background of the respondents and concluded that a positive relationship exists between job satisfaction and job performance. Ryan & Deci (2000) they presented the view point that employees who are motivated more are self driven with contrast to the employees who are less motivated hence the variation in performance comes to measure in these scenario‟s also. Employee performance is also termed as job performance it is what he or she contributes to the company and on which scale it also refers to the means of measure of work done by the employee which can be either good or bad, poor or worse, high or low. Job performance can be influenced by the individual‟s ability to use emotions and perform. Many employees use their emotions either positive or negative to gain advantage in order to improve performance. They also talked about the high role of performance and its influence on the work performed by the employees. Mustafa Amir Zia and Haziq Sheikh / Elixir Org. Behaviour 118 (2018) 50899-50908 50901 Bruce & Trence (1997) Job satisfaction in terms of power can be seen in two dimensions horizontally as well as vertically in this case you give the employees the opportunity to participate and give suggestions that not only is a means of motivation but also gives the employees the feeling of being a part of the organization or company. This gives them a sense of motivation and ensures them the authority to make decisions. This study works around both the vertical effects and the horizontal effects on the performance of the employees which can be highlighted through their output, productivity as well as the existence of satisfaction in them. The results forecasted through this study presented that power has a significant role or it can be said that it has a noticeable effect on employee satisfaction. Heilman & Block (1992) conjugated the view that the means to measure job satisfaction is through three items 1. Like to work in a company 2. Like the job 3. Satisfied with the job, responses were taken by the employees on these items. In order to measure job satisfaction. To insure what are the basic means that allows the employees to work and perform better basically the motivators present. Cf. Lam et. al (2002) Employee turnover basically refers to the rate, number or the percentage of employees who leave a company or organization and are replaced by new ones this phenomenon is termed as employee turnover. This article highlights the negative relationship between the intention to leave and job satisfaction, and also says that these demand consideration as the intention to leave is the actual entity tha",
		"rank": 10,
		"year": 2018,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"Mustafa Amir Zia",
			"Haziq Sheikh"
		]
	},
	{
		"uid": "S2:6327a26d10d2226aeef64b42d3b416a947b735cb",
		"title": "Delayed Consequence Delivery 1 Running head: DELAYED CONSEQUENCE DELIVERY IN APPLIED SETTING S Delayed Consequence Delivery in Applied Settings: Examining Unsignaled Feed back Delays in Visual- Visual Matching Tasks",
		"abstract": "A reinforcer is a stimulus presented closely following a response which res ults in a future increase in frequency of that response. In an ideal applied setting, a stimulus should b e presented immediately. But, it may not always be possible to present stimuli immediately in the a pplied setting. The current study compares the effects of consequence delivery delivered immediately to consequences delayed by five seconds. Three typically developing males, ages 25-30 participated in vi sualisual matching tasks using arbitrary stimuli. Half of the stimuli were always presented with im mediate consequences, while the other half was only followed by delayed consequences. Participants quickly achiev ed mastery of the matching tasks when consequences were immediate, but did not achieve quick master y (within five sessions) when consequences were delayed by five seconds. These results support the ac cept d wisdom that immediate consequences are always best for skill acquisition and that high trea tment integrity is crucial. Additionally, it provides an example of adapting basic research practice s for use in applied contexts. Delayed Consequence Delivery 5 Delayed Consequence Delivery in Applied Settings: Examining Unsignaled Feed back Delays in VisualVisual Matching Tasks Individuals diagnosed with Autism Spectrum Disorders (ASD) often present deficiencies in responding in three areas of typical human development (DSM-IV, 2000). Firs t, these individuals do not respond to social stimuli with the same regularity that their typical pee rs might. Second, it is often difficult or impossible for people diagnosed with ASDs to communicate effect ively without environmental assistance in the form of picture exchange systems or speech-generating devices. Finally, individuals with ASDs often demonstrate highly restricted or repetiti v interests in specific kinds of stimulation, and this interest often leads to self-stimulatory behavior such as motor stereotypy. When taken together, these three challenges result in individuals wh o may only learn new functional skills though very precisely controlled teaching strategies. Clinicians have found great success when using the application of the principles of behavior analysis in the treatment of t hese individuals, in particular the use of the principle of reinforcement. Properly defined, reinforcement occurs when a stimulus presented or remove d sh rtly following a response results in an increase in the probability of the occurrenc e of that response (Skinner, 1953). Thus, clinicians have used stimuli demonstrated to have a reinforcing effect for the behavior of individuals with whom they are working to increase desirable, functional behavior. Though this principle is widely used, there is one aspect of the definition which does not escape some measure of subj ctivity. That is, a reinforcing stimulus must be changed closely following the targeted response. At present, the conventional wisdom among clinicians has determined that it is best if closely following was equal to immediately following for the purposes of effective behavioral shaping. However, clinical prac tice is typically not automated. Teachers are not as precise with regard s to the time of delivery as food hoppers connected to mechanical devices or specially calibrated timing switc hes. Consequences may often be delivered as immediately as possible, though at other times it may be pre ceded by a time delay. This time Delayed Consequence Delivery 6 delay could be as short as 1 or 2 seconds, or could be as long as 10 seconds or more in som e treatment settings. When faced with a population which has difficulty attending cont inuously to instruction and may be actively engaging in self-stimulatory behavior, such delays may interfere with learning. Lattal (2010), presented an extensive overview of how delay of reinfo cement has been studied with respect to its effects on the operant behavior of animal subjects . In this overview, the author attempts to answer three main questions. First, can the effects of the temporal relation between response and reinforcer be isolated from other environmental changes which accom pany delays? Lattal suggests that they can, as long as the delays are relatively short, on the order of a few seconds. If delayed longer than a few seconds the effect becomes indistinguishable from an absenc e of response-reinforcer dependency. Second, what effect do delays have on operant behavior? Researc h, a noted by Lattal, has found that delays can reduce the effectiveness of response shaping, can induce response differentiation in the absence of effective response training, and can reduce the accuracy a nd rate of responses already established through immediate reinforcement. Third, how can we effectiv ely interpret delay of reinforcement effects? Lattal emphasizes that delay to rein fo cement both imposes contingencies and is imposed by contingencies. Because it is hard to separate temporal contiguity f rom he effects of a reinforcement schedule, it may be best to consider delay to reinforce ment in terms of how it affects behavior. Lattal also describes several variations in how delay to reinf orcement procedures can be implemented. Delays can be either signaled or unsignaled. If a delay is un signaled, there is no stimulus change indicating that a delay is going to take place. Also, delays can be eit her fixed or variable. A fixed delay remains constant throughout a given study, and variable delays are po tentially different for each trial. Finally, delays can be resetting or nonresetting. A resetting dela y is one in which the delay period starts over if any target response occurs during the prescribed delay. N onresetting delays terminate at a predetermined interval regardless of subject responding during that s ame period. Delayed Consequence Delivery 7 Throughout the basic behavioral literature, assessments have been conduct ed with animals to assess some of the effects that delays of reinforcement have on behavi or (Arbuckle & Lattal, 1988; Keeley, Feola, & Lattal, 2007; Odom, Ward, Burke, & Barnes, 2006; Pierce, Ha nford, & Zimmerman, 1972; Royalty, Williams, & Fantino, 1987; Wilkenfield, Nickel, Blakely, & P oling, 1993). In particular, Williams (1976) examined the effects of delayed reinforcer deliver y on the lever-pressing behavior of pigeon subjects. In this study, pecks were reinforced according to a variab le-interval schedule. Then, a delay-of-reinforcement contingency was added onto the schedule, in which a period of un-signaled delay was added following the target response. Behavior was reduced signifi cantly in strength even with just a 3-s delay between response and consequence. Additionally, several studies using animal subjects have incorporat ed multiple schedules and differing values of fixed-time components to examine the effects of res ponding when non-contiguous with consequence delivery. In Sizemore and Lattal (1977), the authors used a yoke -control procedure to assess the differences between response-dependent reinforcement sche dule and response-independent reinforcement schedules. Their data suggest that differences in r ates of responding by pigeons were not entirely due to differences in reinforcer distribution, but were rathe influenced by the non-contiguous nature of their reinforcer delivery. In Sizemore and Lattal (1978), the authors again used tandem variable-inte rval fix d-time schedules with pigeons. The duration of the fixed-time component of the schedul e was varied to assess the effects of unsignaled delays. As the length of the delay was increase d, the pigeons’ response rates decreased. This suggests a weakening of the response reinforcer con tingency and the contiguity between the two was also weakened. Animal studies have shown that the less an absolute delay between res po se and consequences the less the variability of responding for rat and pigeon subjects. If hese studies were to be replicated with human participants, behavior analysts could use the results to make more specific claims about the Delayed Consequence Delivery 8 principle of reinforcement with regards to its application to t he applied setting. The results would add to the growing field of translational research which utilizes the concl usions derived through basic research to the clinical problems of applied behavior analysis, particularly those regarding losses of treatment integrity. One such study, Okouchi (2009), used undergraduate students as partic ipants, and reinforced specific response sequences using a point economy. This study suggested that typically developing adults should be able to acquire a target response under a sizable consequence delay of 1 0 s or more. Treatment Integrity is defined as the extent to which an independent var iable is implemented as intended (Peterson, Homer, & Wonderlich, 1982). There are many ways in which the trea ment integrity of clinical intervention may be less than adequate in applied settings whe re the diagnosis and treatment of problem behavior is the primary goal of practitioners. Behavior programs m ay not be correctly implemented by all direct care staff, operational definitions may be too ge neral, consequences may be delivered inconsistently for target behavior, data collected by direc t care staff may be unreliable, and staff training insufficient for optimal performance. Additionally, without c onsistent supervision and feedback the behavior of caretakers may not conform to contingencies prescribed by be havioral specialists (Vollmer, Sloman, & Pipkin, 2008). One crucial aspect of maintaining high treatment integrity is to en sur the prompt and reliable delivery of reinforcers, as prescribed in behavioral guidelines. Inconsist e cy of reinforcer delivery may be a particularly troubling threat to validity in treatment scenarios in which adaptive skills and alternative responses to problem behavior are being taught to a developmentall",
		"rank": 11,
		"year": 2013,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"C. Moore"
		]
	},
	{
		"uid": "S2:17a2cf09fa935edfb02764b72a8a47112b7361d9",
		"title": "Delayed Consequence Delivery in Applied Settings: Examining Unsignaled Feedback Delays in Visual- Visual Matching Tasks",
		"abstract": "A reinforcer is a stimulus presented closely following a response which res ults in a future increase in frequency of that response. In an ideal applied setting, a stimulus should b e presented immediately. But, it may not always be possible to present stimuli immediately in the a pplied setting. The current study compares the effects of consequence delivery delivered immediately to consequences delayed by five seconds. Three typically developing males, ages 25-30 participated in vi sualisual matching tasks using arbitrary stimuli. Half of the stimuli were always presented with im mediate consequences, while the other half was only followed by delayed consequences. Participants quickly achiev ed mastery of the matching tasks when consequences were immediate, but did not achieve quick master y (within five sessions) when consequences were delayed by five seconds. These results support the ac cept d wisdom that immediate consequences are always best for skill acquisition and that high trea tment integrity is crucial. Additionally, it provides an example of adapting basic research practice s for use in applied contexts. Delayed Consequence Delivery 5 Delayed Consequence Delivery in Applied Settings: Examining Unsignaled Feed back Delays in VisualVisual Matching Tasks Individuals diagnosed with Autism Spectrum Disorders (ASD) often present deficiencies in responding in three areas of typical human development (DSM-IV, 2000). Firs t, these individuals do not respond to social stimuli with the same regularity that their typical pee rs might. Second, it is often difficult or impossible for people diagnosed with ASDs to communicate effect ively without environmental assistance in the form of picture exchange systems or speech-generating devices. Finally, individuals with ASDs often demonstrate highly restricted or repetiti v interests in specific kinds of stimulation, and this interest often leads to self-stimulatory behavior such as motor stereotypy. When taken together, these three challenges result in individuals wh o may only learn new functional skills though very precisely controlled teaching strategies. Clinicians have found great success when using the application of the principles of behavior analysis in the treatment of t hese individuals, in particular the use of the principle of reinforcement. Properly defined, reinforcement occurs when a stimulus presented or remove d sh rtly following a response results in an increase in the probability of the occurrenc e of that response (Skinner, 1953). Thus, clinicians have used stimuli demonstrated to have a reinforcing effect for the behavior of individuals with whom they are working to increase desirable, functional behavior. Though this principle is widely used, there is one aspect of the definition which does not escape some measure of subj ctivity. That is, a reinforcing stimulus must be changed closely following the targeted response. At present, the conventional wisdom among clinicians has determined that it is best if closely following was equal to immediately following for the purposes of effective behavioral shaping. However, clinical prac tice is typically not automated. Teachers are not as precise with regard s to the time of delivery as food hoppers connected to mechanical devices or specially calibrated timing switc hes. Consequences may often be delivered as immediately as possible, though at other times it may be pre ceded by a time delay. This time Delayed Consequence Delivery 6 delay could be as short as 1 or 2 seconds, or could be as long as 10 seconds or more in som e treatment settings. When faced with a population which has difficulty attending cont inuously to instruction and may be actively engaging in self-stimulatory behavior, such delays may interfere with learning. Lattal (2010), presented an extensive overview of how delay of reinfo cement has been studied with respect to its effects on the operant behavior of animal subjects . In this overview, the author attempts to answer three main questions. First, can the effects of the temporal relation between response and reinforcer be isolated from other environmental changes which accom pany delays? Lattal suggests that they can, as long as the delays are relatively short, on the order of a few seconds. If delayed longer than a few seconds the effect becomes indistinguishable from an absenc e of response-reinforcer dependency. Second, what effect do delays have on operant behavior? Researc h, a noted by Lattal, has found that delays can reduce the effectiveness of response shaping, can induce response differentiation in the absence of effective response training, and can reduce the accuracy a nd rate of responses already established through immediate reinforcement. Third, how can we effectiv ely interpret delay of reinforcement effects? Lattal emphasizes that delay to rein fo cement both imposes contingencies and is imposed by contingencies. Because it is hard to separate temporal contiguity f rom he effects of a reinforcement schedule, it may be best to consider delay to reinforce ment in terms of how it affects behavior. Lattal also describes several variations in how delay to reinf orcement procedures can be implemented. Delays can be either signaled or unsignaled. If a delay is un signaled, there is no stimulus change indicating that a delay is going to take place. Also, delays can be eit her fixed or variable. A fixed delay remains constant throughout a given study, and variable delays are po tentially different for each trial. Finally, delays can be resetting or nonresetting. A resetting dela y is one in which the delay period starts over if any target response occurs during the prescribed delay. N onresetting delays terminate at a predetermined interval regardless of subject responding during that s ame period. Delayed Consequence Delivery 7 Throughout the basic behavioral literature, assessments have been conduct ed with animals to assess some of the effects that delays of reinforcement have on behavi or (Arbuckle & Lattal, 1988; Keeley, Feola, & Lattal, 2007; Odom, Ward, Burke, & Barnes, 2006; Pierce, Ha nford, & Zimmerman, 1972; Royalty, Williams, & Fantino, 1987; Wilkenfield, Nickel, Blakely, & P oling, 1993). In particular, Williams (1976) examined the effects of delayed reinforcer deliver y on the lever-pressing behavior of pigeon subjects. In this study, pecks were reinforced according to a variab le-interval schedule. Then, a delay-of-reinforcement contingency was added onto the schedule, in which a period of un-signaled delay was added following the target response. Behavior was reduced signifi cantly in strength even with just a 3-s delay between response and consequence. Additionally, several studies using animal subjects have incorporat ed multiple schedules and differing values of fixed-time components to examine the effects of res ponding when non-contiguous with consequence delivery. In Sizemore and Lattal (1977), the authors used a yoke -control procedure to assess the differences between response-dependent reinforcement sche dule and response-independent reinforcement schedules. Their data suggest that differences in r ates of responding by pigeons were not entirely due to differences in reinforcer distribution, but were rathe influenced by the non-contiguous nature of their reinforcer delivery. In Sizemore and Lattal (1978), the authors again used tandem variable-inte rval fix d-time schedules with pigeons. The duration of the fixed-time component of the schedul e was varied to assess the effects of unsignaled delays. As the length of the delay was increase d, the pigeons’ response rates decreased. This suggests a weakening of the response reinforcer con tingency and the contiguity between the two was also weakened. Animal studies have shown that the less an absolute delay between res po se and consequences the less the variability of responding for rat and pigeon subjects. If hese studies were to be replicated with human participants, behavior analysts could use the results to make more specific claims about the Delayed Consequence Delivery 8 principle of reinforcement with regards to its application to t he applied setting. The results would add to the growing field of translational research which utilizes the concl usions derived through basic research to the clinical problems of applied behavior analysis, particularly those regarding losses of treatment integrity. One such study, Okouchi (2009), used undergraduate students as partic ipants, and reinforced specific response sequences using a point economy. This study suggested that typically developing adults should be able to acquire a target response under a sizable consequence delay of 1 0 s or more. Treatment Integrity is defined as the extent to which an independent var iable is implemented as intended (Peterson, Homer, & Wonderlich, 1982). There are many ways in which the trea ment integrity of clinical intervention may be less than adequate in applied settings whe re the diagnosis and treatment of problem behavior is the primary goal of practitioners. Behavior programs m ay not be correctly implemented by all direct care staff, operational definitions may be too ge neral, consequences may be delivered inconsistently for target behavior, data collected by direc t care staff may be unreliable, and staff training insufficient for optimal performance. Additionally, without c onsistent supervision and feedback the behavior of caretakers may not conform to contingencies prescribed by be havioral specialists (Vollmer, Sloman, & Pipkin, 2008). One crucial aspect of maintaining high treatment integrity is to en sur the prompt and reliable delivery of reinforcers, as prescribed in behavioral guidelines. Inconsist e cy of reinforcer delivery may be a particularly troubling threat to validity in treatment scenarios in which adaptive skills and alternative responses to problem behavior are being taught to a developmentall",
		"rank": 12,
		"year": 2010,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"C. Moore"
		]
	},
	{
		"uid": "S2:9159b45aa84acd721a6b774ad391c3fd12d8e71f",
		"title": "Från storsvindel till småfiffel - teman i internationell ekobrottsforskning",
		"abstract": "123 English Summary From major swindles to minor fiddles—themes in international white-collar crime research Authors: Sven-Ake Lindgren and Christer Theandersson Published by: The National Council for Crime Prevention (BRA) P.O. Box 1386 SE-111 93 Stockholm, Sweden Internet: www.bra.se Reference: BRA-Report 2000: 23 ISSN 1100-6676, ISBN 91-38-31719-2 Available in Swedish from: Fritzes kundservice SE-106 47 Stockholm, Sweden This report presents and discusses international research into economic crime. The presentation includes concrete research findings as well as both attendant theoretical perspectives and other prominent themes. The principle focus is directed at internationally recognised, empirically based research, primarily produced in the Anglo-Saxon countries. Terms and definitions The first chapter examines definitional and terminological issues. Concepts such as white-collar crime, corporate crime, occupational crime, organisational crime, economic crime and so forth suggest that this is a multi-facetted area of research. Besides the history of such conceptualisations, the chapter discusses a number of contentious questions concerning: • Actor status—are economic offences committed by individuals (physical persons) or organisations (legal persons) or both? • The crime concept—should research and counter measures be limited purely to acts that contravene the criminal law, or should acts that are sanctioned in accordance with civil and administrative legislation also be included? • The distinction between white-collar (economic) crime and organised crime —is it theoretically or empirically meaningful to separate these phenomena given the complex nature of the world we live in? • The relationship between definitions and research objectives and traditions —is it reasonable to talk of “definitional confusion” without taking into consideration the way different questions and interests are rooted in differing disciplinary traditions, philosophical perspectives, theoretical assumptions and favoured methodologies? Who is the economic offender? The first point to be made is that economic offences are not solely committed by high status (white-collar) individuals in the course of their occupation. A considerable number of such offences are committed by individuals with a middle class background as well as by persons from the lower (blue-collar) classes. The perpetrators are most commonly men. The gender pattern of economic crime is very similar to that characteristic of what is often termed traditional offending. It should nonetheless be noted that economic crime is generally unlike conventional street crime. The proportion of perpetrators drawn from ethnic minorities, the poor or the unemployed, for example, are considerably smaller among economic offenders. One of the fundamental questions relating to the assessment of research into the character of economic offending is that of the research data and methodology employed. As a rule of thumb, the greater the proportion of court data and material from registers maintained by state agencies, the greater the proportion of unqualified economic offences, and the smaller the proportion of perpetrators conforming to the classical (whitecollar)definitions. Causes and conditions Causality is one of the most widespread and fertile themes to be found in international research into economic crime. Causal relations are demonstrated at the levels of the individual, the organisation and society at large. Reference is often made to personality characteristics, social learning and rational choice. At the level of the organisation, the factors deemed to be of interest in this context include the size of an organisation, the existence of subcultures, sectorspecific modes of conduct and the interplay between organisations and supervisory agencies etc. Conditions referred to at the societal level relate to structural and cultural factors such as social class divisions, social desorganisation, attitudes based on gender roles, and the principles of profit and competition inherent in the economic system. There is a lack of integrated theories combining factors from different analytical levels in a more holistic model. Up to now, the ambition to explain the causes of economic crime has been dominated by the search for a single fundamental and decisive factor. Victims and harm The issues of the victims of economic crime and the harm caused by this type of offence have long been accorded a secondary role in economic crime research. It wasn’t until the 1980s and 1990s that interest in victimological issues finally began to gain ground. The term victimology refers to studies where the situation of the victim before, during and after the criminal act forms the basis for different problem formulations and approaches. In order to differentiate categories of victims and different sorts of victimization at the analytical level, distinctions are made between direct and indirect victims and between primary, secondary and tertiary harm (affecting individuals, affecting companies and institutions or affecting more complex and diffuse phenomena such as the physical environment or the welfare state). The victimological discussion contains elements and arguments reminiscent of those formulated in the debate on the limits of the economic crime concept. Is one a victim of economic crime only where one has been exposed to the intentional acts of individuals? One counter argument contends that harm is harm, irrespective of whether it is caused by accident, neglect or a premeditated criminal act, irrespective of whether it is of a physical, sexual, psychological or economic nature, and irrespective of whether it is caused by individuals, organisations, companies, governments or by society at large. Counter measures to reduce economic crime The term counter measures covers many different activities whose objective is to prevent, reduce or do away with economic crime. The questions addressed in relation to this theme have been and remain the object of deep differences of opinion and conflicting perceptions. The demands and arguments of the actors involved are as a rule based on the principle either of effectiveness or of justice. But opinions also differ when it comes to the interpretation of findings from empirically based research. A summary of the state of the research in relation to a number of current issues finds that: • There is evidence in support of the thesis that businesses occupy a privileged legal position in relation to private individuals, but this does not mean that all economic crimes are met with only mild sanctions. • Research that has tested hypotheses relating to the class-based partiality of the judicial system has produced contradictory findings. Some results indicate the existence of a positive correlation between high social status and mild sanctions, while other results show quite the reverse, i.e. the probability of receiving a more punitive sanction increases with the professional status of the accused. Other findings indicate that that the courts deal with economic offenders on the basis of the same criteria that are applied in the context of more traditional crime. • The view taken of economic crime has become more serious since the mid 1970s (post-Watergate), and the 1980s and 1990s provide several concrete examples of both more stringent intervention practices and more punitive sentencing. • A number of researchers contend that proceeding through the civil courts and making use of administrative sanctions are referable to the employment of criminal sanctions—arguments and models favouring a “self-regulatory” logic have become increasingly prominent during the 1980s and 1990s. • Several studies have shown that negative publicity constitutes a much feared form of sanction among the owners and directors of businesses. • Two lines of reasoning have evolved to counter the theoretical and ideological shift towards arguments for deregulation, the ideal of self-regulation and service focused agency supervision as witnessed over the last decades. The first of these arguments emphasises the political-ideological basis for the shift as well as providing information on the shortcomings and imperfections of the neo-liberal strategy. The other is based on the idea of “just deserts” and contends that punitive sanctions are a legitimate means (even in relation to corporate offending) of demonstrating the moral censure of society. The need for interdisciplinary research The report concludes with a number of reflections. One conclusion is that an already complex research object is not made any less heterogeneous by a research community which approaches this complexity using differing assumptions and employing distinctive theoretical perspectives and conceptualisations. At the same time, the point is made that this variation in the use of conceptualisations and the choice of perspectives is not unique to the study of economic crime. Instead it mirrors the state of contemporary social science research. With regard to current lines of development within research, the continued strength of the Sutherland tradition is noted, despite the existence of research findings which contradict the classical definitions and hypotheses in important respects, and despite the fact that the business community of today is quite different from that which existed when these hypotheses were first formulated during the 1940s. It is further noted that the field still lacks genuinely interdisciplinary research efforts. The Yale Studies on White-Collar Crime are presented as a good example of a productive research environment characterised by a certain degree of interdisciplinary cooperation. In conclusion, attention is called to the need to replicate classical studies and for interdisciplinary, comparative research at the national level.",
		"rank": 13,
		"year": 2001,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 5,
		"ecc": 5,
		"use": 1,
		"authors": [
			"S. Lindgren",
			"Christer Theandersson"
		]
	},
	{
		"uid": "S2:237e79cbb565926c4fef56dd5232268d0db45449",
		"title": "Benchmarking in Heatlh Care Foodservice Operations",
		"abstract": "TEXT: \"Ready-Set-Cook\" was an interactive program designed to offer hands-on food preparation experience to university freshmen and sophomore student-athletes. Anecdotes of poor nutrition practices and expressed interest in sports nutrition were the catalyst for this programming. Designing the program was a cooperative effort that included staff from the university health center, athletic academic services and training table facility. Programs took place over 5 sessions, with 242 athletes and 8 coaches attending. After a brief discussion about optimal nutrition for peak athletic performance, participants were assigned to one of 6 cook stations supplied with a microwave oven, grill, toaster oven or blender. Participants used ingredients provided to prepare a snack, entrre and/or side dishes. Each group sampled the prepared food and completed a work sheet describing the recipe, ingredients used, degree of difficulty, and how to incorporate the food items into a healthful meal plan. Information from each group's work sheet was presented to the group at large. Written evaluations completed by 192 athletes (79% response) suggest they found the program to be useful, informative and fun. Presenters identified the interactive format as a program strengtl~ however it necessitated limiting group size. Athletes and program leaders agreed that the recipes were simple to prepare and using more difficult recipes would improve the program. TITLE: DECREASING ON THE JOB INJURIES AT A LARGE MILITARY HOSPITAL FOODSERV/CE DEPARTMENT AUTHOR (S): J. Reagan, MS, MHA, R.D., MAd, U.S. Army; B. Forman, MS, R.D., COL, U.S. Army; A. Briscoe, LT, U.S. Army; T. Olson, LT, U.S. Army, Walter Reed Army Medical Center, Washington, D.C. LEARNING OUTCOME: To provide resources for decreasing on the job injuries in healthcare organizations. ABSTRACT TEXT: Using a FOCUS-PDCA (find a problem, organize a team, clarify current knowledge, understand variables, and select the process to improve) approach, we organized a process-action team to decrease on the job injuries of foodservice employees. Over the past four years, 105 injuries occurred in the kitchen with the majority of injuries in the dish room. Most injuries were contusions or back injuries. Total lost time from injuries included 120 lost days from 19 injuries. The process action team concentrated on four areas of improvement: equipment, environment, employee involvement, and management processes. The team initiated a safety shoe program to decrease slips and falls. The team made improvements to the environment by identifying and eliminating physical and procedural hazards while also improving workflow in the kitchen. Employees developed a safety motto \"Nutrition Care Directorate U B Safe\" and safety contests for each section. Management focused on employee training incorporating a safety topic of the month with supervisor focused monthly training and new employee safety training. Employee safety certification was also developed. This program is the starting point for decreasing accidents, which will potentially reduce injuries, improve employee morale, and decrease workman's cumpensation insurance premiums. TITLE: A STANDARDIZED METHOD FOR BENCHMARKING CLINICALTEXT: Using a FOCUS-PDCA (find a problem, organize a team, clarify current knowledge, understand variables, and select the process to improve) approach, we organized a process-action team to decrease on the job injuries of foodservice employees. Over the past four years, 105 injuries occurred in the kitchen with the majority of injuries in the dish room. Most injuries were contusions or back injuries. Total lost time from injuries included 120 lost days from 19 injuries. The process action team concentrated on four areas of improvement: equipment, environment, employee involvement, and management processes. The team initiated a safety shoe program to decrease slips and falls. The team made improvements to the environment by identifying and eliminating physical and procedural hazards while also improving workflow in the kitchen. Employees developed a safety motto \"Nutrition Care Directorate U B Safe\" and safety contests for each section. Management focused on employee training incorporating a safety topic of the month with supervisor focused monthly training and new employee safety training. Employee safety certification was also developed. This program is the starting point for decreasing accidents, which will potentially reduce injuries, improve employee morale, and decrease workman's cumpensation insurance premiums. TITLE: A STANDARDIZED METHOD FOR BENCHMARKING CLINICAL SERVICES AUTHOR(S): C.C. Bowman, M.S., R.D./L.D., Saint Francis Health System, Tulsa, OK; L.A. Coston, MBA, R.D./L.D., Saint Francis Health Care System, Tulsa, OK LEARNING OUTCOME: To provide managers a standardized method for benchmarking clinical nutrition services. ABSTRACT TEXT: The continuum of change in health care reimbursement has necessitated the development of output standards or benchmarks that allow managers to compare their operation to other similarly organized and operated departments of like ~ize. The result of this process has been \"right-sizing.\" The guidelines for benchmarking within the foodservice industry are clearly defined by many sources such as MECON and HFM, and include statistics such as meal equivalents generated per man-hour, net cost per patient day, and other activities that are quantifiable in numeric terms. More difficult, however, is the standardization of clinical nutrition output where a revenue stream may not be attached to the service provided. In our 660 bed urban hospital, an existing computer program for measuring clinical productivity is utilized to allow comparisons to benchmarking standards. Using Microsoft Excel T M , the services of all clinical staff, including dietitians and dietetic technicians are classified in increments of time required to provide either nutritional assessment and/or counseling services. An intervention of 1 to 7 minutes is short (S); 8 to 15 minutes is brief(B); 16 to 30 minutes is intermediate (I); and, 31 to 60 minutes is comprehensive ((2). The goal is for each dietitian to generate between 60 to 80% of his or her productive time in direct patient care. This process allows clinical managers to capture and integrate into the productivity model additional clinical activities required for successful patient outcomes. Among these activities are time spent in patient care rounds, calorie counts, and group counseling. In addition to productivity monitoring, one result is an internal benchmarking program that gives managers a concept of the amount of work required for each clinical position. The most important result is the ready availability of data that is easily adapted to any external benchmarking program adopted by the facility. TITLE: BENCHMARKING IN HEALTH CARE FOODSERVICE OPERATIONS AUTHOR (S): 1. J. Reagan, MS, MttA, R.D., MAJ, U.S. Army, C.M. Bednar, Ph.D., RD., M Rew, MS, R.D., Texas Woman's University, Denton, TX and M. Worley, MS. RD., LTC, US Army LEARNING OUTCOME: To describe benchmarking practices in health care foodservice operations ABSTRACT TEXT: A questionnaire focusing on benchmarldng measures and practices was developed and pilot tested with 22 foodservice directors at healthcare facilities. A revised questionnaire was mailed nationwide to 200 members of the American Society of Healthcare Foodservice Administrators and 200 members of the American Dietetic Association Practice Group, Management in Food and Nutrition Systems. The 111 respondents (28% response rate) included mostly self-operating foodservice directors using conventional production systems at facilities with an average of 300 patient beds. Nearly all used some type of benchmarking; however, only 28 facilities were benchmarking clinical productivity. Eighty-eight directors had used a benchmarking partner. Most directors used meal equivalents, patient days or meal transactions as workload indicators. Meal equivalents were calculated by a wide variety of methods. Labor hours were most often calculated as productive hours, and full-time equivalents. Respondents considered the most effective performance measure to be net expense per meal and food cost per meal. Chi-square analysis and a modified Friedman test showed that most variations in benchmarking practices were not related to size of hospital or type ofmanagement (self-operation vs. contract). For accurate comparisons, foodservice directors should use the same method of calculating meal equivalents as their benchmarking partners.TEXT: A questionnaire focusing on benchmarldng measures and practices was developed and pilot tested with 22 foodservice directors at healthcare facilities. A revised questionnaire was mailed nationwide to 200 members of the American Society of Healthcare Foodservice Administrators and 200 members of the American Dietetic Association Practice Group, Management in Food and Nutrition Systems. The 111 respondents (28% response rate) included mostly self-operating foodservice directors using conventional production systems at facilities with an average of 300 patient beds. Nearly all used some type of benchmarking; however, only 28 facilities were benchmarking clinical productivity. Eighty-eight directors had used a benchmarking partner. Most directors used meal equivalents, patient days or meal transactions as workload indicators. Meal equivalents were calculated by a wide variety of methods. Labor hours were most often calculated as productive hours, and full-time equivalents. Respondents considered the most effective performance measure to be net expense per meal and food cost per meal. Chi-square analysis and a modified Friedman test showed that most variations in benchmarking practices were not related to size of hospital or type ofmanagement (self-operation vs. contract). For accurate comparisons, foodservice directors should use the same ",
		"rank": 14,
		"year": 2001,
		"volume": 0,
		"issue": 0,
		"startpage": 0,
		"endpage": 0,
		"cites": 0,
		"ecc": 0,
		"use": 1,
		"authors": [
			"J. J. Reagon",
			"C. Bedmar",
			"M. Worley"
		]
	}
]
