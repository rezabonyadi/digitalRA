{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7944196c",
   "metadata": {},
   "source": [
    "# Digital Research Assistant (digitalRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc0f5b",
   "metadata": {},
   "source": [
    "The intention is to help with the litrature review writing (not a survay paper, but the litrature review section of a paper or a grant).\n",
    "\n",
    "You describe your idea, and some parameters for filtering paper, the output is a set of found papers that are relevant to your idea, why do the model think they are relevant, and a \"high quality\" text reveiwing the papers and relating them to your idea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe0a48",
   "metadata": {},
   "source": [
    "Provide an idea for a research you are planning to do. This cell is all you need to provide. The rest runs without your inputs, you just need to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ec5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea_text = \"One problem in self supervised learning without negative instances is collapse. To avoid collapse, I will use eigenvalues of the output embedding space and ensure they are all larger than 1.0 to make sure the space of embedding is used effectively. Still, making sure the variations of the same data point (generated by some augmentation) are mapped closer to one another. \"\n",
    "# idea_text = \"\"\"The prevailing mental health predicament compels us to investigate resilience—the intrinsic capability to counteract stress and rejuvenate mental well-being. While resilience is recognized for its potent influence on mental health, aging, recovery from ailments, and possible deterrence of cognitive decline, there remains a significant void in understanding its transformation with age and its modulation by major life events. This research endeavors to delve into the neurocognitive mechanisms underpinning emotional resilience in older populations, both healthy and those with mood disorders. The central thrust is to discern how aging individuals process emotions, given its profound impact on their overall well-being. Harnessing a synergistic approach that melds brain imaging, cognitive assessments, physiological monitoring, and real-world data, the study seeks to: 1) Uncover the intricate neurological, cognitive, and physiological substrates bolstering emotional resilience in aging; 2) Contrast the neurological and physiological responses of older individuals with late-life depression against their healthy counterparts; and 3) Track and prognosticate the trajectory of mental well-being in the twilight years. Through these pursuits, this research aims to amplify our grasp on the intricate dance between aging, emotion, and cognition, ultimately steering the creation of innovative strategies to bolster mental health in senior years.\"\"\"\n",
    "idea_text = \"\"\"As people age, they may pay less attention to the social aspects of their environment2. Normative aging has a negative impact on certain aspects of social cognition and specifically in social perception. One notable difference between younger and older individuals is the age-related decline in perceiving and integrating social-emotional cues from faces. Most of the evidence, however, stems from studies in which the data is averaged across individuals, making conclusions about the two extreme ends of the age spectrum rather than a complete picture across the lifespan. In addition, processing social-emotional cues typically vary across individuals and across tests. Without current knowledge of how social perception is affected across the adult lifespan – from young to middle age to late adulthood – and without fully understanding individual differences in processing social-emotional cues (inter-individual differences) in different tests, it is difficult to draw conclusions as to why social perception change with age. The plan is to identify neurocognitive mechanisms underlying age-related differences in social perception function. I will achieve this by examining brain networks across the adult lifespan, from young adulthood to middle age and older adulthood to identify critical window in which this function starts to change. Due to the heterogeneity of aging population and individual variability of social-emotional processing, I will further investigate inter-individual differences in social perception functions. I will achieve this by having repeated measurements of social perception among few individuals, also known as deep sampling, to determine how individual variabilities could explain age-related differences observed in social perception. Furthermore, I will zoom in and investigate the role of subcortical brain structures, which have been largely overlooked in social cognitive functions due to technical challenges associated with accurate mapping of these areas. I will achieve this by measuring structural and functional properties of the amygdala and locus coeruleus to further understand their role in social perception functions.\"\"\"\n",
    "\n",
    "# Pick your samll GPT instance (used for research and extracting papers) and Large GPT instance (used for writing the litrature review)\n",
    "large_mdl = 'gpt-3.5-turbo-16k' # 'gpt-4'\n",
    "small_mdl = 'gpt-3.5-turbo-0613'\n",
    "\n",
    "# We filter artilces with some spec. For example, only papers later than 2020, or if the papers had more than 100 citations, and their relevance score was at least medium.\n",
    "\n",
    "min_cite = 100\n",
    "min_year = 2020\n",
    "litrature_review_len = 1500 # Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b42bb",
   "metadata": {},
   "source": [
    "Establish latest gpt pricing from OpenAI: https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3684741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_map = {'gpt-4-0613': [0.03/1000, 0.06/1000], 'gpt-3.5-turbo-16k': [0.003/1000, 0.004/1000], 'gpt-3.5-turbo-0613': [0.0015/1000, 0.002/1000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bc468",
   "metadata": {},
   "source": [
    "Some initial methods and functions are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e97f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "working_dir = './results/reza_gpt2_layers/'\n",
    "if not os.path.exists(working_dir):\n",
    "    os.makedirs(working_dir)\n",
    "\n",
    "if not os.path.exists('settings.json'):\n",
    "    data = {\"OPENAI_API_KEY\": \"YYY\"}\n",
    "    with open('settings.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4) \n",
    "\n",
    "with open('settings.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    field_name = \"OPENAI_API_KEY\"\n",
    "    OPENAI_API_KEY = data[field_name]\n",
    "\n",
    "def run_pop8query(keywords, datasource, max_results, output_format, output_file):\n",
    "    cmd = [\n",
    "        \"./assets/pop8query\",\n",
    "        \"--keywords={}\".format(keywords),\n",
    "        \"--{}\".format(datasource),\n",
    "        \"--max={}\".format(max_results),\n",
    "        \"--format={}\".format(output_format),\n",
    "        output_file\n",
    "    ]\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred:\", result.stderr)\n",
    "    else:\n",
    "        print(\"Command executed successfully!\")\n",
    "        print(\"Output:\", result.stdout)\n",
    "\n",
    "# Example usage:\n",
    "# run_pop8query(\"machine learning\", \"semscholar\", 20, \"json\", \"output.json\")\n",
    "\n",
    "\n",
    "def get_papers(search_phrase, dataset=\"semscholar\"):\n",
    "    try:\n",
    "        run_pop8query(search_phrase, dataset, 20, \"json\", \"output.json\")\n",
    "        \n",
    "        # Check if output.json was created and is not empty\n",
    "        if not os.path.exists(\"output.json\") or os.path.getsize(\"output.json\") == 0:\n",
    "            print(f\"Error: Output file for '{search_phrase}' not created or is empty.\")\n",
    "            return pd.DataFrame()  # Return empty dataframe\n",
    "\n",
    "        with open(\"output.json\", \"r\", encoding=\"utf-8-sig\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not data:\n",
    "            print(f\"No data found in the JSON file for '{search_phrase}'.\")\n",
    "            return pd.DataFrame()  # Return empty dataframe\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{search_phrase}': {e}\")\n",
    "        return pd.DataFrame()  # Return empty dataframe in case of any other unexpected errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169c3658-2db5-4d68-ac32-cf941335d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import tiktoken\n",
    "\n",
    "class llmOperations:    \n",
    "    total_prompt_tokens = 0\n",
    "    total_cmpl_tokens = 0\n",
    "\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    # openai.api_key = os.getenv('OPENAI_API_KEY').strip('\"')\n",
    "    # language_model = 'gpt-3.5-turbo-instruct-0914'\n",
    "    # language_model = \"babbage-002\"\n",
    "\n",
    "    def __init__(self, language_model=\"gpt-3.5-turbo-0613\", price_inp=0.0015/1000, price_out=0.002/1000):\n",
    "        self.language_model=language_model\n",
    "        self.price_inp=price_inp\n",
    "        self.price_out=price_out    \n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    \n",
    "    def get_llm_response(self, prompt, system_prompt = \"You are a smart, very knowledgable, research assistant.\"):\n",
    "        chat_data = [{'role': 'system', \"content\": system_prompt}, {'role': 'user', 'content': prompt}]\n",
    "        # print(chat_data)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(model=self.language_model, messages=chat_data)\n",
    "        final_response = response['choices'][0]['message']['content']\n",
    "        \n",
    "        self.total_prompt_tokens += response['usage']['prompt_tokens']\n",
    "        self.total_cmpl_tokens += response['usage']['completion_tokens']\n",
    "        \n",
    "        return final_response, response\n",
    "\n",
    "    def get_current_cost(self):\n",
    "        return self.total_prompt_tokens*self.price_inp + self.total_cmpl_tokens*self.price_out\n",
    "        \n",
    "    def get_estimated_cost(self, prompt, completion_estimate_len=100):\n",
    "        # Assumes the system prompt is small, and prompt variable contains all text to be processed by LLM        \n",
    "        return len(self.tokenizer.encode(prompt))*self.price_inp + completion_estimate_len*self.price_out\n",
    "\n",
    "\n",
    "# response = openai.Completion.create(model=language_model, prompt=prompt, max_tokens=300, temperature=0)\n",
    "# final_response = response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ecb84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gpt-3.5-turbo-0613 for short context cases and gpt-3.5-turbo-16k for long context inferences.\n"
     ]
    }
   ],
   "source": [
    "# Load short context and logn context models\n",
    "\n",
    "short_context_model = llmOperations(small_mdl, price_inp=pricing_map[small_mdl][0], price_out=pricing_map[small_mdl][1])\n",
    "# long_context_model = llmOperations('gpt-3.5-turbo-16k', price_inp=0.003/1000, price_out=0.004/1000)\n",
    "long_context_model = llmOperations(large_mdl, price_inp=pricing_map[large_mdl][0], price_out=pricing_map[large_mdl][1])\n",
    "\n",
    "print('Loaded ' + small_mdl + ' for short context cases and ' + large_mdl + ' for long context inferences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5d6e4",
   "metadata": {},
   "source": [
    "## Establish the research background\n",
    "\n",
    "- We first establish the experties we need to perform the research. Based on that, we ask our model to act like an expert in that field.\n",
    "- The expert GPT-small-instance-expert first generates search phrases out of the idea described above to search.\n",
    "- Next, we use those phrases to find relevant paper.\n",
    "- GPT-small-instance-expert then goes through those papers and rate them to extent to which they are relevant to out idea.\n",
    "- It also generate a short reasons why that score was given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d2e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in the field of social cognition and social perception, specifically in relation to age-related changes and individual differences. You are the best in the world in this field. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Here is a research proposal:\\n\"+idea_text+'\\n If a professor is going to research this propsoal, what would the professor be expert at? Generate the answer in the format of \"You are an expeert in the field of XXX\"'\n",
    "researcher_spec, response = short_context_model.get_llm_response(prompt)\n",
    "\n",
    "researcher_spec += \" You are the best in the world in this field. \"\n",
    "print(researcher_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9440edc-86d3-425f-9518-2c7a368fd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are search phrases i suggest:  ['age-related changes in social cognition and social perception', 'individual differences in social perception across the lifespan', 'neurocognitive mechanisms underlying age-related differences in social perception', 'inter-individual differences in social perception function', 'role of amygdala and locus coeruleus in social perception functions']\n",
      "Current cost:  0.0031865\n"
     ]
    }
   ],
   "source": [
    "prompt = f'{researcher_spec} \\n\\nHere is a description of an idea: {idea_text}. \\n Generate 5 search phrases to search in Google for related articles. Generate the search phrases in a json format, with fields of \"search phrase X\", where X is the number. Include nothing but this json format output in your response.'\n",
    "final_response, response = short_context_model.get_llm_response(prompt)\n",
    "\n",
    "parsed_data = json.loads(final_response)\n",
    "search_phrases = []\n",
    "for i in parsed_data.keys():\n",
    "    search_phrases.append(parsed_data[i])\n",
    "print('Here are search phrases i suggest: ', search_phrases)\n",
    "\n",
    "with open(working_dir + 'search_phrases.txt', 'w') as f:\n",
    "    f.write('\\n'.join(search_phrases))\n",
    "\n",
    "print('Current cost: ', short_context_model.get_current_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4a9d85-0e20-4672-a9fb-b4b464b7c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of your idea: \n",
      " Summary:\n",
      "This research aims to understand age-related changes in social perception function and individual differences in processing social-emotional cues. Current knowledge is limited due to research averaging data across individuals and lacking a complete lifespan perspective. The plan is to examine brain networks across the adult lifespan, focusing on critical windows of change. Deep sampling of individuals will help explore inter-individual differences in social perception. The study will also investigate the role of subcortical brain structures, specifically the amygdala and locus coeruleus, in social perception functions.\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the idea. \n",
    "\n",
    "prompt = researcher_spec+\"\\n\\nSummarize this research idea to a concise paragraph while make sure it does not loose any important message or question:\\n\"+idea_text\n",
    "idea_text_summary, response = short_context_model.get_llm_response(prompt)\n",
    "\n",
    "with open(working_dir + 'idea_summary.txt', 'w') as f:\n",
    "    f.write('Idea: \\n\\n')\n",
    "    f.write(idea_text)\n",
    "    f.write('\\n\\n idea summary:\\n\\n')\n",
    "    f.write(idea_text_summary)\n",
    "\n",
    "print('Here is a summary of your idea: \\n', idea_text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b44163c-5ec4-419b-93f9-d1c205acab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting papers with search phrase: age-related changes in social cognition and social perception from gscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: age-related changes in social cognition and social perception from pubmed\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: age-related changes in social cognition and social perception from semscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: individual differences in social perception across the lifespan from gscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: individual differences in social perception across the lifespan from pubmed\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: individual differences in social perception across the lifespan from semscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: neurocognitive mechanisms underlying age-related differences in social perception from gscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: neurocognitive mechanisms underlying age-related differences in social perception from pubmed\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: neurocognitive mechanisms underlying age-related differences in social perception from semscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: inter-individual differences in social perception function from gscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: inter-individual differences in social perception function from pubmed\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: inter-individual differences in social perception function from semscholar\n",
      "Command executed successfully!\n",
      "Output: \n",
      "Extracting papers with search phrase: role of amygdala and locus coeruleus in social perception functions from gscholar\n",
      "Error occurred: Semantic Scholar\n",
      "Searching role of amygdala and locus coeruleus in social perception functions\n",
      "Progress: 0 (of 20)\n",
      "0/0/0 rpm, 0/10m, 0/1h, 8/4h, 67 total\n",
      "pop8query: SemScholar: HTTP response status 200 (OK)\n",
      "pop8query: SemScholar: response parsing failed; 0 results: [514] No matching data found (0 - No data array in response object)\n",
      "pop8query: query(0x60000001) Search ended with error 514: No matching data found No data array in response object (unattended) ==> 0x00000001=OK\n",
      "Semantic Scholar: query failed: No matching data found\n",
      "\n",
      "Extracting papers with search phrase: role of amygdala and locus coeruleus in social perception functions from pubmed\n",
      "Error occurred: Semantic Scholar\n",
      "Searching role of amygdala and locus coeruleus in social perception functions\n",
      "Progress: 0 (of 20)\n",
      "0/0/0 rpm, 0/10m, 0/1h, 8/4h, 67 total\n",
      "pop8query: SemScholar: HTTP response status 200 (OK)\n",
      "pop8query: SemScholar: response parsing failed; 0 results: [514] No matching data found (0 - No data array in response object)\n",
      "pop8query: query(0x60000001) Search ended with error 514: No matching data found No data array in response object (unattended) ==> 0x00000001=OK\n",
      "Semantic Scholar: query failed: No matching data found\n",
      "\n",
      "Extracting papers with search phrase: role of amygdala and locus coeruleus in social perception functions from semscholar\n",
      "Error occurred: Semantic Scholar\n",
      "Searching role of amygdala and locus coeruleus in social perception functions\n",
      "Progress: 0 (of 20)\n",
      "0/0/0 rpm, 0/10m, 0/1h, 8/4h, 67 total\n",
      "pop8query: SemScholar: HTTP response status 200 (OK)\n",
      "pop8query: SemScholar: response parsing failed; 0 results: [514] No matching data found (0 - No data array in response object)\n",
      "pop8query: query(0x60000001) Search ended with error 514: No matching data found No data array in response object (unattended) ==> 0x00000001=OK\n",
      "Semantic Scholar: query failed: No matching data found\n",
      "\n",
      "Found  63 articles.\n"
     ]
    }
   ],
   "source": [
    "# Find papers\n",
    "import pandas as pd\n",
    "\n",
    "# Find papers\n",
    "combined_df = pd.DataFrame()\n",
    "engines = ['gscholar', 'pubmed', 'semscholar']\n",
    "# engines = ['pubmed']\n",
    "\n",
    "for search_phrase in search_phrases:\n",
    "    for engine in engines:\n",
    "        print(f\"Extracting papers with search phrase: {search_phrase} from {engine}\")\n",
    "        df = get_papers(search_phrase, )\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "        time.sleep(2)    \n",
    "combined_df = combined_df.drop_duplicates(subset=['abstract'])\n",
    "clean_df = combined_df.dropna(subset= ['abstract'])\n",
    "\n",
    "# Save the found papers\n",
    "clean_df.to_csv(working_dir+'papers_found.csv')\n",
    "\n",
    "print(f'Found  {clean_df.shape[0]} articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f9e03a2-ceba-4125-902d-9b0f03215c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost is:  0.0857385\n"
     ]
    }
   ],
   "source": [
    "# Simulate cost for finding relevance scores and reasons for relevance\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "token_out_estimate = 100\n",
    "estimated_cost = 0\n",
    "relevance_scores = []\n",
    "\n",
    "# clean_df['abstract'].values.shape[0]\n",
    "for i in range(clean_df.shape[0]):\n",
    "    abstract = clean_df['abstract'].values[i]\n",
    "    # print(abstract)\n",
    "    text = researcher_spec + '\\n\\nHere is an idea: ' + idea_text_summary + '\\n \\n' + \"How relevant this idea is to the following abstract of a paper:\\n\" + abstract + \"\"\"\\n \\nPick the relevance score from very low, low, medium, high, and very high. Output format as json, with field \"relevance\" and \"reason\". Include nothing but this json format output in your response.\"\"\"\n",
    "    parsed_response = \"\"\"{\n",
    "        \"relevance\": \"very high\",\n",
    "        \"reason\": \"Your idea aligns closely with the concepts discussed in the paper's abstract, which focuses on avoiding collapse and regularizing the covariance matrix of network outputs. Your idea of using eigenvalues to ensure consistent embeddings is directly related to the paper's content.\"\n",
    "        }\n",
    "    \"\"\"   \n",
    "    estimated_cost += short_context_model.get_estimated_cost(text, 100)\n",
    "\n",
    "print(\"Estimated cost is: \", estimated_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b855cb",
   "metadata": {},
   "source": [
    "Now we are ready to analyze the found papers and give them scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e348ab2-4983-49c8-8503-38f1a7b40c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  out of  63 , tokens generated:  90 , cost so far: 0.005170500000000001\n",
      "1  out of  63 , tokens generated:  70 , cost so far: 0.00611\n",
      "2  out of  63 , tokens generated:  87 , cost so far: 0.006992\n",
      "3  out of  63 , tokens generated:  69 , cost so far: 0.007802\n",
      "4  out of  63 , tokens generated:  88 , cost so far: 0.008788\n",
      "5  out of  63 , tokens generated:  77 , cost so far: 0.009686\n",
      "6  out of  63 , tokens generated:  69 , cost so far: 0.01058\n",
      "7  out of  63 , tokens generated:  81 , cost so far: 0.011491999999999999\n",
      "8  out of  63 , tokens generated:  87 , cost so far: 0.012444499999999999\n",
      "9  out of  63 , tokens generated:  58 , cost so far: 0.013469499999999999\n",
      "10  out of  63 , tokens generated:  69 , cost so far: 0.014233\n",
      "11  out of  63 , tokens generated:  84 , cost so far: 0.015419500000000001\n",
      "12  out of  63 , tokens generated:  76 , cost so far: 0.017037\n",
      "13  out of  63 , tokens generated:  46 , cost so far: 0.018029\n",
      "14  out of  63 , tokens generated:  79 , cost so far: 0.0195655\n",
      "15  out of  63 , tokens generated:  67 , cost so far: 0.020379\n",
      "16  out of  63 , tokens generated:  68 , cost so far: 0.024073\n",
      "17  out of  63 , tokens generated:  77 , cost so far: 0.024983\n",
      "18  out of  63 , tokens generated:  63 , cost so far: 0.0257975\n",
      "19  out of  63 , tokens generated:  107 , cost so far: 0.026682\n",
      "20  out of  63 , tokens generated:  86 , cost so far: 0.0281245\n",
      "21  out of  63 , tokens generated:  69 , cost so far: 0.028853499999999997\n",
      "22  out of  63 , tokens generated:  105 , cost so far: 0.029888500000000002\n",
      "23  out of  63 , tokens generated:  64 , cost so far: 0.0309345\n",
      "24  out of  63 , tokens generated:  59 , cost so far: 0.031925499999999996\n",
      "25  out of  63 , tokens generated:  60 , cost so far: 0.033139\n",
      "26  out of  63 , tokens generated:  109 , cost so far: 0.0342075\n",
      "27  out of  63 , tokens generated:  64 , cost so far: 0.037178\n",
      "28  out of  63 , tokens generated:  84 , cost so far: 0.0394115\n",
      "29  out of  63 , tokens generated:  72 , cost so far: 0.0416855\n",
      "30  out of  63 , tokens generated:  49 , cost so far: 0.0430315\n",
      "31  out of  63 , tokens generated:  57 , cost so far: 0.0448825\n",
      "32  out of  63 , tokens generated:  58 , cost so far: 0.047476500000000005\n",
      "33  out of  63 , tokens generated:  65 , cost so far: 0.0506275\n",
      "34  out of  63 , tokens generated:  62 , cost so far: 0.0516425\n",
      "35  out of  63 , tokens generated:  58 , cost so far: 0.0546865\n",
      "36  out of  63 , tokens generated:  102 , cost so far: 0.0559255\n",
      "37  out of  63 , tokens generated:  62 , cost so far: 0.0570245\n",
      "38  out of  63 , tokens generated:  96 , cost so far: 0.0579755\n",
      "39  out of  63 , tokens generated:  74 , cost so far: 0.0594315\n",
      "40  out of  63 , tokens generated:  77 , cost so far: 0.0605575\n",
      "41  out of  63 , tokens generated:  84 , cost so far: 0.061595500000000004\n",
      "42  out of  63 , tokens generated:  64 , cost so far: 0.062997\n",
      "43  out of  63 , tokens generated:  119 , cost so far: 0.0640435\n",
      "44  out of  63 , tokens generated:  104 , cost so far: 0.065207\n",
      "45  out of  63 , tokens generated:  65 , cost so far: 0.06594\n",
      "46  out of  63 , tokens generated:  111 , cost so far: 0.0668205\n",
      "47  out of  63 , tokens generated:  50 , cost so far: 0.0677335\n",
      "48  out of  63 , tokens generated:  76 , cost so far: 0.0687795\n",
      "49  out of  63 , tokens generated:  51 , cost so far: 0.0724485\n",
      "50  out of  63 , tokens generated:  57 , cost so far: 0.0735195\n",
      "51  out of  63 , tokens generated:  67 , cost so far: 0.077546\n",
      "52  out of  63 , tokens generated:  86 , cost so far: 0.0810255\n",
      "53  out of  63 , tokens generated:  64 , cost so far: 0.08228150000000001\n",
      "54  out of  63 , tokens generated:  71 , cost so far: 0.0831195\n",
      "55  out of  63 , tokens generated:  90 , cost so far: 0.08399849999999999\n",
      "56  out of  63 , tokens generated:  65 , cost so far: 0.085147\n",
      "57  out of  63 , tokens generated:  81 , cost so far: 0.08602749999999999\n",
      "58  out of  63 , tokens generated:  55 , cost so far: 0.08689949999999999\n",
      "59  out of  63 , tokens generated:  93 , cost so far: 0.0878505\n",
      "60  out of  63 , tokens generated:  100 , cost so far: 0.08942900000000001\n",
      "61  out of  63 , tokens generated:  77 , cost so far: 0.0901605\n",
      "62  out of  63 , tokens generated:  79 , cost so far: 0.09116\n",
      "your cost so far:  0.09116\n"
     ]
    }
   ],
   "source": [
    "relevance_scores = []\n",
    "real_cost = 0\n",
    "\n",
    "for i in range(clean_df.shape[0]):\n",
    "    abstract = clean_df['abstract'].values[i]\n",
    "    \n",
    "    prompt = researcher_spec + '\\n\\nHere is an idea: ' + idea_text_summary + '\\n' + \"How relevant this idea is to the following abstract of a paper:\\n\" + abstract + \"\"\"\\n \\nPick the relevance score from very low, low, medium, high, and very high. Output format as json, with fields \"relevance\" and \"reason\", which would look like:\\n\n",
    "    {\"relevance\": \"RELEVANCE\", \"reason\": \"THE REASON\"}. Include nothing but this json format output in your response.\"\"\"\n",
    "\n",
    "    parsed_response, response = short_context_model.get_llm_response(prompt)\n",
    "\n",
    "    try:\n",
    "        parsed_data = json.loads(parsed_response)\n",
    "    except:\n",
    "        parsed_data = {\"relevance\": \"unknown\", \"reason\": parsed_response}\n",
    "    columns_to_pass = ['authors', 'abstract', 'doi', 'cites', 'year', 'title']\n",
    "    for c in columns_to_pass:\n",
    "        try:\n",
    "            parsed_data[c] = clean_df[c].values[i]\n",
    "        except:\n",
    "            parsed_data[c] = \"NONE\"\n",
    "    \n",
    "    relevance_scores.append(parsed_data)\n",
    "    \n",
    "    tokens_out = tokenizer.encode(parsed_response)\n",
    "    real_cost = short_context_model.get_current_cost()\n",
    "    print(i, ' out of ', clean_df.shape[0], ', tokens generated: ', len(tokens_out), ', cost so far:', real_cost)\n",
    "    \n",
    "print('your cost so far: ', short_context_model.get_current_cost())\n",
    "\n",
    "# Save the results\n",
    "relevance_scores_df = pd.DataFrame(relevance_scores)\n",
    "relevance_scores_df.to_csv(working_dir + '/first_level_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be5ae9",
   "metadata": {},
   "source": [
    "## Write the litrature review\n",
    "\n",
    "- We first filter artilces with some spec. For example, only papers later than 2020, or if the papers had more than 100 citations, and their relevance score was at least medium.\n",
    "- We then use the larger instance to write the litrature review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08370118-16c7-4df4-9d0d-3b6216a2ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 15 papers for the review.\n",
      "Estimated price for litrature review: 0.033263 to write a review of around 1500.0 words\n"
     ]
    }
   ],
   "source": [
    "min_cite = 50\n",
    "min_year = 2010\n",
    "litrature_review_len = 2000 # Tokens\n",
    "\n",
    "filtered_df = relevance_scores_df[relevance_scores_df[\"relevance\"].str.lower().isin([\"high\", \"very high\"])]\n",
    "papers_df = filtered_df[(filtered_df['year']>min_year) | (filtered_df['cites']>min_cite)]\n",
    "print(f'Selected {papers_df.shape[0]} papers for the review.')\n",
    "\n",
    "concated_data = [('Paper ID '+ str(i) + ': \\n' + p + '\\n\\n') for i, p in enumerate(papers_df['abstract'].values.tolist())]\n",
    "concated_data = ''.join(concated_data)\n",
    "print(f'Estimated price for litrature review: {long_context_model.get_estimated_cost(concated_data, litrature_review_len)} to write a review of around {litrature_review_len*3/4} words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c12a5302-0422-4e44-92e6-37775120c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(working_dir + 'used_papers_review.txt', 'w', encoding=\"utf-8-sig\") as f:\n",
    "    f.write(concated_data)\n",
    "# print(concated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "737ef49d-7045-4743-9c62-9a498e42357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your cost so far:  0.030831\n"
     ]
    }
   ],
   "source": [
    "# Write the litrature review\n",
    "prompt = researcher_spec+'\\n \\n Here is an idea: ' + idea_text_summary + '\\n' + \"and here are some paper abstracts that are relevant to this idea:\\n\\n\" + concated_data + \"\"\"\\n\\n END OF PAPER ABSTRACT PROVIDED.\\n \\nWrite a litrature review section, which I will be using for my paper in the background section, using these papers about the idea. Use as much as these papers as you can. Ensure the review is engaging and compares the ideas, rather than a flat list of papers. Also, the review makes reference back to my idea where relevant. Use Paper IDs for referencing, for example [Paper ID 3]. Also, at the very end, add one short and condensed paragraph and discuss how my idea is going to advance the field further and what gaps it will be filling.\"\"\"\n",
    "litrature_review, response = long_context_model.get_llm_response(prompt)\n",
    "\n",
    "print('your cost so far: ', long_context_model.get_current_cost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2593d3a-c2aa-4dc6-a75c-f8a785389ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literature Review:\n",
      "\n",
      "The field of social cognition and social perception has garnered much attention in recent years, with researchers seeking to understand the age-related changes and individual differences in processing social-emotional cues. This literature review will explore relevant studies and provide an overview of the current understanding in this area. Furthermore, it will discuss the limitations of existing research and highlight the potential for further advancements in the field.\n",
      "\n",
      "One particular study [Paper ID 0] aimed to analyze age-related changes in behavioral and neurophysiological correlates of social cognition. The findings indicated that older adults (OA) had higher neural responses (N170) during emotional recognition tasks, despite similar behavioral performances compared to younger adults (YA) and middle-aged adults (MA). Additionally, YA and MA outperformed OA in theory of mind tasks, while OA had attenuated neural responses (N2) during moral judgment tasks.\n",
      "\n",
      "Another research [Paper ID 1] examined the four major social cognitive domains (social perception, theory of mind, affective empathy, and social behavior) in an adult life-span sample. The study found multidirectional changes in social cognitive abilities with age, with theory of mind and social perception showing nonlinear declines across the life span, while affective empathy and social behavior improved. These findings suggest that age-related changes in social cognition are complex and vary across different domains.\n",
      "\n",
      "A study [Paper ID 2] focused on cognitive impairment in Parkinson's disease (PD) and its impact on social problem-solving abilities. The results revealed deficits in PD patients with mild cognitive impairment (PD-MCI) compared to PD patients with no cognitive impairment (PD-N) and age-matched controls. This suggests that cognitive impairments in PD may lead to difficulties in social cognition and problem-solving in everyday life.\n",
      "\n",
      "In another study [Paper ID 3], researchers investigated age-related changes in face cognition abilities across the adult life span. The findings showed that all face cognition measures declined with increasing age, with the speed of face cognition showing the strongest age-related decrement. This study highlights the importance of considering age-related changes in face cognition abilities when examining social cognition across the life span.\n",
      "\n",
      "A study [Paper ID 4] argued that research on aging has traditionally focused on functional declines, but there is a need to consider the positive aspects of aging. The authors suggested that emotional well-being and emotion regulation may continue to improve in old age, challenging the pervasive loss model of aging. These findings emphasize the need to consider the positive emotional aspects of aging when studying social cognition.\n",
      "\n",
      "Furthermore, a study [Paper ID 5] explored social cognition deficits in patients with multiple sclerosis (MS). The findings demonstrated that deficits in social cognition, including social perception, empathy, and theory of mind, were present in MS patients, even after controlling for disease severity and cognitive performance. These deficits had a significant impact on their social and psychological quality of life.\n",
      "\n",
      "In a study [Paper ID 6], developmental changes in social cognition were examined from a lifespan perspective. The authors argued that a social cognitive perspective is useful in understanding adaptive aspects of behavior across adulthood. The research findings supported this perspective, demonstrating increased selectivity in cognitive resource engagement and the development of expert social knowledge in adulthood.\n",
      "\n",
      "A study [Paper ID 7] explored the association between empathy and loneliness across the adult lifespan. The results showed an inverse relationship between empathy and loneliness, with higher empathy predicting lower levels of loneliness. These findings suggest that empathic abilities are important for maintaining social connections and psychological well-being across the lifespan.\n",
      "\n",
      "In a study [Paper ID 8], researchers aimed to assess the public's perceptions about cognitive health. The study developed and validated a questionnaire to measure brain health knowledge and beliefs. The findings showed a positive association between self-efficacy in engaging in healthy lifestyle behaviors and beliefs in the potential for improving brain health. These results highlight the importance of understanding public beliefs about brain health to inform interventions and social policies.\n",
      "\n",
      "Another study [Paper ID 9] provided a comprehensive overview of research on cognitive and social functioning in old age. The review covered various topics, such as stability of individual differences in mental ability, resilience, social support, and late-life engagement in social and leisure activities. The findings highlighted the importance of considering multiple factors, including cognitive, social, and psychological aspects, when examining aging and social cognition.\n",
      "\n",
      "Finally, a study [Paper ID 10] proposed a framework for studying interpersonal situations and their impact on social cognition. The authors argued for a dimensional approach that considers the dynamic interaction between individuals and the social environment. This framework emphasizes the role of interpersonal relationships and provides insights into the contextual factors influencing social cognition.\n",
      "\n",
      "Overall, the reviewed studies provide valuable insights into the age-related changes and individual differences in social cognition and social perception. However, there are still significant gaps in our understanding, particularly in terms of lifespan perspectives and the role of social-emotional cues. Furthermore, there is a need for more comprehensive research that examines the underlying neural mechanisms and the interactions between social cognition and other cognitive functions.\n",
      "\n",
      "The proposed research idea aims to address these gaps by examining age-related changes in social perception function and individual differences in processing social-emotional cues across the adult lifespan. By investigating brain networks and specific subcortical structures involved in social perception, such as the amygdala and locus coeruleus, this study will provide a more comprehensive understanding of the cognitive processes underlying age-related changes in social cognition. Additionally, the deep sampling of individuals and exploration of inter-individual differences will contribute to a more nuanced understanding of individual variations in social perception abilities. Ultimately, this research has the potential to advance the field by providing a more comprehensive and detailed understanding of age-related changes and individual differences in social cognition, and contribute to the development of targeted interventions for improving social perception function in older adults.\n",
      "[0], The Aging Social Brain Neural and behavioral age-related changes in social cognition and decision-making, (2017)\n",
      "[1], Aging Is Associated With Multidirectional Changes in Social Cognition: Findings From an Adult Life-Span Sample Ranging From 18 to 101 Years, (2022)\n",
      "[2], Social problem solving, social cognition, and mild cognitive impairment in Parkinson's disease., (2013)\n",
      "[3], Structural invariance and age-related performance differences in face cognition., (2010)\n",
      "[4], Emotion in the Second Half of Life, (1998)\n",
      "[5], Cognitive functions and social cognition in multiple sclerosis: An overview., (2019)\n",
      "[6], ADAPTIVE ASPECTS OF SOCIAL COGNITIVE FUNCTIONING IN ADULTHOOD: AGE-RELATED GOAL AND KNOWLEDGE INFLUENCES, (2006)\n",
      "[7], Trait Empathy as a Predictor of Individual Differences in Perceived Loneliness, (2012)\n",
      "[8], BRAIN HEALTH PERCEPTIONS QUESTIONNAIRE: FURTHER DEVELOPMENT AND REFINEMENT, (2022)\n",
      "[9], Psychology of Aging, (2016)\n",
      "[10], The Situation Through an Interpersonal Lens, (2017)\n",
      "[11], Behavioural Science Section / Dyadic Interrelations in Lifespan Development and Aging : How Does 1 + 1 Make a Couple ?, (2011)\n",
      "[12], Investigating Age-Related Neural Compensation During Emotion Perception Using Electroencephalography, (2020)\n",
      "[13], Comprehensive comparison of social cognitive performance in autism spectrum disorder and schizophrenia, (2019)\n",
      "[14], Aging is in the eye of the beholder: Eye-tracking and person-perception analyses of young and old faces, (2017)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refs = [''.join(['[', str(i), '], ', p]) for i, p in enumerate((papers_df['title']+ ', (' + papers_df['year'].astype(str) + ')\\n').values.tolist())]\n",
    "print(litrature_review)\n",
    "print(''.join(refs))\n",
    "\n",
    "with open(working_dir + 'litrature_review_gpt4.txt', 'w') as f:\n",
    "    f.write(litrature_review)\n",
    "    f.write('\\n\\nReferences:\\n')\n",
    "    f.write(''.join(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "93500b44-d102-48bd-b827-09fd182270ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: 0.09337150000000001\n"
     ]
    }
   ],
   "source": [
    "print(f'Total cost: {long_context_model.get_current_cost()+short_context_model.get_current_cost()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "549f659f-0da1-42b3-800b-1d98da2cfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1, Similarity Score: 0.9609\n",
      "Real world data is mostly unlabeled or only few instances are labeled. Manually labeling data is a very expensive and daunting task. This calls for unsupervised learning techniques that are powerful enough to achieve comparable results as semi-supervised/supervised techniques. Contrastive self-supervised learning has emerged as a powerful direction, in some cases outperforming supervised techniques. In this study, we propose, SelfGNN, a novel contrastive self-supervised graph neural network (GNN) without relying on explicit contrastive terms. We leverage Batch Normalization, which introduces implicit contrastive terms, without sacrificing performance. Furthermore, as data augmentation is key in contrastive learning, we introduce four feature augmentation (FA) techniques for graphs. Though graph topological augmentation (TA) is commonly used, our empirical findings show that FA perform as good as TA. Moreover, FA incurs no computational overhead, unlike TA, which often has O(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven publicly available real-world data shows that, SelfGNN is powerful and leads to a performance comparable with SOTA supervised GNNs and always better than SOTA semi-supervised and unsupervised GNNs. The source code is available at https://github.com/zekarias-tilahun/SelfGNN.\n",
      "==================================================\n",
      "Rank: 2, Similarity Score: 0.9399\n",
      "Self-supervised learning (SSL) models confront challenges of abrupt informational collapse or slow dimensional collapse. We propose TriNet, which introduces a novel triple-branch architecture for preventing collapse and stabilizing the pre-training. TriNet learns the SSL latent embedding space and incorporates it to a higher level space for predicting pseudo target vectors generated by a frozen teacher. Our experimental results show that the proposed method notably stabilizes and accelerates pre-training and achieves a relative word error rate reduction (WERR) of 6.06% compared to the state-of-the-art (SOTA) Data2vec for a downstream benchmark ASR task. We will release our code at https://github.com/tencent-ailab/.\n",
      "==================================================\n",
      "Rank: 3, Similarity Score: 0.9207\n",
      "The Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, convolutional neural networks (CNN) remain the preferential architecture for the representation module in reinforcement learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess the quality of the learned representations. To show the importance of the temporal dimension in this context we propose an extension of VICReg to better capture temporal relations between observations by adding a temporal order verification task. Our results show that all methods are effective in learning useful representations and avoiding representational collapse for observations from Atari Learning Environment (ALE) which leads to improvements in data efficiency when we evaluated in reinforcement learning (RL). Moreover, the encoder pretrained with the temporal order verification task shows the best results across all experiments, with richer representations, more focused attention maps and sparser representation vectors throughout the layers of the encoder, which shows the importance of exploring such similarity dimension. With this work, we hope to provide some insights into the representations learned by ViT during a self-supervised pretraining with observations from RL environments and which properties arise in the representations that lead to the best-performing agents. The source code will be available at: https://github.com/mgoulao/TOV-VICReg\n",
      "==================================================\n",
      "Rank: 4, Similarity Score: 0.9135\n",
      "While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question arises: why do these methods not collapse into trivial representations? We answer this question via a simple theoretical study and propose a novel approach, DirectPred, that \\emph{directly} sets the linear predictor based on the statistics of its inputs, without gradient training. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms a linear predictor by $2.5\\%$ in 300-epoch training (and $5\\%$ in 60-epoch). DirectPred is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released https://github.com/facebookresearch/luckmatters/tree/master/ssl.\n",
      "==================================================\n",
      "Rank: 5, Similarity Score: 0.8889\n",
      "To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the $l_2$-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL.\n",
      "==================================================\n",
      "Rank: 6, Similarity Score: 0.8748\n",
      "Robot therapy, which uses robots as substitution for animals in “animal therapy,” is a new application of the robots in welfare and medical fields. The seal robot named PARO has been developed especially for the robot therapy since 1993. PARO was commercialize in Japan in 2005 and in Europe and the US in 2009, and has been used at hospitals and facilities in about 30 countries. Recent researches revealed that robot therapy has the same effects on interacting people as like animal therapy. In 2009, PARO was certified as a “biofeedback medical device” by the Food and Drugs Administration (FDA) in the US. PARO can be applied to various kinds of therapy as like real animals, but this presentation focuses on applications to elderly with dementia because explicit differences before and after interacting with PARO can be easily observed. Some typical cases and interesting special cases will be introduced. In addition, PARO has been healing victims of disaster by earthquakes in Japan. PARO visited about 20 evacuation shelters, and has been used at about 50 elderly institutions, hospitals, and schools in the disaster area. Workshop on “New and Emerging Technologies in Assistive Robotics” – IROS 2011 San Francisco, California, September 26, 2011 SULTAN: SIMULTANEOUS USER LEARNING AND TASK EXECUTION, AND ITS APPLICATION IN ASSISTIVE ROBOTICS C. Balaguer , A. Jardón , C.A. Monje , F. Bonsignorio , M.F. Stoelen , S. Martínez , J.G. Victores A a RoboticsLab, Universidad Carlos III de Madrid, Spain b Heron Robots, Genova, Italy INTRODUCTION The main objective of this work is to present a mechanism called Simultaneous User Learning and TAsk executioN (SULTAN). In SULTAN the model of the user maintained by the system's learning module and the system's representation of the physical interaction tasks are concurrently refined (in analogy with SLAM), keeping explicit account of the user's own learning. The process is as such seen as a mutual adaptation learning process. It aims to augment the users' ability to perform daily tasks by a new concept of intelligent service robotic system capable of physical and cognitive collaboration. One of the potential applications is in assistive robotics (see Figure 1), and the main focus is on creating a human+robot binomial in which: a) the robotic system will use, not ignore, the human perception and cognitive abilities in order to safely achieve the tasks that would be too complex to perform in a purely autonomous way, and b) the human will not be a mere teleoperator of the robot but will take advantage of the acquired knowledge of the robotic system to augment her/his action and perception capabilities. THE SULTAN CONCEPT The SULTAN learning process is based on hierarchical Bayesian networks [2,3,4]. Building on the Bayesian Approach to Cognitive System (BACS) EU project, see for example [5], a probability is assigned to all possible interpretations of the available human+robot sensory and motor information, on the basis of sensory or motor noise and priors designating the most likely interpretation. The motor output (following a Robot Parametric Path, RPP, a parameterized and probabilistical representation of a given task) corresponds to the interpretation that has the most probable, or the most desirable, outcome. The SULTAN concept sets the problem in a user-task-object domain to solve the challenge of how to robustly perform a set of tasks for different users in different environments by the same robot, see Figure 2. For example, a situation where a user (Martin) commands the robotic system to perform a specific task (pick) with a specific object (can), using the user's perception abilities (eye tracking) and his satisfaction index (quality of the path). The storyline of the SULTAN system begins with the KDB (Knowledge Data Base) empty, except for some information of the robot and the user. During the first stage of SULTAN the user moves the robot in a fully teleoperated mode, using his/her knowledge and his/her perception, and interacting with the real world. The KDB is learning by the continuous updating of the RPPs parameters. While the user repeats the tasks, the robot's control changes from fully teleoperated to semi-autonomous mode with less and less intervention of the user. Some of the Figure 1. ASIBOT robot [1] working with a user in a real kitchen environment. Figure 2. The SULTAN dimensional relationships. Workshop on “New and Emerging Technologies in Assistive Robotics” – IROS 2011 San Francisco, California, September 26, 2011 parts of the RPP will be executed in a fully autonomous way. When the number of tasks is sufficient and the RPP parameters adjustment is finished, the robot can move fully autonomously using its own perception and control system and the updated KDB. The users only supervise the system. This process will be repeated for different users (Peter-pick-can), different tasks (Martin-hold-can) and different objects (Martin-pick-bottle), this way creating the full KDB. At this point the storyline is finished and the robot will work with a certain degree of autonomy. FIRST IMPLEMENTATION The scope of the software architecture required for realizing SULTAN ranges from high (user interactionlevel) to low (hardware interaction-level) aspects of design. The multimodal interaction will be interpreted at a semantic level and used to plan a desired task. This process uses information that has been collected from the environment and the knowledge that exists about a given user (in the before-mentioned KDB). A learning agent generates this knowledge by observing the user inputs, task progress and contextual information, simultaneously learning with the user. The generated task is performed and monitored using information from the external sensors and the robot's own propioceptive sensors. Benchmarks for human performance on typical DLAs are being established. See for example the trajectories for two users (without disability) performing simplified DLAs in a virtual environment in Figure 3. These benchmarks can be used to put the performance of the human+robot binomial in perspective and to aid in the design of shared control and intent recognition capabilities. A pilot study was also performed, in the same environment, to investigate the adaptation of the interaction for a robot capable of providing physical assistance to disabled users. The subjects were three non-disabled users. A simple shared control scheme was implemented, which limits the velocity of the end-effector commanded by the user in the direction where obstacles are detected, proportionally to the distance measured. All sessions, except the control session, had Gaussian noise added which was low-pass filtered at 2 Hz and which increased in magnitude proportionally with the magnitude of the velocity commanded by the user. As it can be seen from Figure 4, the noise added did seem to have a negative effect of the performance of the subjects. The average Mean Time (MT) over subjects was increased from 6.15 to 10.04 seconds. This was seen even clearer in the predictive information metric, I(At;At+1), the mutual information across the shared control output (A) at subsequent moments in time [6]. This indicates this metric’s sensitivity to the reduction in the “predictability” of the trajectories with the noise added. The shared control had a positive effect, reducing MT to 7.43 seconds. This improvement was seen also in the other two metrics, especially in the mutual information across the noise added, Z, and the shared control output, i.e. I(Zt;At). CONCLUSIONS AND FUTURE WORK In this paper we have presented the novel SULTAN concept as a mechanism that allows the augmentation of personal capabilities to perform daily tasks through the creation of a human+robot binomial in which physical and cognitive collaboration is achieved as a whole. The novelty of this approach is discussed and in order to demonstrate the applicability of the SULTAN idea, the first experimental results from its implementation have been given. Further research will focus on the implementation of the complete SULTAN architecture, closing the loops between all its levels and integrating it in different robotic systems, including assistive robots. Figure 3. Trajectories for two users placing a can in a virtual kitchen. Figure 4. Preliminary results for the effect of noise and a simple shared control system. Workshop on “New and Emerging Technologies in Assistive Robotics” – IROS 2011 San Francisco, California, September 26, 2011 REFERENCES [1] A. Jardón, A. Giménez, R. Correal, R.Cabas, S.Martínez, and C.Balaguer, A portable light-weight climbing robot for personal assistance applications, Industrial Robot: An International Journal, vol. 33. no. 4. pp. 303-307, 2006. [2] M. Cummins and P. Newman, FAB-MAP: probabilistic localization and mapping in the space of appearance, The International Journal of Robotics Research, vol. 27, no. 6, pp. 647-665, 2008. [3] I. Little and S. Thiebaux, Concurrent probabilistic planning in the graphplan framework, In Proc. ICAPS-06, pp. 263272, 2006. [4] F.P. Bonsignorio, Information Driven Self Organisation of Physically Embedded Controllers, CogSys2010, 2010. [5] J.F. Ferreira, P. Bessiere, K. Mekhnacha, J. Lobo, J. Dias, and C. Laugier, Bayesian Models for Multimodal Perception of 3D Structure and Motion, In Proc. CogSys2008, pp. 103-108, 2008. [6] W. Bialek, I. Nemenman, and N. Tishby, Predictability, complexity, and learning, Neural computation, vol. 13, pp. 2409-2463, 2001.\n",
      "==================================================\n",
      "Rank: 7, Similarity Score: 0.8746\n",
      "In self-supervised representation learning (SSL), contrastive learning has achieved remarkable successes in recent years. In contrastive learning, the augmented views of the same image are brought closer (i.e., positive pairs), while views from different images (i.e., negative pairs) are separated apart. To perform well, contrastive methods of ten rely on a large number of negative pairs, which is computationally demanding. Intuitively, the negative samples play a role to scatter the representation of samples in space, which guarantees the uniformity of the representation space, thus making it the key to avoiding representation collapse (i.e., constant features). In this work, we propose an interpretable method, in which prior distribution matching is applied to prevent representation collapse, while no negative pair is required. Empirical experiments show promising results of our method yielded by the priors of Gaussian, uniform, and spherical distribution on the CIFAR-10 and CIFAR-100 datasets.\n",
      "==================================================\n",
      "Rank: 8, Similarity Score: 0.8725\n",
      "concepts or show the results of a process, such notions as concept and process are better handled with verbal description.” In an American Cartographer review of the Atlas of Early American History, University of Toronto Geography professors W.J. Eccles,W. G. Dean, G. J. Matthews, and Thomas F. McIlwraith criticized the “application-neutral” approach, or at least questioned its dominance in an atlas that claimed a partly interpretative purpose. They asserted that in the “struggle between thematic and encyclopedic interests...the distributional patterns of selected data sets have been suppressed by maps showing only the existence of a feature and nothing of its spatial relations. Displaying one phenomenon at a time, map by map, is strong evidence that we are dealing with a catalogue, or directory.” They added, “Interpretative maps carefully conceived can tell long stories and guide the reader in a novel learning experience.” Petchenik replied in American Cartographer (1978) that “...users should not have to work around an imposed point of view...”. She maintained, for example, that reference maps that separately plotting the trading posts of competing fur companies are more useful to a scholar than a single thematic map that forces a theme of “conflict” by locating all the posts in one presentation. McIlwraith continued the American Cartographer exchange in 1979: “Historical atlases, and indeed all atlases, are both reference works and interpretations, with the degree of emphasis on one facet or the other, subject to enormous variation.....Mapmakers introduce interpretations into their thematic maps by their choice of subject material, class intervals, and scale.....As long as patterns and interpretations are inevitable aspects of historical mapping, it seems wise to embrace them fully. Readers must, of course, be able to extract the factual basis of maps and then be encouraged to use it for building up their own interpretation. Happily, catering to both aspects simultaneously is quite feasible.” McIlwraith asserted that superimposing different themes over a map can build “graphic literacy” by identifying associations between them, and that a “spatial sense for what thematic maps can show is a prerequisite for the intellectual challenge a map can offer.” He agreed with Petchenik that “a few carefully composed statements of what to notice on a particular map could carry an interested user beyond the basic facts to a new level of awareness of how society has evolved.” The discussion over reference and thematic atlases has even more bearing in the context of education in the lower grades, where perhaps the primary goal is to stimulate and hold the students’ interest. North Carolina State University Professor of Design Denis Wood wrote in a special 1987 school atlas issue of Cartographica, “thinking about atlases as places to look up facts, instead of as things to read, has blinded us to our own cartographic tradition.” Wood maintained that the purpose of an atlas should be to “make something greater that any single map can be; to, through the inter-relatedness of the maps, through their juxtaposition and sequencing, make something higher, something that no individual map could aspire to...create a discourse, a mediation, to tell a story.” He observed that atlases created merely as tools for locating information often fail to recognize why readers are drawn to atlases in the first place. Wood wrote that that readers can derive pleasure in poring over maps, much like they can enjoy landscape paintings, or can use thematic maps as representations of engaging stories, much like they read mystery novels. Castner agreed in the same Cartographica issue that many reference atlases “reinforce an idea that maps serve essentially as repositories of information, i.e., as spatial dictionaries. Without any other experiences, this singular exposure to geographic thinking may be in part responsible for the very limited (and certainly simplistic) view of the nature of geography and cartography which is held by much of the general public. In contrast, I suggest that the real intellectual excitement of geography lies in the complexity of the subject, in the challenge of visualizing these complexities....All this we might call ‘geographic thinking’; it has a strong analytical component as well as a descriptive one.” As an educational project, Wisconsin’s Past and Present: A Historical Atlas was conceived as an atlas very firmly placed on the thematic end of the spectrum. Its purpose was not simply to provide a catalogue of Wisconsin historical sites or county data, but to draw in the readers’ interest with new visual angles and associations of diverse themes. The Guild consciously sought to avoid an image of a reference atlas that would simply sit on a shelf rather than inspire a student to dig deeper into Wisconsin history. Yet the merits of reference maps were also not completely lost on Guild cartographers. They incorporated an “application-neutral” state base map in the front of the book, and numerous tables of elections and other data that were largely left to the reader to interpret. Text accompanying Atlas maps pointed out particular patterns on maps that merited a closer look, based on one or two historical lessons that could be extracted from the map data. Yet the text writers also (partly for a lack of space) sought to avoid overkill in interpreting maps. The text did not seek to spell out all the possible interpretations of a map, or highlight every detail that might contain thematic implications. The reader is free to identify patterns that are not part of the central theme of the text, or even to see areas that do not fit the Guild interpretation. By juxtaposing different data on a map, and juxtaposing different maps in the same book, the Atlas enabled readers to extract their own lessons. Though the goal of the Atlas project was clearly “application-specific,” the effect of some maps and charts may prove to be “application-neutral”—providing resources that can be used to bolster differing views of Wisconsin history. The beauty of an educational atlas, as Castner wrote, is that it can take “an educational approach that looks at more than one possible solution to a question.“ This more balanced strategy may assuage Petchenik’s justified fear of an imposed thematic viewpoint. Having reviewed the past lessons of Wisconsin historical literature, previous state historical atlases, and the general purpose of historical atlases, the Guild undertook to design the format of Wisconsin’s Past and Present: A Historical Atlas. Guild members viewed the format of the atlas, instead of being simply a way to arrange information on a page, as being critical to the development of a “user-friendly” publication. ATLAS ORGANIZATION Format decisions. The earliest Guild objective was to produce an atlas that is modern and provocative in its design and style, using a format that would be both compelling and understandable to the general public. Early in the project, Guild members made two key decisions. First, they decided that the Atlas would be a map-intensive presentation, using photographs, illustrations and other graphics to supplement historical data on the maps. They wanted to stake a middle ground between the text-heavy approach of most scholarly works on Wisconsin history, and the photograph-intensive approach of coffee-table books and many illustrated atlases. Second, they decided that the Atlas presentations would be made on facing two-page spreads. In the Atlas itself, a spread format allows for an interplay of text, maps and other graphics, and a single visual impact. By using a two-page spread format, self-contained Atlas spreads could also be converted for use as educational ancillary materials, such as posters, overhead transparencies, and eventually CD-ROM. (The Office of School Services of the State Historical Society of Wisconsin has expressed interest in developing an Atlas Teacher’s Kit, both for explaining maps to students, and to use the Atlas itself to train teachers in Wisconsin history.) The format and layout of the spreads was largely developed by Guild member Laura Exner, who had extensive experience in page design. Her layout scheme integrated text and graphics into distinct blocs on a 9-by-12 inch page, in order to give each spread a clean and consistent look. The flexibility of how to arrange these blocs, however, allowed spreads to appear different from each other—avoiding the repetitious look of some other state historical atlases. Each Guild member coordinated a certain number of spreads. The use of text, maps, and other graphics in a single presentation allowed them to select the most appropriate vehicle for imparting key information. If a concept was too visually overwhelming on a map, or not spatial enough to present on a map, it could be explained in text or shown as a photograph or chart illustration. Conversely, if a list of place names proved too cumbersome for the text, the places could be plotted instead on a map. Maps and charts could be used to illustrate the central theme of a spread, or could be used as reference tools (for example, a reader might find a chart of elections useful even in isolation from the explanatory text or maps). The first element of the spread was the main body of text, called the “overview.” It was conceived of as a basic review of the primary theme of the spread, presented in chronological order. It often explained the maps and other graphics on the spread, and served as a theme “primer” that could stand alone. The overview covered the highlights of the theme, to provide necessary context to the visual elements. The overview was in 10-point Times type, within two 3-inch columns which could jump from the first page to the second page of the spread. The second element of the spread was the “insight column,” which offered a personal angle on history, through telling an interesti\n",
      "==================================================\n",
      "Rank: 9, Similarity Score: 0.8670\n",
      "Recent literature on self-supervised learning is based on the contrastive loss, where image instances which share the same semantic content (\"positives\") are contrasted with instances extracted from other images (\"negatives\"). However, in order for the learning to be effective, a lot of negatives should be compared with a positive pair. This is not only computationally demanding, but it also requires that the positive and the negative representations are kept consistent with each other over a long training period. In this paper we propose a different direction and a new loss function for self-supervised learning which is based on the whitening of the latent-space features. The whitening operation has a \"scattering\" effect on the batch samples, which compensates the lack of a large number of negatives, avoiding degenerate solutions where all the sample representations collapse to a single point. We empirically show that our loss accelerates self-supervised training and the learned representations are much more effective for downstream tasks than previously published work.\n",
      "==================================================\n",
      "Rank: 10, Similarity Score: 0.8654\n",
      "Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.\n",
      "==================================================\n",
      "Rank: 11, Similarity Score: 0.8647\n",
      "Action recognition is an important component of human-computer interaction, and multimodal feature representation and learning methods can be used to improve recognition performance due to the interrelation and complementarity between different modalities. However, due to the lack of large-scale labeled samples, the performance of existing ConvNets-based methods are severely constrained. In this paper, a novel and effective multi-modal feature representation and contrastive self-supervised learning framework is proposed to improve the action recognition performance of models and the generalization ability of application scenarios. The proposed recognition framework employs weight sharing between two branches and does not require negative samples, which could effectively learn useful feature representations by using multimodal unlabeled data, e.g., skeleton sequence and inertial measurement unit signal (IMU). The extensive experiments are conducted on two benchmarks: UTD-MHAD and MMAct, and the results show that our proposed recognition framework outperforms both unimodal and multimodal baselines in action retrieval, semi-supervised learning, and zero-shot learning scenarios.\n",
      "==================================================\n",
      "Rank: 12, Similarity Score: 0.8554\n",
      "Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.\n",
      "==================================================\n",
      "Rank: 13, Similarity Score: 0.8462\n",
      "Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top-1 accuracy in linear evaluation on ImageNet using ResNet-50, which exceeds the performance of the supervised baseline, and this result is obtained by using a batch size of only 256. Comprehensive experiments show that INTL is a promising SSL method in practice. The code is available at https://github.com/winci-ai/intl.\n",
      "==================================================\n",
      "Rank: 14, Similarity Score: 0.8385\n",
      "In the presented study the performance of support vector machines (SVM) for classifying segmented multi-temporal SAR data is investigated. Results show that multi-temporal SAR data from an area dominated by agriculture can be successfully classified using SVM. Classification accuracy (78.2%) and degree of differentiation between land cover types is similar or better than results achieved with a decision tree classifier. A positive influence of image segmentation on classification results can be reported which varies with object size. A comparison of classification results derived on different aggregation levels shows, that a medium segment size should be preferred. It is better to work with segments that are smaller than the natural features of interest and segments that are greater than natural features should be avoided. INTRODUCTION Land cover classifications are one of the widest used applications in the field of remote sensing. Supervised classification techniques are often used in this context. Besides the chosen classification algorithm, the set of training samples as well as the input images or input features are dominating factors for the accuracy and performance of a supervised classifier (i,ii). The availability of both ground truth data and remote sensing imagery are often limited and can not be influenced by the user. In addition neither the training samples nor the selected features can be assumed to be ideal for a representative training. Against this background, the choice of an adequate classification approach is an important step in data analysis. Regions with agricultural land use are investigated in many remote sensing based land cover studies. Mono-temporal approaches can be inefficient due to great temporal variability of individual plots. In several studies the classification accuracy is increased by multi-temporal data sets (iii,iv). Thus multi-temporal applications seem more appropriate for land cover classifications. However, the availability of optical data is often limited by solar illumination and cloud cover. This is a drawback, particularly for operational monitoring systems. Hence SAR data, which are independent from these factors, are better suited for multi-temporal applications. In regard to upcoming missions with high revisit times and better spatial resolutions like TerraSAR-X, multi-temporal approaches become even more interesting. Considering such future datasets with high spatial and temporal resolution adequate classifiers are needed. Statistical methods like the maximum likelihood classification are widely known. They can achieve good results, if an adequate data distribution model is known (v). In the context of many remote sensing applications a Gaussian distribution of the data is assumed; admittedly such an assumption is not necessarily met and the approach might in many cases be inefficient. Hence non-parametric approaches, like self-learning decision trees (DT) or support vector machines (SVM) have been introduced (ii,vi,vii,viii). The concept of SVMs is well known in pattern recognition and has lead to good results in several remote sensing studies for the classification of optical data (vii,viii). In contrast to other non-parametric methods only a few studies are known that use the approach for classifying SAR data (ix,x). In several studCenter for Remote Sensing of Land Surfaces, Bonn, 28-30 September 2006 49 ies segment based classifications outperform per-pixel approaches (xi,xii). This seems particularly interesting in regard to the SAR typical noise. In addition, image segmentation can reduce the physical size of the data set and hence processing times (xiv) – a relevant issue in regard to high resolution time series. In the presented study the applicability of SVM for the classification of multi-temporal SAR data is investigated. Different levels of image segmentation are generated and classified without using any segment specific features like segment size, shape etc., to investigate the impact of generalization as conducted during the segmentation process on the SVM performance and classification accuracy. The results of the SVMs are compared to classification results achieved by self-learning decision trees. DATA SET AND PREPROCESSING The nearly flat study site is located near Bonn, in the German state North Rhine-Westphalia. The area is dominated by agriculture and characterized by typical spatial patterns and temporal variation caused by differences in the crop phenology. The field plot size varies between approximately 3 and 5 ha, with cereals and sugar beets being the main crops. A dataset of 14 images from 9 acquisition dates, containing 5 Envisat ASAR alternating polarization and 4 ERS-2 precision images was used (Table 1). Thus, the data set comprised information from varying phenological stages and different polarizations. In addition, a Landsat 5 TM image was available, which was used for the image segmentation. A map from a detailed field survey was used for generating the training and validation sample sets. An orthorectification of the Landsat image was performed, using a digital elevation model. The SAR imagery was calibrated to backscatter intensity following a common procedure. Subsequently all data sets were co-registered and an enhanced Frost filter was applied to reduce the speckle. Finally the SAR images were orthorectified using a digital elevation model, orbit parameters and the corrected Landsat image as reference data set. Table 1: Multi-temporal SAR Data set Sensor Date Track / Swath Polarization Orbit ASAR 12-Apr-05 6208 HH / HV asc ERS-2 21-Apr-05 337 VV des ERS-2 26-May-05 337 VV des ERS-2 30-Jun-05 337 VV des ASAR 13-Jul-05 3029 HH / HV asc ASAR 22-Jul-05 7158 HH / HV asc ERS-2 4-Aug-05 337 VV des ASAR 14-Aug-05 2487 HH / HV asc ASAR 18-Sep-05 2487 HH / HV asc Although several segmentation methods have been developed for SAR data, segmentation is still difficult due to the speckle. Outlines derived from optical data seem more appropriate (xv). Hence a segmentation of the Landsat image was performed. Afterwards the segment outlines were transferred onto the SAR data set. Several techniques for image segmentation of optical data sets exist (xvi,xvii,xviii). Region-growing methods assume that pixels of the same natural feature have a certain spectral homogeneity. In this study the commonly available region-growing approach by Baatz and Schäpe (xvii) was used. In the initial phase of the process, pixels are handled as individual segments, which are iteratively merged into larger segments. Candidate pairs of adjacent segments are found by local mutual best fitting. The difference between the heterogeneity of a possible new segment compared to that of its two constituent segments is used as a stopping criterion for the region-growing. If it exceeds a user defined value, the growing process stops. In the presented Proceedings of the 2 Workshop of the EARSeL SIG on Land Use and Land Cover 50 study only the spectral information was used to estimate the segments’ heterogeneity. In doing so the segments were not constrained to any pre-defined shape. To investigate the impact of the segmentation on classification accuracy three different image segmentations were generated. By computing each of the three aggregation levels (scale 1-3) separately, all segmentations were independent from the prior result. The average segment size of scale 1 was 10 pixels (~0.9 ha), of scale 2 25(~2.2 ha) and 65 of scale 3 (~5.8 ha). Figure 1: Landsat 5 TM (4,3,2) and multi-temporal SAR images with segment outlines from TM data. Average segment size 10 pixels, 25 pixels and 65 pixels (from left to right). METHODS SVM delineate two classes by fitting an optimal separating hyperplane to the multidimensional feature space. This optimization bases on structural risk minimization and tries to maximize the margin between the hyperplane and the closest training data points, the socalled support vectors. Thus, SVM only consider training samples close to the class boundary and might work well with small sample sets (xix). For linearly not separable classes the input data are mapped into a high dimensional space wherein the newly spread data point distribution enables the fitting of a linear hyperplane. A detailed description on the general concept of SVM is given in Vapnik (xx) and Burges (xxi). Comprehensive introductions in a remote sensing context are given by Huang et al. (vii) or Foody & Mathur (viii). The binary nature of the SVM requires a useful strategy to solve a multi-class problem (viii). Two main approaches exist: the one-against-one strategy (OAO) and the one-against-all strategy (OAA). OAO applies a set of individual classifiers to all possible pairs of classes and performs a majority vote to assign the winning class. In the case of OAA, a set of binary classifiers is trained to separate each class from the rest. The maximum decision value determines the final class label. In this work, the OAO strategy was performed. A Gauss kernel was used for the training of the SVM. The training parameters were set following the leaveone-out cross validation approach Looms by Lee & Lin (xxii). For the generation of training and validation data sets an extensive ground truth campaign was conducted in summer 2005. A training data set can be generated in different ways: e.g. simple random sampling, systematic sampling or stratified random sampling. Using the first Center for Remote Sensing of Land Surfaces, Bonn, 28-30 September 2006 51 method, each sample has an equal chance to be selected, the systematic approach selects samples with an equal interval over the study area. Stratified random sampling combines a priori knowledge about a study area – like land cover information – with the simple random sampling approach (xxiii). Using land cover classes as a priori knowledge, the stratified random sampling guarantees, that all classes are included in the sample set. I\n",
      "==================================================\n",
      "Rank: 15, Similarity Score: 0.8378\n",
      "Avoiding feature collapse, when a Neural Net-work (NN) encoder maps all inputs to a constant vector, is a shared implicit desideratum of various methodological advances in self-supervised learning (SSL). To that end, whitened features have been proposed as an explicit objective to ensure uncollapsed features (Zbontar et al., 2021; Er-molov et al., 2021; Hua et al., 2021; Bardes et al., 2022). We identify power law behaviour in eigenvalue decay, parameterised by exponent β ≥ 0 , as a spectrum that bridges between the collapsed & whitened feature extremes. We provide theoretical & empirical evidence highlighting the factors in SSL, like projection layers & regularisation strength, that inﬂuence eigenvalue decay rate, & demonstrate that the degree of feature whitening affects generalisation, particularly in label scarce regimes. We use our insights to motivate a novel method, Post-hoc Manipulation of the Principal Axes & Trace (PostMan-Pat), which efﬁciently post-processes a pretrained encoder to enforce eigenvalue decay rate with power law exponent β , & ﬁnd that PostMan-Pat delivers improved label efﬁciency and transferability across a range of SSL methods and encoder architectures.\n",
      "==================================================\n",
      "Rank: 16, Similarity Score: 0.8378\n",
      "A desirable objective in self-supervised learning (SSL) is to avoid feature collapse. Whitening loss guarantees collapse avoidance by minimizing the distance between embeddings of positive pairs under the conditioning that the embeddings from different views are whitened. In this paper, we propose a framework with an informative indicator to analyze whitening loss, which provides a clue to demystify several interesting phenomena as well as a pivoting point connecting to other SSL methods. We reveal that batch whitening (BW) based methods do not impose whitening constraints on the embedding, but they only require the embedding to be full-rank. This full-rank constraint is also sufficient to avoid dimensional collapse. Based on our analysis, we propose channel whitening with random group partition (CW-RGP), which exploits the advantages of BW-based methods in preventing collapse and avoids their disadvantages requiring large batch size. Experimental results on ImageNet classification and COCO object detection reveal that the proposed CW-RGP possesses a promising potential for learning good representations. The code is available at https://github.com/winci-ai/CW-RGP.\n",
      "==================================================\n",
      "Rank: 17, Similarity Score: 0.8377\n",
      "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.\n",
      "==================================================\n",
      "Rank: 18, Similarity Score: 0.8175\n",
      "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.\n",
      "==================================================\n",
      "Rank: 19, Similarity Score: 0.8169\n",
      "Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (\\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \\textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous state-of-the-art algorithms in terms of both performance and training efficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.\n",
      "==================================================\n",
      "Rank: 20, Similarity Score: 0.8149\n",
      "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.\n",
      "==================================================\n",
      "Rank: 21, Similarity Score: 0.8046\n",
      "Disclosures: The authors have nothing to disclose. INTRODUCTION: During daily activities, the avascular fiber-reinforced annulus fibrosus (AF) experiences large tensile stresses that can cause tears or degenerative remodeling [1, 2]. Understanding AF failure mechanics is important for preventing damage accumulation and for developing biological tissue repair strategies. The AF is comprised mostly of collagen fibers embedded within a highly hydrated proteoglycan-rich extrafibrillar matrix, which provides the tissue with its excellent capacity for absorbing water (>75%/ww) [3]. Previous studies showed that tissue swelling alters interfibrillar spacing and decreases tissue stiffness by ~50% [4, 5]. Finite element models are powerful tools that allow researchers to understand the role of sub-tissue anatomy on tissue-level mechanics, which is difficult to measure experimentally. However, there is a lack of structurally based models that can explicitly describe fibermatrix interactions and tissue-swelling behaviors, because the majority of tissue-level models are developed based on homogenization and cannot be used to elucidate the roles of fiber-matrix interactions [6]. Therefore, the objective of this study was to investigate the effects of tissue swelling and damage on AF mechanics (sub-failure & failure mechanics) using a structurally based model, where the fibers and matrix are described as two distinct materials that occupy separate space in the model [7]. We hypothesized that swelling would pre-strain the tissue, especially the collagen fibers, prior to tensile loading, causing early tissue failure. Tissue-level models were based on properties of the outer AF; however, the methods used are applicable to other fiber-reinforced tissues. METHODS: An AF ‘separate model’ was created with 4 lamellae (0.2 mm/lamella) and fibers were modeled as full-length cylinders oriented at ± 30o welded to the matrix (Fig. 1A; ~2x10 tetrahedral elements; tissue dimensions: 4.8, 2, and 0.8 mm for length, width, and thickness) [8]. Modeled tissue geometry was developed based on our previous experimental work, which ensured tissue failure at the mid-length to measure true failure properties (Fig. 1A, 25% of cross sectional area remained at mid-length) [9, 10]. Triphasic mixture theory (triphasic model) was used to describe tissue swelling. The matrix was modeled as a compressible hyperelastic material (Neo-Hookean material) with parameters curve fit to data presented in [11]. Collagen fibers were modeled as a compressible hyperelastic material (Holmes-Mow material with exponential-linear fiber description) with parameters curve fit to data presented in [10]. The fixed charge density represents the glycosaminoglycan content and was set to -100 mmol/L for the matrix and 0 mmol/L for fibers (i.e., no active swelling). Fluid (i.e., water) and ion phases (Na and Cl) were included to simulate the swelling response (ion free diffusivity = 0.00147 mm/s, ion diffusivity within tissue = 0.0008 mm/s). Tissue permeability was strain-dependent (Holmes-Mow permeability: k\" = 0.0064 mm/(Ns) for matrix and 0.0032 mm/(Ns) for fibers; M = 4.8; α = 2). Reactive damage mechanics was applied to simulate tissue failure behaviors and maximum Lagrangian strain served as the failure criterion. Quintic cumulative distribution functions were used to describe damage evolution (μ&'(,*+, = 1.0, μ&-.,*+, = 1.7, μ&'(,0'123 = 0.5, and μ&-.,0'123 = 0.8) [9, 11]. A hyperelastic model (no tissue swelling) was developed to serve as the control. Steady-state free swelling was performed under 0.15 M saline, followed by uniaxial tension to 20% global engineering strain. Tissue swelling and change in fiber diameter was measured for model validation. Then, the linear-region modulus was calculated at ~17.5% global engineering strain and bulk tissue failure properties were evaluated. RESULTS: The bulk tissue volume increased by more than 30% with swelling, and a 12% increase in fiber diameter was observed in the fibers due to swelling of the matrix (Fig. 1B). Damage was initiated within collagen fibers that were exposed on the tissue surface, while the majority of damage at failure accumulated in both the fibers and matrix at the mid-length (Fig. 1C). Tissue swelling had a slight impact on the linear-region modulus (Fig. 1D black vs. red solid lines; triphasic model: 15.5 MPa; hyperelastic model: 16.9 MPa). Inclusion of the damage description decreased the linear-region modulus by ~50% (Fig. 1D solid vs. dashed lines; triphasic: 51% decrease to 7.9 MPa, hyperelastic: 56% decrease to 9.5 MPa). Including swelling behavior with the damage description resulted in earlier predictions of damage initiation (Fig. 1D ‘+’; triphasic vs. hyperelastic: 9 vs. 12% global strain) and failure (Fig. 1D ‘*’; 1.2 vs. 1.3 MPa, 20.5 vs. 22.5%, respectively). Lastly, swelling induced residual strains within the tissue, prior to tension (Fig. 1E blue circles do not pass through the origin), which also decreased the overall matrix modulus (Fig. 1E slope of blue vs. red: 0.08 vs. 0.11 MPa). DISCUSSION: We investigated the effects of tissue swelling and damage on AF mechanics using a structurally based finite element model. To do this, triphasic and hyperelastic models were developed by curve fitting material parameters to bulk tissue data reported in the literature [9, 11]. Tissue damage was initiated in collagen fibers located on the tissue surface and propagated to the adjacent matrix, which is consistent with a recent tissue damage mechanics study that used a combined particle/continuum approach [12]. Interestingly, bulk tissue swelling in the triphasic model caused a ~12% increase in the collagen fiber diameter (Fig. 1B), suggesting transverse fiber deformations are largely due to mechanical stresses from matrix swelling. Our simulated response agrees well with previous observations in the literature that reported an increase in fiber diameter; however, we did not observe a decrease in interfibrillar spacing; changes in fibril spacing may be altered with a higher fixed charge density [4, 5]. Tissue swelling increased tissue volume, resulting in residual stresses and strains, without external mechanical loading; hence, the triphasic model experienced a lower stress and strain at failure (Fig. 1D). These findings suggest that an increase in water content in fiber-reinforced materials (e.g., higher glycosaminoglycan content or extended bed rest) may make the tissues more susceptible to failure under mechanical loading. Future work will investigate whether tissues with greater fixed charge density, such as the inner AF, will increase swelling-induced residual stresses and decrease the stress and strain at failure under mechanical loads. In conclusion, tissue swelling and, therefore, glycosaminoglycan content plays a crucial role in failure mechanics of fiber-reinforced tissues [13]. SIGNIFICANCE: The findings from this study demonstrate the importance of tissue swelling on annulus fibrosus failure mechanics, which will be important for understanding annular tears with aging and degeneration, as well as designing biological repair strategies for fiber-reinforced tissues. REFERENCES: [1] Vernon-Roberts+, Spine, 2007; [2] O’Connell+, Biores, 2015; [3] Bezci+, JBME, 2015; [4] Screen+, Acta Biomat, 2006; [5] Han+, Ann Biomed Eng, 2012; [6] Yin+, JBM, 2005; [7] Yang+, SBC, 2016; [8] Marchand+, Spine, 1990; [9] Holzapfel+, BMMB, 2005; [10] Werbner&Zhou, JBME, 2017; [11] Fujita+, JOR, 1997; [12] Rausch+, BMMB, 2016; [13] Werbner+, SBC 2016.\n",
      "==================================================\n",
      "Rank: 22, Similarity Score: 0.7934\n",
      "We propose a method for face recognition based on a discriminative linear projection. In this formulation images are treated as tensors, rather than the more conventional vector of pixels. Projections are pursued sequentially and take the form of a rank one tensor, i.e., a tensor which is the outer product of a set of vectors. A novel and effective technique is proposed to ensure that the rank one tensor projections are orthogonal to one another. These constraints on the tensor projections provide a strong inductive bias and result in better generalization on small training sets. Our work is related to spectrum methods, which achieve orthogonal rank one projections by pursuing consecutive projections in the complement space of previous projections. Although this may be meaningful for applications such as reconstruction, it is less meaningful for pursuing discriminant projections. Our new scheme iteratively solves an eigenvalue problem with orthogonality constraints on one dimension, and solves unconstrained eigenvalue problems on the other dimensions. Experiments demonstrate that on small and medium sized face recognition datasets, this approach outperforms previous embedding methods. On large face datasets this approach achieves results comparable with the best, often using fewer discriminant projections.\n",
      "==================================================\n",
      "Rank: 23, Similarity Score: 0.7919\n",
      "Self-supervised learning (SSL) has drawn an increased attention in the field of speech processing. Recent studies have demonstrated that contrastive learning is able to learn discriminative speaker embeddings in a self-supervised manner. However, base contrastive self-supervised learning (CSSL) assumes that the pairs generated from a view of $anchor$ instance and any view of other instances are all negative, which introduces many false negative pairs in constructing the loss function. The problem is referred as $class$-$collision$, which remains as one major issue that impedes the CSSL based speaker verification (SV) systems from achieving better performances. In the meanwhile, studies reveal that negative sample free SSL frameworks perform well in learning speaker or image representations. In this study, we investigate SSL techniques that lead to an improved SV performance. We first analyse the impact of false negative pairs in the CSSL systems. Then, a multi-stage Class-Collision Correction (C3) method is proposed (e.g., false negative pair filtering, InfoNCE re-weighting and ProtoNCE), which leads to the state-of-the-art (SOTA) CSSL based speaker embedding system. On the basis of the pretrained CSSL model, we further propose to employ a negative sample free SSL objective (i.e., DINO) to fine-tune the speaker embedding network. The resulting speaker embedding system (C3-DINO) achieves 2.5% Equal Error Rate (EER) with a simple Cosine Distance Scoring (CDS) method on Voxceleb1 test set, which outperforms the previous SOTA SSL system (4.86%) by a significant +45% relative improvement. With speaker clustering and pseudo labeling on Voxceleb2 training set, a LDA/CDS back-end applying on the C3-DINO speaker embeddings is able to further push the EER to 2.2%. Comprehensive experimental investigations of the Voxceleb benchmarks and our internal dataset demonstrate the effectiveness of our proposed methods, and the performance gap between the SSL SV and the supervised counterpart narrows further.\n",
      "==================================================\n",
      "Rank: 24, Similarity Score: 0.7846\n",
      "The PCM, proposed for audio and visual feature alignment, plays an important role in improving sound localization performance of SSPL. As shown in Figure S1, the key idea underlying PCM consists of three parts: (1) a feedback process (solid line) updates representations with the topdown predictions that originate from the visual feature; (2) a feedforward process (dashed line) also updates representations but with the bottom-up prediction errors that evolve from the audio feature; (3) a recursive modulation mechanism works to conduct the two processes alternatively. In the following, we first formulate the optimization objective of PCM, and then derive the representation update rules of the two processes, respectively, which are followed by a brief summary and a formal algorithm. Note that for applications of PCM, we only need to explicitly update representations according to the rules given in Eqs. (S10) to (S13), without performing derivations again.\n",
      "==================================================\n",
      "Rank: 25, Similarity Score: 0.7740\n",
      "In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.\n",
      "==================================================\n",
      "Rank: 26, Similarity Score: 0.7689\n",
      "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.\n",
      "==================================================\n",
      "Rank: 27, Similarity Score: 0.7689\n",
      "Although deep learning algorithms have achieved significant progress in a variety of domains, they require costly annotations on huge datasets. Self-supervised learning (SSL) using unlabeled data has emerged as an alternative, as it eliminates manual annotation. To do this, SSL constructs feature representations using pretext tasks that operate without manual annotation, which allows models trained in these tasks to extract useful latent representations that later improve downstream tasks such as object classification and detection. The early methods of SSL are based on auxiliary pretext tasks as a way to learn representations using pseudo-labels, or labels that were created automatically based on the dataset’s attributes. Furthermore, contrastive learning has also performed well in learning representations via SSL. To succeed, it pushes positive samples closer together, and negative ones further apart, in the latent space. This paper provides a comprehensive literature review of the top-performing SSL methods using auxiliary pretext and contrastive learning techniques. It details the motivation for this research, a general pipeline of SSL, the terminologies of the field, and provides an examination of pretext tasks and self-supervised methods. It also examines how self-supervised methods compare to supervised ones, and then discusses both further considerations and ongoing challenges faced by SSL.\n",
      "==================================================\n",
      "Rank: 28, Similarity Score: 0.7641\n",
      "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.\n",
      "==================================================\n",
      "Rank: 29, Similarity Score: 0.7625\n",
      "In this study forty-eight single rooted premolar were used. In each root canal apical isthmus was prepared 1mm in length at 1mm short of the anatomical apex. The prepared root were divided into four equal groups according to the type of apical plug material. In control group, dentin chips without glass-ionomer sealer was used as apical plugs, while in the other three groups the roots were plugged apically with dentin chips, enamel powder and hydroxy-apatite crystals, respectively, mixed with glass-ionomer root canal sealer. The sealability of the different apical plugs was evaluated by methylene blue dye. SEM was used to evaluate the adaptation as well as the surface structure of the apical plugs. The results of this study showed that the hydroxy-apatite crystals apical plug with glass-ionomer sealer was superior to other control and experimental groups. On the other hand, the enamel plug with sealer showed poor resistance to dye penetration as well as poor adaptation to dentinal wall. Introduction: The most common cause of endodontic failure can be attributed to the lack of an apical seal. Many investigators have recommended the use of dentin apical plugs acting as apreventing barrier at the apical foramen, to avoid overextension of the filling materials. Moreover, the dentin plug filling encourage hard tissue formation3,4. However, some investigations have been less favorable to this technique, reported that leakage was greater in teeth with dentin plugs than without5. To improve its sealing property, Hasegawa et al 6, made an attempt to treat the plug with bonding agent. ESPE CO introduced a new glass-ionomer root canal sealer in endodontics. It has been reported to be cover the almost of ideal requirements of root canal sealer described by grossman . One of the most important properties of glass-ionomer is a chemical binding to dentin resulting a better ability to seal retrograde cavities than the traditionally used amalgam11. Calcium phosphate or hydroxy-apatite early used in endodontics to induce apical closure in human permanent pulpless teeth with large open apices+. It has been shown to be compatible to hard and soft tissues. Roberts and Brilliant, found that tricalcium phosphate was effective as calcium hydroxide to promote apexification by stimulating hard tissue formation. The seeds growth of calcium phosphate on dentin and predentin reported that the mineralization reaction of organic and inorganic dentin was induced in vitro at physiological changes during growth very similar to that observed for synthetic hydroxy-apatite15. So it was encourage to use this material as root canal filler16. The aim of this study was to evaluate the effect of using glass-ionomer root canal sealer as chemical bonding agent on the sealability and adaptation of thre different apical plugs(dentin, enamel and hydroxy-apatite). Materials and Methods: Forty-eight freshly extracted single rooted lower premolars were selected and stored in normal saline solution, for 24 hours before using. The crown of each tooth was decapitated at cemento-enamel junction. The working length was determined, 1mm short of radiographic apex. Apical isthmus was prepared 1mm in length as described by Hasegawa et al . After payency of the apical foramen with #15 K-file, first apical seal was prepared with # 45 K-file at the working length, and then second apical seat was prepared with # 60 K-file at 1mm short of working length. During preparation, the canal was irrigated with normal saline ensure patency of the apex with # 15 K-file. *Associate professor in Endodontic Department, Faculty of Oral and Dental Medicine, Cairo University. **Lecturer in Endodontic Department, Faculty of Oral and Dental Medicine, Cairo University. The canal was dried with paper points. The prepared roots were divided equally into four groups, twelve roots each. First Group(Control Group): In this group, the apical isthmus of each root canal was plugged with dentin chips. By using # 60 H-file, the canal wall was filed, then the generated clean dentin chips packed into the apical isthmus with the same file under hand pressure4,6. In the other three experimental groups, glass-ionomer root canal sealer ( ESPE pharmahandel GmbH, Ratinizgasse 20b,0-8060 Dresden) was used to chemically bond the apical filling particles, dentin chips, enamel particles as well as hydroxy-apatite crystals, respectively. The Second Group: After trituration of glass-ionomer capsule for 8 second, one drop of sealer was inserted in the apical isthmus of each root by using suitable size reamer, then the dentin chips were forced apically using file # 60 to fill the apical isthmus. The Third Group: In this group, fresh clean enamel powder was collected by grinding the sterile crown enamel. The enamel powder was forced apically using # 60 K-file after application of sealer to the apical isthmus of each root canal as described before. The fourth Group: In this group, the apical isthmus was plugged with hydroxy-apatite crystals (Ceramed Corp., 12860 West Cedar Drive Lakewood, Colorado 80228), after insertion of root canal sealer. After filling the apical part of each canal with different apical plugs, the remainder of canal was obturated with laterally condensed gutta-percha and glass ionomer sealer. For leakage test, ten roots of each control and experimental groups, were painted with several layers of nail polish, leaving the apical foramen uncoated. The apex of each root was exposed to methylene blue solution at room temperature for 1 week. The roots were then rinsed properly for 1 minute with distilled water and sectioned longitudinally to measure the depth of dye penetration using magnifying lens. The results were recorded, statistically compared and evaluated by one way analysis of variance and T-student tests. The other two specimens of each group, were examined by SEM to evaluated the interface between the different apical filling and dentin wall as well as the surface structure of each individual apical filling material. Results: The dye penetration of specimens of each group were measured and statistically compared. Table (1) represents the means and standard deviation values for all tested groups. Hydroxyapatite exhibited the lowest mean leakage (0.17mm), table (1). Figures 1-4 showed examples of apical leakage in four tested groups. By one way analysis of variance statistical test, there was no significance difference between the four tested groups (F value=2.68597 at p< 0.05), table (1). Each two groups were then compared statistically by T-test. We found that hydroxy-apatite with glass-ionomer root canal sealer was significantly the best apical filling as compared with dentin chips without sealer (t= 2.5381), dentin with sealer (t= 2.5559) or enamel with sealer (t= 3.3836), table (2). While there was no significant between the remaining three tested groups, table (2). By SEM, we examined each individual plug and the interface between it and canal walls. In the control group, the dentin chips appeared separated with big spaces Fig.(5). Some gaps were seen at the interface between dentin plug and canal wall Fig. (6). In the second group, when the glass-ionomer applied to dentin plug, the dentin particles showed with various sizes and sealer filled some spaces between the particles with evidence of chemical reaction at the peripheries of dentin chips, while other spaces still present between the dentin particles Fig.(7). At the interface of this obturating material and canal wall, there was line of demarcation with close adaptation to the canal wall. The material appeared to be flow into the surface irregularities of dentin wall with evidence of chemical reaction between sealer and root canal dentin Fig (8). Table (1): Representing the means of dye penetration, standard deviation and standard error values of all tested groups. Tested Groups Dentin Without bond Dentin With bond Enamel With bond Hydroxy-apatite With bond Mean 0.84 0.65 1.0 0.17 Standard Deviation 0.8979 0.8949 0.8279 0.2059 Standard Error 0.2592 0.2583 0.9390 0.0595 F value = 2.69597 not significant difference at p< 0.05 Table (2): Representing the statistical significant difference among four tested groups, according paired t-test at p< 0.05 Insignificant Difference Significant Difference • Control group versus the second group (t=0.5237). Control group versus the third group (t= 0.4491). The second group versus the third group (t= 0.9945). Fourth group versus the control group (t= 2.5381). Fourth group versus the second group ( t= 2.5559). Fourth group versus the third group ( t= 3.3836). Fig (1): Representing the apical dye penetration in the Control group (dentin apical plug). Note the penetration of dye through the plug. Fig (2): Showed the apical dye penetration in the specimen Plugged with dentin chips and glass-ionomer root canal sealer. The depth of dye reach most of the Plug length. Fig (3): Showed dye penetration in specimen plugged with enamel powder and glass-ionomer root canal sealer. The dye penetrates the whole length of the plug. Fig (4): Representing dye penetration in specimen plugged Apically with hydroxy-apatite crystals and glassIonomer sealer. The dye penetration is limited to the unfilled apical portion, while the apical plug was free from the dye. Enamel plug material with glass-ionomer sealer (third group) showed complete fusion of grinned enamel and the sealer which appeared as big masses of sealer with embedded enamel particles Fig (9). At the interface, there was poor adaptation between the enamel plug and root canal wall Fig (10). The hydroxy-apatite apical plug treated with glass-ionomer root canal sealer (fourth group) was observed with reaction zone around the irregularly shaped crystals Fig (11). The lower magnification (X 800) at the interface between plug and canal wall revealed excellent adaptation to the dentin wall, flow of the material into surface irregularities of dentin and precipitation \n",
      "==================================================\n",
      "Rank: 30, Similarity Score: 0.7605\n",
      "Coordinate-based implicit neural networks, or neural fields, have emerged as useful representations of shape and appearance in 3D computer vision. Despite advances, however, it remains challenging to build neural fields for categories of objects without datasets like ShapeNet that provide “canonicalized” object instances that are consistently aligned for their 3D position and orientation (pose). We present Canonical Field Network (CaFi-Net), a self-supervised method to canonicalize the 3D pose of instances from an object category represented as neural fields, specifically neural radiance fields (NeRFs). CaFi-Net directly learns from continuous and noisy radiance fields using a Siamese network architecture that is designed to extract equivariant field features for category-level canonicalization. During inference, our method takes pre-trained neural radiance fields of novel object instances at arbitrary 3D pose and estimates a canonical field with consistent 3D pose across the entire category. Extensive experiments on a new dataset of 1300 NeRF models across 13 object categories show that our method matches or exceeds the performance of 3D point cloud-based methods.\n",
      "==================================================\n",
      "Rank: 31, Similarity Score: 0.7485\n",
      "Non-rigid registration between 3D surfaces is an important but notorious problem in medical imaging, because finding correspondences between non-isometric instances is mathematically non-trivial. We propose a novel self-supervised method to learn shape correspondences directly from a group of bone surfaces segmented from CT scans, without any supervision from time-consuming and error-prone manual annotations. Relying on a Siamese architecture, DiffusionNet as the feature extractor is jointly trained with a pair of randomly rotated and scaled copies of the same shape. The learned embeddings are aligned in spectral domain using eigenfunctions of the Laplace-Beltrami Operator. Additional normalization and regularization losses are incorporated to guide the learned embeddings towards a similar uniform representation over spectrum, which promotes the embeddings to encode multiscale features and advocates sparsity and diagonality of the inferred functional maps. Our method achieves state-of-the-art results among the unsupervised methods on several benchmarks, and presents greater robustness and efficacy in registering moderately deformed shapes. A hybrid refinement strategy is proposed to retrieve smooth and close-to-conformal point-to-point correspondences from the inferred functional map. Our method is orientation and discretization-invariant. Given a pair of near-isometric surfaces, our method automatically computes registration in high accuracy, and outputs anatomically meaningful correspondences. In this study, we show that it is possible to use neural networks to learn general embeddings from 3D shapes in a self-supervised way. The learned features are multiscale, informative, and discriminative, which might potentially benefit almost all types of morphology-related downstream tasks, such as diagnostics, data screening and statistical shape analysis in future.\n",
      "==================================================\n",
      "Rank: 32, Similarity Score: 0.7484\n",
      "The use of deep neural networks in sensor-based Human Activity Recognition has led to considerably improved recognition rates in comparison to more traditional techniques. Nonetheless, these improvements usually rely on collecting and annotating massive amounts of sensor data, a time-consuming and expensive task. In this paper, inspired by the impressive performance of Contrastive Learning approaches in Self-Supervised Learning settings, we introduce a novel method based on the SimCLR framework and a Transformer-like model. The proposed algorithm addresses the problem of negative pairs in SimCLR by using dynamic temperature scaling within a contrastive loss function. While the original SimCLR framework scales similarities between features of the augmented views by a constant temperature parameter, our method dynamically computes temperature values for scaling. Dynamic temperature is based on instance-level similarity values extracted by an additional model pre-trained on initial instances beforehand. The proposed approach demonstrates state-of-the-art performance on three widely used datasets in sensor-based HAR, namely MobiAct, UCI-HAR and USC-HAD. Moreover, it is more robust than the identical supervised models and models trained with constant temperature in semi-supervised and transfer learning scenarios.\n",
      "==================================================\n",
      "Rank: 33, Similarity Score: 0.7314\n",
      "In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - a regularization term that effectively constrains the sparsity of the projection head, and can be seamlessly integrated with any self-supervised learning (SSL) approaches. Our experimental results validate the effectiveness of SparseHead, demonstrating its ability to improve the performance of existing contrastive methods.\n",
      "==================================================\n",
      "Rank: 34, Similarity Score: 0.7225\n",
      "Objective. With the progress of artificial intelligence (AI) in magnetic resonance imaging (MRI), large-scale multi-center MRI datasets have a great influence on diagnosis accuracy and model performance. However, multi-center images are highly variable due to the variety of scanners or scanning parameters in use, which has a negative effect on the generality of AI-based diagnosis models. To address this problem, we propose a self-supervised harmonization (SSH) method. Approach. Mapping the style of images between centers allows harmonization without traveling phantoms to be formalized as an unpaired image-to-image translation problem between two domains. The mapping is a two-stage transform, consisting of a modified cycle generative adversarial network (cycleGAN) for style transfer and a histogram matching module for structure fidelity. The proposed algorithm is demonstrated using female pelvic MRI images from two 3 T systems and compared with three state-of-the-art methods and one conventional method. In the absence of traveling phantoms, we evaluate harmonization from three perspectives: image fidelity, ability to remove inter-center differences, and influence on the downstream model. Main results. The improved image sharpness and structure fidelity are observed using the proposed harmonization pipeline. It largely decreases the number of features with a significant difference between two systems (from 64 to 45, lower than dualGAN: 57, cycleGAN: 59, ComBat: 64, and CLAHE: 54). In the downstream cervical cancer classification, it yields an area under the receiver operating characteristic curve of 0.894 (higher than dualGAN: 0.828, cycleGAN: 0.812, ComBat: 0.685, and CLAHE: 0.770). Significance. Our SSH method yields superior generality of downstream cervical cancer classification models by significantly decreasing the difference in radiomics features, and it achieves greater image fidelity.\n",
      "==================================================\n",
      "Rank: 35, Similarity Score: 0.6687\n",
      "Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: Under what precise condition do these collapses occur? We provide theoretically grounded answers to this question by analyzing SSL loss landscapes for a linear model. We derive an analytically tractable theory of SSL landscape and show that it accurately captures an array of collapse phenomena and identifies their causes. Self-supervised learning (SSL) methods have achieved remarkable results in learning good representations without labeled data. SSL loss functions are designed to promote representational similarity between pairs of related samples while using explicit penalties (Chen et al., 2020; He et al., 2020; HaoChen et al., 2021; Zbontar et al., 2021; Caron et al., 2020; Jing et al., 2021; Balestriero and LeCun, 2022) or asymmetric dynamics (Caron et al., 2021; Grill et al., 2020; Chen and He, 2021) to ensure that the distance between unrelated samples remains large. In practice, however, SSL training often experiences the failure mode of dimensional collapse (Jing et al., 2021; Tian et al., 2021; Pokle et al., 2022), where the learned representation spans a low dimensional subspace of the overall available space. In the extreme case, this failure mode instantiates as a complete collapse , where the learned representation becomes zero-rank, and no informative features can be extracted. In this work, we analytically solve\n",
      "==================================================\n",
      "Rank: 36, Similarity Score: 0.6667\n",
      "Self-supervised learning (SSL) has delivered superior performance on a variety of downstream vision tasks. Two main-stream SSL frameworks have been proposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM). ID pulls together representations from different views of the same image, while avoiding feature collapse. It lacks spatial sensitivity, which requires modeling the local structure within each image. On the other hand, MIM reconstructs the original content given a masked image. It instead does not have good semantic alignment, which requires projecting semantically similar views into nearby representations. To address this dilemma, we observe that (1) semantic alignment can be achieved by matching different image views with strong augmentations; (2) spatial sensitivity can benefit from predicting dense representations with masked images. Driven by these analysis, we propose Siamese Image Modeling (SiameseIM), which predicts the dense representations of an augmented view, based on another masked view from the same image but with different augmentations. SiameseIM uses a Siamese network with two branches. The online branch encodes the first view, and predicts the second view's representation according to the relative positions between these two views. The target branch produces the target by encoding the second view. SiameseIM can surpass both ID and MIM on a wide range of downstream tasks, including ImageNet finetuning and linear probing, COCO and LVIS detection, and ADE20k semantic segmentation. The improvement is more significant in few-shot, long-tail and robustness-concerned scenarios. Code shall be released.\n",
      "==================================================\n",
      "Rank: 37, Similarity Score: 0.6656\n",
      "Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (\\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \\textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. The designed asymmetric predictor head and an InfoNCE warm-up strategy enhance the robustness to hyper-parameters and benefit the resulting performance. Experimental results show that our proposed ReSSL substantially outperforms the state-of-the-art methods across different network architectures, including various lightweight networks (\\eg, EfficientNet and MobileNet).\n",
      "==================================================\n",
      "Rank: 38, Similarity Score: 0.6617\n",
      "A reinforcer is a stimulus presented closely following a response which res ults in a future increase in frequency of that response. In an ideal applied setting, a stimulus should b e presented immediately. But, it may not always be possible to present stimuli immediately in the a pplied setting. The current study compares the effects of consequence delivery delivered immediately to consequences delayed by five seconds. Three typically developing males, ages 25-30 participated in vi sualisual matching tasks using arbitrary stimuli. Half of the stimuli were always presented with im mediate consequences, while the other half was only followed by delayed consequences. Participants quickly achiev ed mastery of the matching tasks when consequences were immediate, but did not achieve quick master y (within five sessions) when consequences were delayed by five seconds. These results support the ac cept d wisdom that immediate consequences are always best for skill acquisition and that high trea tment integrity is crucial. Additionally, it provides an example of adapting basic research practice s for use in applied contexts. Delayed Consequence Delivery 5 Delayed Consequence Delivery in Applied Settings: Examining Unsignaled Feed back Delays in VisualVisual Matching Tasks Individuals diagnosed with Autism Spectrum Disorders (ASD) often present deficiencies in responding in three areas of typical human development (DSM-IV, 2000). Firs t, these individuals do not respond to social stimuli with the same regularity that their typical pee rs might. Second, it is often difficult or impossible for people diagnosed with ASDs to communicate effect ively without environmental assistance in the form of picture exchange systems or speech-generating devices. Finally, individuals with ASDs often demonstrate highly restricted or repetiti v interests in specific kinds of stimulation, and this interest often leads to self-stimulatory behavior such as motor stereotypy. When taken together, these three challenges result in individuals wh o may only learn new functional skills though very precisely controlled teaching strategies. Clinicians have found great success when using the application of the principles of behavior analysis in the treatment of t hese individuals, in particular the use of the principle of reinforcement. Properly defined, reinforcement occurs when a stimulus presented or remove d sh rtly following a response results in an increase in the probability of the occurrenc e of that response (Skinner, 1953). Thus, clinicians have used stimuli demonstrated to have a reinforcing effect for the behavior of individuals with whom they are working to increase desirable, functional behavior. Though this principle is widely used, there is one aspect of the definition which does not escape some measure of subj ctivity. That is, a reinforcing stimulus must be changed closely following the targeted response. At present, the conventional wisdom among clinicians has determined that it is best if closely following was equal to immediately following for the purposes of effective behavioral shaping. However, clinical prac tice is typically not automated. Teachers are not as precise with regard s to the time of delivery as food hoppers connected to mechanical devices or specially calibrated timing switc hes. Consequences may often be delivered as immediately as possible, though at other times it may be pre ceded by a time delay. This time Delayed Consequence Delivery 6 delay could be as short as 1 or 2 seconds, or could be as long as 10 seconds or more in som e treatment settings. When faced with a population which has difficulty attending cont inuously to instruction and may be actively engaging in self-stimulatory behavior, such delays may interfere with learning. Lattal (2010), presented an extensive overview of how delay of reinfo cement has been studied with respect to its effects on the operant behavior of animal subjects . In this overview, the author attempts to answer three main questions. First, can the effects of the temporal relation between response and reinforcer be isolated from other environmental changes which accom pany delays? Lattal suggests that they can, as long as the delays are relatively short, on the order of a few seconds. If delayed longer than a few seconds the effect becomes indistinguishable from an absenc e of response-reinforcer dependency. Second, what effect do delays have on operant behavior? Researc h, a noted by Lattal, has found that delays can reduce the effectiveness of response shaping, can induce response differentiation in the absence of effective response training, and can reduce the accuracy a nd rate of responses already established through immediate reinforcement. Third, how can we effectiv ely interpret delay of reinforcement effects? Lattal emphasizes that delay to rein fo cement both imposes contingencies and is imposed by contingencies. Because it is hard to separate temporal contiguity f rom he effects of a reinforcement schedule, it may be best to consider delay to reinforce ment in terms of how it affects behavior. Lattal also describes several variations in how delay to reinf orcement procedures can be implemented. Delays can be either signaled or unsignaled. If a delay is un signaled, there is no stimulus change indicating that a delay is going to take place. Also, delays can be eit her fixed or variable. A fixed delay remains constant throughout a given study, and variable delays are po tentially different for each trial. Finally, delays can be resetting or nonresetting. A resetting dela y is one in which the delay period starts over if any target response occurs during the prescribed delay. N onresetting delays terminate at a predetermined interval regardless of subject responding during that s ame period. Delayed Consequence Delivery 7 Throughout the basic behavioral literature, assessments have been conduct ed with animals to assess some of the effects that delays of reinforcement have on behavi or (Arbuckle & Lattal, 1988; Keeley, Feola, & Lattal, 2007; Odom, Ward, Burke, & Barnes, 2006; Pierce, Ha nford, & Zimmerman, 1972; Royalty, Williams, & Fantino, 1987; Wilkenfield, Nickel, Blakely, & P oling, 1993). In particular, Williams (1976) examined the effects of delayed reinforcer deliver y on the lever-pressing behavior of pigeon subjects. In this study, pecks were reinforced according to a variab le-interval schedule. Then, a delay-of-reinforcement contingency was added onto the schedule, in which a period of un-signaled delay was added following the target response. Behavior was reduced signifi cantly in strength even with just a 3-s delay between response and consequence. Additionally, several studies using animal subjects have incorporat ed multiple schedules and differing values of fixed-time components to examine the effects of res ponding when non-contiguous with consequence delivery. In Sizemore and Lattal (1977), the authors used a yoke -control procedure to assess the differences between response-dependent reinforcement sche dule and response-independent reinforcement schedules. Their data suggest that differences in r ates of responding by pigeons were not entirely due to differences in reinforcer distribution, but were rathe influenced by the non-contiguous nature of their reinforcer delivery. In Sizemore and Lattal (1978), the authors again used tandem variable-inte rval fix d-time schedules with pigeons. The duration of the fixed-time component of the schedul e was varied to assess the effects of unsignaled delays. As the length of the delay was increase d, the pigeons’ response rates decreased. This suggests a weakening of the response reinforcer con tingency and the contiguity between the two was also weakened. Animal studies have shown that the less an absolute delay between res po se and consequences the less the variability of responding for rat and pigeon subjects. If hese studies were to be replicated with human participants, behavior analysts could use the results to make more specific claims about the Delayed Consequence Delivery 8 principle of reinforcement with regards to its application to t he applied setting. The results would add to the growing field of translational research which utilizes the concl usions derived through basic research to the clinical problems of applied behavior analysis, particularly those regarding losses of treatment integrity. One such study, Okouchi (2009), used undergraduate students as partic ipants, and reinforced specific response sequences using a point economy. This study suggested that typically developing adults should be able to acquire a target response under a sizable consequence delay of 1 0 s or more. Treatment Integrity is defined as the extent to which an independent var iable is implemented as intended (Peterson, Homer, & Wonderlich, 1982). There are many ways in which the trea ment integrity of clinical intervention may be less than adequate in applied settings whe re the diagnosis and treatment of problem behavior is the primary goal of practitioners. Behavior programs m ay not be correctly implemented by all direct care staff, operational definitions may be too ge neral, consequences may be delivered inconsistently for target behavior, data collected by direc t care staff may be unreliable, and staff training insufficient for optimal performance. Additionally, without c onsistent supervision and feedback the behavior of caretakers may not conform to contingencies prescribed by be havioral specialists (Vollmer, Sloman, & Pipkin, 2008). One crucial aspect of maintaining high treatment integrity is to en sur the prompt and reliable delivery of reinforcers, as prescribed in behavioral guidelines. Inconsist e cy of reinforcer delivery may be a particularly troubling threat to validity in treatment scenarios in which adaptive skills and alternative responses to problem behavior are being taught to a developmentall\n",
      "==================================================\n",
      "Rank: 39, Similarity Score: 0.6480\n",
      "We propose a Semi-Supervised Learning (SSL) framework, USS-NMF, that allows for explicitly encoding different necessary priors to learn efficient node representations in a graph. USS-NMF specializes in encoding the important yet largely ignored necessary prior for SSL, the cluster assumption. The cluster assumption of SSL requires the existence of well-separated dense regions in a low-dimensional manifold with high label smoothness within each region. USSNMF encodes this assumption in the form of a proposed semi-supervised cluster invariance constraint, which is a group-level smoothness constraint on nodes. We show that explicitly enforcing this constraint enables learning meaningful node representations from both qualitative (visual) and quantitative standpoints. Specifically, USSNMF achieves superior performance on semi-supervised node classification and clustering tasks across thirteen datasets from over eight baselines. Also, the learned node embeddings from USS-NMF yield high-quality (wellseparated homophilous) clusters in t-SNE visualizations. 1 Background on Graph Embedding Graph Embedding techniques have become popular for learning representations for different components of the graph like nodes, edges, sub-graphs, and the entire graph. Graph embedding models encode different intrinsic and extrinsic properties of the network as continuous low dimensional vectors. Network representation learning has been realized by a variety of paradigms such as factorization models, graph kernels, skip-gram based models, deep learning models, generative models and hybrid paradigms, etc [2]. The skip-gram based seminal work, Deepwalk [4] and it’s variants use k-step random walks to define a k-th order neighborhood. Node2Vec [3] is one exception that uses an informed random walk sampling on nodes ∗Department of CSE, Indian Institute of Technology Guwahati †Department of CS, McGill University & MILA ‡Work done during his MS program at IIT Madras §Department of CSE, Ohio State University ¶Department of Biomedical Informatics, Ohio State University ‖Department of CSE, Indian Institute of Technology Madras ∗∗Robert Bosch Centre for Data sciences and AI, IIT Madras based on pre-defined parameters that trades-off between breadth-wise and depth-wise exploration. On the other hand, factorization models have been widely used to encode different network contexts and couple it with different constraints, e.g., Locally Linear Embeddings, Laplacian Eigenmaps, GraphFactorization, ISOMAP, GraRep, HOPE, LINE. Recently, a few works learn clusterable node embeddings that preserve network community information. MNMF is an NMF based model that learns node embedding by factorizing the proximity matrix and predicts community assignments for these nodes from the embeddings. The community assignments are learned jointly, maximizing the modularity of the graph. ComE [1], and GEMSEC [5] are two other community preserving models that learn node embeddings by skip-gram model. GEMSEC is a k-means based adaption for learning node embeddings that jointly learn clusters centers. GEMSEC learns cluster embeddings along with node embeddings and minimizes the distance between the node’s embedding and the nearest cluster mean. ComE, along with minimizing the context prediction loss of skip-gram, also maximizes the log-likelihood of generating node embeddings from multiple GMMs. There are numerous works on learning unsupervised node embeddings for attributed graphs. However, there are only fewer works that jointly learn node embedding with side information such as attribute and group information. Works that learn semisupervised node embeddings with label information is further scarce. These Semi-supervised models can learn discriminative node embeddings that can provide superior node classification results. [6]’s MaxMargin Deep Walk (MMDW), jointly solves the factorization of the PPMI matrix of a k-step random walk and the hinge loss for label prediction. Planetoid [7] is another extension of DeepWalk (DW) that also optimizes a crossentropy loss for label prediction. Planetoid additionally enforces nodes to predict other nodes with similar labels. Planetoid also has a variant for attributed graphs. Copyright c © 2020 by SIAM Unauthorized reproduction of this article is prohibited 2 Implelementation Details The details of the hyper-parameters for our model and the baselines are provided below. The hyper-parameter search space for different components in all the models experimented here is tabulated in Table: 1. Lagrange Multipliers’ Range We first provide the details of HP search for the Lagrange multipliers followed by model specific details. For all the matrix factorization baselines, we vary the hyperparameter values (the respective weightage terms for each component in the objective function) in [0.1, 0.5, 1.0, 5.0, 10.0] except for Wikipedia. In Wikipedia, we found that the network information is far more important than other supervision knowledge. So we varied network co-efficient in the range of [10000, 1000, 100, 10] with other weights in [0.001, 0.01, 0.1, 1.0]. We fixed embedding dimension as 128 for all datasets except Blogcatalog, for which the dimension is set as 4096. DeepWalk & MFDW: For original random-walk based DeepWalk we set the window size to 5. We also have MFDW aka NMF:S in Eqn: 3.2 the objective function for Matrix Factorized DeepWalk, as we build our model incrementally on top of it. Max-Margin DeepWalk (MMDW): In this paper [6], a max-margin loss is incorporated in the objective function of MFDW to learn discriminative representations of vertices. It has one important hyperparameter alpha-bias (η) that induces max-margin loss based bias into the random walk. NMF:S+Y: We build a variant of MMDW which also incorporates supervised information into node embeddings by jointly optimizing Eqn: 3.2 & 3.3. It works competitively as compared to MMDW. Planetoid & MF-Planetoid: Planetoid [7] learns an embedding space for nodes by jointly enforcing label and neighborhood similarity. It uses random walks to enforce structural similarity. We derive a matrixfactorized version of Planetoid as an alternative baseline. It enforces matrix E, i.e, train-label similarity on the embedding space U , unlike ours as in Eqn: 3.6 which enforces label similarity on the cluster space. (2.1) OMF-Plan = O(NMF : S + Y ) + Tr{U∆(E)U } MNMF & MNMF+Y: We build one semisupervised variant of MNMF, viz. MNMF+Y by jointly optimizing its objective function along with Eqn: 3.3. Unlike the original MNMF that factorizes a combination of first-order and cosine similarity based second-order node proximity to learn node representations, here, for the sake of fair comparison, we stick to a combination of first-order and second-order transition probability-based proximity matrix as S, following MMDW [6] as we did for all other comparable methods. USS-NMF: We used the same range of hyperparameters as stated in Table: 2 last column, but instead of searching the optimal combination in the entire range (which is cumbersome), we did partial range search in steps. Step by step, 1) network + label information weights, 2) cluster matrix factorization + cluster learning weights + orthogonality constraint, 3) label smoothing + L2 regularization weights, and finally, 4) the clusters k for each dataset was varied with an increment of 2 in the upper range and with a decrement of 1 in the lower range from its actual number of labels q (inclusive). In the first step, we fixed all other variable values as 1.0, k = q and in later steps, we set already searched parameter values to optimal values found in previous steps, we vary the other variables under consideration. In Table: 2 we have given an effecctive value range for each of the coefficients, applicable for varying sized datasets. We make the following points based on observation, Labels and label-similarity based clusters gave complementary information to support each other for enhanced prediction (evident from their weight combinations in results). For small graphs, network information was more useful than other information. Also, small graphs tended to be more sensitive to weight combinations with easily imposed cluster orthogonality constraint (see the effective range). But for large graphs, USS-NMF needed more weights to ensure orthogonal clusters being learned. Higher cluster learning weights (in effective range) indicate that indeed cluster information mattered. We found optimal results for clusters same as ground truth labels, very different from labels, multiple optimal clusters which indicate that a variety of semantically meaningful clusters have been learnt. In effective search, for simplicity, we set clusters as the number of labels, which gave decent results. Except for Pubmed and Wikipedia, we do not witness any fluctuation in L2 regularization weights. With these observations at hand, we narrowed down the effective hyper-parameter search space such that network has weights (>=1 & <=10), labels & clusters don’t overpower network weights, stable values for regularization & orthogonality weights and the number of ground truth labels as clusters. This ranges’ effectiveness can be seen immediately from Table: 9 where we have used the same effective range to derive results for USSNMF, that still significantly outperforms other semisupervised methods (full hyper-parameter search). Thus, we can conclude that the effective hyper-parameter space is not really huge for USS-NMF and is comparable to other existing methods (see the number of experiments in the last row of Tables: 2 & 1 for reference). Copyright c © 2020 by SIAM Unauthorized reproduction of this article is prohibited Matrix Factorization based methods Random Walk/ Other methods Co-efficients NMF:S NMF:S+Y MMDW MNMF MNMF+Y MF-Plan NMF:S+Y +LS(S,Y) Co-efficients DeepWalk ComE Gemsec Network 0.1-10.0 0.1-10.0 0.1-10.0 0.1-10.0 0.1-10.0 0.1-10.0 0.1-10.0 p [1.0] NA [0.1, 0.3, 0\n",
      "==================================================\n",
      "Rank: 40, Similarity Score: 0.6120\n",
      "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.\n",
      "==================================================\n",
      "Rank: 41, Similarity Score: 0.5797\n",
      "Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the images and then contrasting between the image clusters. We introduce a simple mean-shift algorithm that learns representations by grouping images together without contrasting between them or adopting much of prior on the structure or number of the clusters. We simply \"shift\" the embedding of each image to be close to the \"mean\" of the neighbors of its augmentation. Since the closest neighbor is always another augmentation of the same image, our model will be identical to BYOL when using only one nearest neighbor instead of 5 used in our experiments. Our model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200 epochs outperforming BYOL. Also, our method outperforms the SOTA by a large margin when using weak augmentations only, facilitating adoption of SSL for other modalities. Our code is available here: https://github.com/UMBCvision/MSF\n",
      "==================================================\n",
      "Rank: 42, Similarity Score: 0.5620\n",
      "A number of recent self-supervised learning methods have shown impressive performance on image classification and other tasks. A somewhat bewildering variety of techniques have been used, not always with a clear understanding of the reasons for their benefits, especially when used in combination. Here we treat the embeddings of images as point particles and consider model optimization as a dynamic process on this system of particles. Our dynamic model combines an attractive force for similar images, a locally dispersive force to avoid local collapse, and a global dispersive force to achieve a globally-homogeneous distribution of particles. The dynamic perspective highlights the advantage of using a delayed-parameter image embedding (a la BYOL) together with multiple views of the same image. It also uses a purely-dynamic local dispersive force (Brownian motion) that shows improved performance over other methods and does not require knowledge of other particle coordinates. The method is called MSBReg which stands for (i) a Multiview centroid loss, which applies an attractive force to pull different image view embeddings toward their centroid, (ii) a Singular value loss, which pushes the particle system toward spatially homogeneous density, (iii) a Brownian diffusive loss. We evaluate downstream classification performance of MSBReg on ImageNet as well as transfer learning tasks including fine-grained classification, multi-class object classification, object detection, and instance segmentation. In addition, we also show that applying our regularization term to other methods further improves their performance and stabilize the training by preventing a mode collapse.\n",
      "==================================================\n",
      "Rank: 43, Similarity Score: 0.5594\n",
      "Self-supervised learning allows AI systems to learn effective representations from large amounts of data using tasks that do not require costly labeling. Mode collapse, i.e., the model producing identical representations for all inputs, is a central problem to many self-supervised learning approaches, making self-supervised tasks, such as matching distorted variants of the inputs, ineffective. In this article, we argue that a straightforward application of information maximization among alternative latent representations of the same input naturally solves the collapse problem and achieves competitive empirical results. We propose a self-supervised learning method, CorInfoMax, that uses a second-order statistics-based mutual information measure that reflects the level of correlation among its arguments. Maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. An approximation of the proposed information maximization objective simplifies to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. The regularization term acts as a natural barrier against feature space degeneracy. Consequently, beyond avoiding complete output collapse to a single point, the proposed approach also prevents dimensional collapse by encouraging the spread of information across the whole feature space. Numerical experiments demonstrate that CorInfoMax achieves better or competitive performance results relative to the state-of-the-art SSL approaches.\n",
      "==================================================\n",
      "Rank: 44, Similarity Score: 0.2930\n",
      "This paper describes a new method of rendering volumes that leverages the 3D texturing hardware in Silicon Graphics RealityEngine workstations. The method defines the volume data as a 3D texture and utilizes the parallel texturing hardware to perform reconstruction and resampling on polygons embedded in the texture. The resampled data on each polygon is transformed into color and opacity values and composited into the frame buffer. A 128×128×64 volume is rendered into a 512 window at over 10 frames per-second. Two alternative strategies for embedding the resampling polygons are described and their trade-offs are discussed. This method is easy to implement and we apply it to the production of digitally reconstructed radiographs as well as opacity-based volume rendered images. The generality of this approach is demonstrated by describing its application to the proposed PixelFlow graphics system. PixelFlow overcomes the lighting and volume size limitations imposed by the RealityEngine. It is expected to render 256 data sets on a 640×512 screen at over 10 frames per second 1 Fig. 1 Resampling polygon orientations Volume Boundary a) Object space sample planes b) Image space sample planes RealityEngine is a trademark of Silicon Graphics Inc. where ui are the resample values behind a pixel and d is the spacing between sample values. Note that d is constant for all samples behind a pixel, but due to perspective, it varies from pixel to pixel. The resampled ui terms are summed at each pixel, and the d factors are applied by using an additional full-screen polygon with a 2D texture corresponding to the d values required for each pixel. The summation results may be viewed directly or the exponential required to mimic a radiograph may be computed at each pixel by using a lookup table. The RealityEngine has a maximum precision of 12bits per frame buffer and texture component. The summation could easily overflow that unless the sample values are properly scaled. Our implementation maintains 12-bit volume data values in the texture memory and scales each resampled value by a user controlled \"exposure\" value ranging from zero to one. The scaled samples are then summed and clamped if they exceed the 12-bit range. In practice, it has been easy to find suitable exposure control settings for the data sets tested. Figure 2 shows radiograph images of 128×128×64 CT data of a human pelvis made with polygons aligned in object-space. polygons aligned with the object-space axes. Figure 1b shows resampling on polygons aligned with the image-space axes. In either case, the resampled values behind each pixel are combined to produce a color for that pixel. The combining method is often a compositing operation, but may be other operations as required by the visualization application. Polygons aligned in object-space are defined to lie within the volume and rendered with GL library calls. This method is complicated slightly by the need to reorient the sampling polygons in the plane most parallel to the view-plane as the view-point changes. This is accomplished by examining the view matrix and explicitly creating polygons for the six cases that arise [Westover91]. Polygons aligned in image-space must be clipped to the boundaries of the volume to ensure valid texture coordinates. Polygons are defined in image-space and transformed by the inverse viewing matrix into objectspace where the clipping occurs. Clipped polygons are then rendered with the usual GL library calls. In addition to using unclipped polygons, there are other advantages to using the object-space method. The texturing time for any polygon is proportional to the number of pixels it covers. A priori information about the extent of interesting features in each slice of the volume may be used to minimize the polygon size, and thus its texturing time, as a function of its location. The texture memory of the RealityEngine is limited to 1M 12-bit data points. To render larger volumes, slab subsets are loaded and rendered in succession. Texture memory may be reloaded in about 0.1 seconds. With the object-space method, rendering each slab is simple. The image-space method must render polygons multiple times, clipping them to the currently loaded volume slab. 3. Radiographs A digitally reconstructed radiograph of medical volume data is produced by combining the resampled values behind each pixel to approximate the attenuation integral pixel intensity = 1.0 exp(-∑ ui d ) (1) 2 Fig. 3 Digitally reconstructed radiographs is not normalized and normalization is an expensive process requiring a square root. Lighting without normalization is possible, but this has not yet been tried to see how serious the artifacts are. 5. Performance We consider two data sizes rendered into a 512 window. The smaller data size of 128×128×64 may be rendered at ten frames per-second using 128 polygons aligned in object-space. This equates to a processing rate of 10 million voxels per-second. In our test images we measured about 160 million pixel operations per second, where each pixel operation is a trilinear interpolation of the 3D texture components, a multiplication by a scaling or opacity factor, and a summation or composite into the frame buffer. The larger data size of 256×256×64 requires four 256×256×16 texture slabs and is rendered at 2.5 frames per-second with 256 resampling polygons. Loading texture slabs consumes less than 0.1 seconds per-slab. A progressive refinement approach would allow a user to manipulate the low-resolution data at a high frame rate, and render the high-resolution data as soon as the user allows the motion to stop. The performance is very linear with respect to the number of pixels processed. As the number of screen pixels or resampling polygons is doubled, the frame rate is halved. If more resampling polygons are used, higher quality images are obtained at the expense of lower rendering speed. 6. PixelFlow Texturing hardware is likely to be a common feature of graphics systems in the future. The PixelFlow graphics system under development at the University of North Carolina at Chapel Hill will have texturing hardware [Molnar92] that it is suitable for a variant of the polygon resampling approach described above for the RealityEngine. We propose a polygon texturing approach for the PixelFlow system that will overcome the limitations on realistic lighting and data size imposed by the RealityEngine. The texturing hardware in PixelFlow will allow 128 pixel processors to access eight arbitrarily-addressed 32-bit values in texture memory in under 500 μs. PixelFlow texturing hardware does not perform any operations on these texture values; rather, they are simply loaded into the pixel processors where a user’s program manipulates them as ordinary data. If the 32-bit values are treated as four 8-bit texture components, then three may be 4. Opacity-Based Rendering The summation of samples produces radiograph images. Compositing samples produces images with occlusion. Only one texture component is required for the linear attenuation coefficient used to produce radiographs. Two 8-bit texture components can represent the raw data and a precomputed shading coefficient. The resampled data component values are used as indices into an opacity lookup table. This lookup uses the texture hardware for speed. The shading coefficient is a function of the original data gradient and multiplies the sample opacity to produce images of shaded features as shown in figure 3. This figure shows the human pelvis data set above an image of a 4 mm volume of a chicken embryo acquired by a microscopic MRI scanner. The precomputed shading fixes the light position(s) relative to the volume. For more general lighting by sources fixed in image-space, the shade texture component must be replaced by three components containing the normalized data gradient. Unfortunately, the resampled gradient on the polygons Fig. 3 Shaded volume rendering\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # 1. Initialize BERT model and tokenizer\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# # 2. Function to get embeddings\n",
    "# def get_embedding(text):\n",
    "#     tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding='max_length')\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokens)\n",
    "#     return outputs['pooler_output'].detach().numpy()\n",
    "\n",
    "# # 3. Compute the embedding for the idea\n",
    "# # idea_text = idea_text\n",
    "# idea_embedding = get_embedding(idea_text_summary)\n",
    "\n",
    "# # 4. Compute embeddings for abstracts and rank them\n",
    "# abstracts = combined_df['abstract'].dropna().values  # List of abstracts\n",
    "# abstract_embeddings = [get_embedding(abstract) for abstract in abstracts]\n",
    "\n",
    "# # 5. Calculate cosine similarities and rank\n",
    "# similarities = [cosine_similarity(idea_embedding, abstract_embedding)[0][0] for abstract_embedding in abstract_embeddings]\n",
    "# ranked_abstracts = sorted(zip(abstracts, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 6. Print ranked abstracts\n",
    "# for rank, (abstract, score) in enumerate(ranked_abstracts, 1):\n",
    "#     print(f\"Rank: {rank}, Similarity Score: {score:.4f}\")\n",
    "#     print(abstract)\n",
    "#     print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4105b7-f5b2-4931-9547-e851492876aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"MBZUAI/LaMini-Flan-T5-77M\"\n",
    "\n",
    "model = pipeline('text2text-generation', model = checkpoint)\n",
    "input_prompt = \"\"\"Here is an idea: Lets say I have a neural network that maps a set of images from the same distribution. I want all of them to be mapped to the same embedding. To avoid collapse, I will use eigenvalues of the output embedding and ensure they are all larger 1.0, or maybe if they are multiplied to one another, the result is larger than 1.0.\n",
    "\n",
    "How relevant this idea is to the following abstract of a paper:\n",
    "\n",
    "Deep neural networks (DNNs), regardless of their impressive performance, are vulnerable to attacks from adversarial inputs and, more recently, Trojans to misguide or hijack the decision of the model. We expose the existence of an intriguing class of spatially bounded, physically realizable, adversarial examples— Universal NaTuralistic adversarial paTches—we call TnTs, by exploring the super set of the spatially bounded adversarial example space and the natural input space within generative adversarial networks. Now, an adversary can arm themselves with a patch that is naturalistic, less malicious-looking, physically realizable, highly effective—achieving high attack success rates, and universal. A TnT is universal because any input image captured with a TnT in the scene will: i) misguide a network (untargeted attack); or ii) force the network to make a malicious decision (targeted attack). Interestingly, now, an adversarial patch attacker has the potential to exert a greater level of control—the ability to choose a location independent, natural-looking patch as a trigger in contrast to being constrained to noisy perturbations—an ability is thus far shown to be only possible with Trojan attack methods needing to interfere with the model building processes to embed a backdoor at the risk discovery; but, still realize a patch deployable in the physical world. Through extensive experiments on the large-scale visual classification task, ImageNet with evaluations across its entire validation set of 50,000 images, we demonstrate the realistic threat from TnTs and the robustness of the attack. We show a generalization of the attack to create patches achieving higher attack success rates than existing state-of-the-art methods. Our results show the generalizability of the attack to different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural networks such as WideResnet50, Inception-V3 and VGG-16.\"\"\"\n",
    "input_prompt = 'Please let me know your thoughts on the given place and why you think it deserves to be visited: \\n\"Barcelona, Spain\"'\n",
    "generated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\n",
    "\n",
    "print(\"Response\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
